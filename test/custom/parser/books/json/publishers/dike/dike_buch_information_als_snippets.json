[
  {
    "bitmark": "[.book]\n[@id:145771]\n[#Information als Snippets]\n[@language:de]\n[@coverImage:https://docs.bitmark.cloud/bit-books/dike_verlag/information_als_snippets/web-resources/images/dike_information_cover.png]\n[@externalId:e-information-als-snippets_jzzasgmi1_2n]",
    "bit": {
      "type": "book",
      "format": "bitmark--",
      "body": "",
      "item": "",
      "hint": "",
      "isExample": false,
      "example": "",
      "id": [
        "145771"
      ],
      "title": "Information als Snippets",
      "language": [
        "de"
      ],
      "coverImage": [
        "https://docs.bitmark.cloud/bit-books/dike_verlag/information_als_snippets/web-resources/images/dike_information_cover.png"
      ],
      "externalId": [
        "e-information-als-snippets_jzzasgmi1_2n"
      ]
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145772]\n[#Prolog]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 1,
      "progress": true,
      "toc": true,
      "id": [
        "145772"
      ],
      "title": "Prolog"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145773]\nIm Anfang war das Wort“, mit der das Evangelium des Johannes beginnt. Faust will das Wort bekanntlich nicht so hoch schätzen, und er schlägt andere Möglichkeiten vor – Sinn, Kraft oder Tat. Das war vor zweihundert Jahren. Heute würden wir die Stelle erneut anders übersetzen, nämlich mit unserem Schlüsselwort: Im Anfang war die Information. „It from Bit“, wie es die englische Sprache in wunderbarer Kürze und mit lässiger Eleganz unter Zuhilfenahme der wissenschaftlich messbaren Einheit der Information, des Bit, auszudrücken gestattet.]\nDie Formel „It from Bit“ stammt von dem kürzlich verstorbenen amerikanischen Physiker John A. Wheeler, der dadurch nicht nur andeutet, dass die plumpe Welt ihren Anfang einer Formbildung – einer In-formation – verdankt, sondern auch, dass wir selbst das Etwas durch die Informationen erzeugen, die wir von ihm bekommen.\n[Umgruppierung, Leichte Umformulierung:] Und wir erwerben Informationen, weil wir nach ihnen streben. Dies ist unsere Natur, was in der Metaphysik des Aristoteles nachzulesen ist: „Alle Menschen streben von Natur nach Wissen“.\nHeute sprechen wir weniger von Wissen als von Informationen. Wir benützen diesen Begriff mit größter Selbstverständlichkeit, aber wir können aber hoffen, dass wir am Ende der Suche nach Informationen über die Information schließlich über ein wenig mehr wirkliches Wissen verfügen – gemeint ist: ein Verständnis dafür, wie wir dazu gekommen sind, die Welt immer mehr als einen Kosmos von Informationen zu verstehen und was das für unser Verständnis der Welt bedeutet.\nWir wollen sehen, zu was sie uns führt.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Im Anfang war das Wort“, mit der das Evangelium des Johannes beginnt. Faust will das Wort bekanntlich nicht so hoch schätzen, und er schlägt andere Möglichkeiten vor – Sinn, Kraft oder Tat. Das war vor zweihundert Jahren. Heute würden wir die Stelle erneut anders übersetzen, nämlich mit unserem Schlüsselwort: Im Anfang war die Information. „It from Bit“, wie es die englische Sprache in wunderbarer Kürze und mit lässiger Eleganz unter Zuhilfenahme der wissenschaftlich messbaren Einheit der Information, des Bit, auszudrücken gestattet.]\nDie Formel „It from Bit“ stammt von dem kürzlich verstorbenen amerikanischen Physiker John A. Wheeler, der dadurch nicht nur andeutet, dass die plumpe Welt ihren Anfang einer Formbildung – einer In-formation – verdankt, sondern auch, dass wir selbst das Etwas durch die Informationen erzeugen, die wir von ihm bekommen.\n[Umgruppierung, Leichte Umformulierung:] Und wir erwerben Informationen, weil wir nach ihnen streben. Dies ist unsere Natur, was in der Metaphysik des Aristoteles nachzulesen ist: „Alle Menschen streben von Natur nach Wissen“.\nHeute sprechen wir weniger von Wissen als von Informationen. Wir benützen diesen Begriff mit größter Selbstverständlichkeit, aber wir können aber hoffen, dass wir am Ende der Suche nach Informationen über die Information schließlich über ein wenig mehr wirkliches Wissen verfügen – gemeint ist: ein Verständnis dafür, wie wir dazu gekommen sind, die Welt immer mehr als einen Kosmos von Informationen zu verstehen und was das für unser Verständnis der Welt bedeutet.\nWir wollen sehen, zu was sie uns führt.",
      "id": [
        "145773"
      ]
    },
    "errors": [
      {
        "message": "Unexpected input ']'",
        "line": 3,
        "column": 539,
        "errorLine": "Im Anfang war das Wort“, mit der das Evangelium des Johannes beginnt. Faust will das Wort bekanntlich nicht so hoch schätzen, und er schlägt andere Möglichkeiten vor – Sinn, Kraft oder Tat. Das war vor zweihundert Jahren. Heute würden wir die Stelle erneut anders übersetzen, nämlich mit unserem Schlüsselwort: Im Anfang war die Information. „It from Bit“, wie es die englische Sprache in wunderbarer Kürze und mit lässiger Eleganz unter Zuhilfenahme der wissenschaftlich messbaren Einheit der Information, des Bit, auszudrücken gestattet.]"
      },
      {
        "message": "Unexpected input '[Umgruppierung'",
        "line": 5,
        "column": 0,
        "errorLine": "[Umgruppierung, Leichte Umformulierung:] Und wir erwerben Informationen, weil wir nach ihnen streben. Dies ist unsere Natur, was in der Metaphysik des Aristoteles nachzulesen ist: „Alle Menschen streben von Natur nach Wissen“."
      },
      {
        "message": "Unexpected input ']'",
        "line": 5,
        "column": 39,
        "errorLine": "[Umgruppierung, Leichte Umformulierung:] Und wir erwerben Informationen, weil wir nach ihnen streben. Dies ist unsere Natur, was in der Metaphysik des Aristoteles nachzulesen ist: „Alle Menschen streben von Natur nach Wissen“."
      }
    ]
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145774]\n[%Prolog - Einführung]\n„Alle Menschen streben von Natur nach Wissen“ – heißt es in der Metaphysik des Aristoteles. „Alle Menschen streben nach __Information__“, würden wir heute formulieren, obwohl wir über diesen Begriff, den wir heute mit größter Selbstverständlichkeit benutzen, noch lange nicht gründlich genug informiert sind. Aber wir dürfen hoffen, dass wir am Ende unserer Suche nach Informationen über die Information zumindest verstehen, wie wir dazu gekommen sind, die Welt immer mehr als einen Kosmos von Informationen zu verstehen und was das für unser Verständnis der Welt bedeutet.\nWie ist dieses Etwas geworden, das Mephisto diese „plumpe Welt“ nennt, die sich „dem Nichts entgegenstellt“? Mephistos Gegenspieler Faust versucht die Entstehung der Welt zu ergründen, in dem er eine bessere Übersetzung für die berühmte These „Im Anfang war das Wort“ sucht, mit der das Evangelium des Johannes beginnt. Faust will das Wort bekanntlich nicht so hoch schätzen und es durch andere Begriffe ersetzen – Sinn, Kraft oder Tat. Heute würden wir die Stelle erneut anders übersetzen, nämlich mit unserem Schlüsselwort: Im Anfang war die Information. „It from Bit“, wie es die englische Sprache in wunderbarer Kürze und mit lässiger Eleganz unter Zuhilfenahme der wissenschaftlich messbaren Einheit der Information, des Bit, auszudrücken gestattet. Die Formel „It from Bit“ stammt von dem kürzlich verstorbenen amerikanischen Physiker John A. Wheeler, der dadurch nicht nur andeutet, dass die plumpe Welt ihren Anfang einer Formbildung – einer In-formation – verdankt, sondern auch, dass wir selbst das Etwas erst durch die Informationen erzeugen, die wir von ihm bekommen…\nWem bei solchen Gedanken schwindelt, dem sei versichert, dass seine wissbegierige Natur ihn dazu bringen wird, sie zu begreifen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "„Alle Menschen streben von Natur nach Wissen“ – heißt es in der Metaphysik des Aristoteles. „Alle Menschen streben nach __Information__“, würden wir heute formulieren, obwohl wir über diesen Begriff, den wir heute mit größter Selbstverständlichkeit benutzen, noch lange nicht gründlich genug informiert sind. Aber wir dürfen hoffen, dass wir am Ende unserer Suche nach Informationen über die Information zumindest verstehen, wie wir dazu gekommen sind, die Welt immer mehr als einen Kosmos von Informationen zu verstehen und was das für unser Verständnis der Welt bedeutet.\nWie ist dieses Etwas geworden, das Mephisto diese „plumpe Welt“ nennt, die sich „dem Nichts entgegenstellt“? Mephistos Gegenspieler Faust versucht die Entstehung der Welt zu ergründen, in dem er eine bessere Übersetzung für die berühmte These „Im Anfang war das Wort“ sucht, mit der das Evangelium des Johannes beginnt. Faust will das Wort bekanntlich nicht so hoch schätzen und es durch andere Begriffe ersetzen – Sinn, Kraft oder Tat. Heute würden wir die Stelle erneut anders übersetzen, nämlich mit unserem Schlüsselwort: Im Anfang war die Information. „It from Bit“, wie es die englische Sprache in wunderbarer Kürze und mit lässiger Eleganz unter Zuhilfenahme der wissenschaftlich messbaren Einheit der Information, des Bit, auszudrücken gestattet. Die Formel „It from Bit“ stammt von dem kürzlich verstorbenen amerikanischen Physiker John A. Wheeler, der dadurch nicht nur andeutet, dass die plumpe Welt ihren Anfang einer Formbildung – einer In-formation – verdankt, sondern auch, dass wir selbst das Etwas erst durch die Informationen erzeugen, die wir von ihm bekommen…\nWem bei solchen Gedanken schwindelt, dem sei versichert, dass seine wissbegierige Natur ihn dazu bringen wird, sie zu begreifen.",
      "id": [
        "145774"
      ],
      "item": "Prolog - Einführung"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145775]\n[%Die informierte Gesellschaft]\nDas Informationszeitalter beginnt etwa in der Mitte des 20. Jahrhunderts, genauer: nach dem Ende des Zweiten Weltkriegs. In der zweiten Hälfte der 1940er Jahre taucht erst das Konzept der Information und bald auch das dazugehörige Wort auf, und beides geschieht nicht nur in einer Naturwissenschaft – der Biologie der Vererbung, in die ein Physiker das Informationskonzept einführt –, sondern gleichzeitig auch im Arbeitsfeld von Ingenieuren und Mathematikern, die von außen gesehen völlig andere Interessen verfolgen. In der Biologie sucht man durch einen neuen Blick auf die Erbanlagen Antworten auf die Frage „Was ist Leben?“ zu finden, und in der Nachrichtentechnik will man wissen, wie viele Störungen (Rauschen und Geräusche) ein Eingangssignal verträgt, um am Ende des Übertragungsweges noch verstanden zu werden. Und während sich die dazugehörigen Ideen entwickeln und auf den damals noch ungewohnten Begriff der „Information“ zulaufen, konstruieren Physiker und Ingenieure ein elektronisches Bauelement namens Transistor.\nTransistoren werden zuerst in Radios eingesetzt, die in der Mitte der 1950er Jahre so klein und billig werden, dass Nachrichten und Musik aus aller Welt bald in jeden Hinterhof gelangen können. Ein Globalisierungsschub findet statt, ohne dass dies so bezeichnet oder beschrieben wird.\nTechniker und Unternehmer nehmen den Transistor von Anfang an ernst. Sie ahnen nämlich, dass er sehr viel mehr kann als Radioapparate verkleinern und ihren Empfang verbessern. Tatsächlich entsteht mit den Transitoren die Informationstechnologie – die IT-Branche –, die im Laufe der Jahre nicht nur die reichsten Männer der Welt – wie Bill Gates –, sondern auch eine ganz neue Form der Ökonomie hervorbringt, nämlich die Netzwirtschaft. Dies wird möglich, weil aus dem unscheinbaren kristallinen Verstärkerelement namens Transistor erst integrierte Schaltkreise und dann immer raffiniertere Mikroprozessoren zusammengesetzt werden, in denen inzwischen auf engstem Raum nicht nur Millionen, sondern Milliarden von Transistoren untergebracht sind. Diese phantastische Menge von intelligent miteinander verbundenen Transistoren sind die Voraussetzung für das Funktionieren der immer zahlreicher werdenden Computer und Laptops, durch die wir alle in einem weltweiten Netz – dem World Wide Web – verbunden sind. Und aus dem dazugehörigen Internet werden wird bereits jetzt derart umfassend mit Informationen versorgt, dass es einige Intellektuelle schick finden, zu beklagen, dass sie das alles in ihrem Kopf nicht mehr unterbringen können und in der Informationsflut ertrinken. Sie tun so, als ob das etwas Neues sei, und haben vergessen, dass Menschen immer schon mehr Erfahrungen gesammelt und beschrieben haben, als ein Einzelner aufzunehmen imstande war.\nDoch natürlich stellt sich die Frage, ob wir Menschen mit den gigantischen Mengen an abrufbaren Informationen nicht überfordert sind und ob wir nicht möglicherweise längst den Wald vor lauter Bäumen nicht mehr sehen. Entspricht der zunehmenden Menge an verfügbaren Daten auch ein zunehmendes Wissen und Verstehen? Wie kann es gelingen, dass wir nicht in Massen für uns gar nicht relevanter Informationen oder Daten stecken bleiben, sondern zu Informationen kommen, die wir untereinander austauschen möchten?",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Das Informationszeitalter beginnt etwa in der Mitte des 20. Jahrhunderts, genauer: nach dem Ende des Zweiten Weltkriegs. In der zweiten Hälfte der 1940er Jahre taucht erst das Konzept der Information und bald auch das dazugehörige Wort auf, und beides geschieht nicht nur in einer Naturwissenschaft – der Biologie der Vererbung, in die ein Physiker das Informationskonzept einführt –, sondern gleichzeitig auch im Arbeitsfeld von Ingenieuren und Mathematikern, die von außen gesehen völlig andere Interessen verfolgen. In der Biologie sucht man durch einen neuen Blick auf die Erbanlagen Antworten auf die Frage „Was ist Leben?“ zu finden, und in der Nachrichtentechnik will man wissen, wie viele Störungen (Rauschen und Geräusche) ein Eingangssignal verträgt, um am Ende des Übertragungsweges noch verstanden zu werden. Und während sich die dazugehörigen Ideen entwickeln und auf den damals noch ungewohnten Begriff der „Information“ zulaufen, konstruieren Physiker und Ingenieure ein elektronisches Bauelement namens Transistor.\nTransistoren werden zuerst in Radios eingesetzt, die in der Mitte der 1950er Jahre so klein und billig werden, dass Nachrichten und Musik aus aller Welt bald in jeden Hinterhof gelangen können. Ein Globalisierungsschub findet statt, ohne dass dies so bezeichnet oder beschrieben wird.\nTechniker und Unternehmer nehmen den Transistor von Anfang an ernst. Sie ahnen nämlich, dass er sehr viel mehr kann als Radioapparate verkleinern und ihren Empfang verbessern. Tatsächlich entsteht mit den Transitoren die Informationstechnologie – die IT-Branche –, die im Laufe der Jahre nicht nur die reichsten Männer der Welt – wie Bill Gates –, sondern auch eine ganz neue Form der Ökonomie hervorbringt, nämlich die Netzwirtschaft. Dies wird möglich, weil aus dem unscheinbaren kristallinen Verstärkerelement namens Transistor erst integrierte Schaltkreise und dann immer raffiniertere Mikroprozessoren zusammengesetzt werden, in denen inzwischen auf engstem Raum nicht nur Millionen, sondern Milliarden von Transistoren untergebracht sind. Diese phantastische Menge von intelligent miteinander verbundenen Transistoren sind die Voraussetzung für das Funktionieren der immer zahlreicher werdenden Computer und Laptops, durch die wir alle in einem weltweiten Netz – dem World Wide Web – verbunden sind. Und aus dem dazugehörigen Internet werden wird bereits jetzt derart umfassend mit Informationen versorgt, dass es einige Intellektuelle schick finden, zu beklagen, dass sie das alles in ihrem Kopf nicht mehr unterbringen können und in der Informationsflut ertrinken. Sie tun so, als ob das etwas Neues sei, und haben vergessen, dass Menschen immer schon mehr Erfahrungen gesammelt und beschrieben haben, als ein Einzelner aufzunehmen imstande war.\nDoch natürlich stellt sich die Frage, ob wir Menschen mit den gigantischen Mengen an abrufbaren Informationen nicht überfordert sind und ob wir nicht möglicherweise längst den Wald vor lauter Bäumen nicht mehr sehen. Entspricht der zunehmenden Menge an verfügbaren Daten auch ein zunehmendes Wissen und Verstehen? Wie kann es gelingen, dass wir nicht in Massen für uns gar nicht relevanter Informationen oder Daten stecken bleiben, sondern zu Informationen kommen, die wir untereinander austauschen möchten?",
      "id": [
        "145775"
      ],
      "item": "Die informierte Gesellschaft"
    }
  },
  {
    "bitmark": "[.article:bitmark++]\n[@id:145776]\n[%Was ist Information?]\nInformationen als über Dinge erfahrbare Daten und Informationen als Mitteilungen zwischen bewussten Akteuren – das Wort Information wird in unterschiedlichen Disziplinen von unterschiedlichen Personen offenbar mit unterschiedlichen Bedeutungen verwendet wird. Zur Zeit kann niemand die Hoheit über den Begriff der Information beanspruchen, und wir sollten ihn deshalb in einer möglichst offenen Weise benutzen, um die kaum übersehbaren Folgen der Anfänge des Informationskonzepts im wissenschaftlichen Denken und technischen Handeln nach dem Zweiten Weltkrieg nachvollziehen zu können.\nEine dieser Folgen besteht darin, dass wir uns seit längerem und voller Stolz als eine informierte Gesellschaft betrachten und beschreiben; wortfindige Sozialwissenschaftler wenden auf unsere alltägliche Lebenswirklichkeit sogar ohne zu zögern den Begriff „Informationsgesellschaft“ an. Und tatsächlich: Wir legen täglich rund um die Uhr die genetische Information von Zellen offen, wir versenden und empfangen Informationen, wir kaufen und verkaufen sie und sind davon so beeindruckt, dass wir etwas übersehen: dass wir derzeit noch nicht in der Lage sind, eine befriedigende Antwort auf die Frage zu geben, „Was ist Information?“.\nDies ist kein Beinbruch: Denn wer meint, dass Wissenschaft mit Begriffen hantiert, die sie genau versteht, befindet sich in einem grundsätzlichen Irrtum. Physiker haben erfolgreich mit Atomen und Elektronen die Welt (oder etwas von ihr) erklärt, ohne zu wissen, was Atome und Elektronen sind. Wir wissen bis heute zum Beispiel nicht, was Energie genau ist, ohne auf diesen Begriff verzichten zu können, und der überlebensgroße Albert Einstein hat am Ende seines Lebens eingeräumt, dass ihm fünfzig Jahre des Nachdenkens keine Antwort auf die Frage gegeben haben, was Licht ist. Es gehört zum Wesen der exakten Naturwissenschaft, dass sich mit ihrem Fortschreiten das Verständnis der Grundbegriffe ändern kann und man eines Tages verschiedene Konzepte unterscheiden oder verweben muss – so wie man im Laufe der Wissenschaftsgeschichte gelernt hat, Kraft und Energie zu trennen und zu verbinden.\nMan kann „Information“ auf ganz verschiedene Weise knapp definieren oder die Bedeutung des Begriffs durch einen Satz ausdrücken (explizieren). Wir führen hier einige Möglichkeiten auf:\n\n• Information ist ein Muster, das mitgeteilt und wahrgenommen werden kann.\n• Information ist etwas, das verstanden wird und dabei Informationen erzeugt.\n• Information ist die Botschaft, die ein Sender einem Empfänger zukommen lässt.\n• Information besteht aus Bits (oder Bytes) und kann quantifiziert werden.\n• Information in der Biologie meint die Reihenfolge (Sequenz) der genetischen Bausteine in einem DNA-Molekül.\n• Information im Gehirn setzt sich aus Frequenzen von Nervenimpulsen und davon ausgelösten chemischen Reaktionen zusammen.\n\nKurz, der Begriff der Information hat so viele Dimensionen, dass man versucht ist, ihn zum Schlüssel einer allgemeinen Welterklärung zu machen.\n\nDas Bibelwort „Im Anfang war das Wort“ aus dem Evangelium des Johannes kann bekanntlich unterschiedlich übersetzt werden. Das griechische „logos“, das meist mit „Wort“ übersetzt wird, erlaubt auch eine Version mit unserem Schlüsselwort:\n\n„Im Anfang war die Information, und die Information war bei Gott, und Gott war die Information. Sie war am Anfang bei Gott. Alle Dinge sind durch sie geworden, und ohne Information ist auch nicht eines geworden, das geworden ist. In ihr war Leben, und das Leben war das Licht für die Menschen.“\n\nUnd vielleicht sollte man auch den „Anfang“ weglassen und durch ein treffenderes Wort ersetzen. Dann würde es heißen:\n\n„Im Urstoff war die Information, und die Information war bei Gott, und Gott war die Information. Sie war im Urstoff bei Gott. Alle Dinge sind durch sie geworden, und ohne Information ist auch nicht eines geworden, das geworden ist. In ihr war Leben, und das Leben war das Licht für die Menschen.“\n\nEs geht beim Begriff der Information, das sollte deutlich geworden sein, nicht um Kleinigkeiten. Entsprechend wird um ihn gerungen. Auf der einen Seite nennt es der Philosoph Peter Janich eine Legende, wenn behauptet wird, „Information ist ein Naturgegenstand“. Er beklagt sogar den „Missstand“ einer „Naturalisierung der Information“, die er in dem Anspruch der (mathematisierten) Naturwissenschaft sieht, allein für die Erforschung der Information zuständig zu sein. Auf der anderen Seite wächst die Zahl der Physiker und anderer Naturforscher, die in dem Satz „Information ist physikalisch“ eine tiefe Wahrheit sehen, durch die sie sich herausgefordert und angespornt fühlen. Sie wollen die Information so in die wissenschaftliche Beschreibung der Natur einfügen, wie es ihnen mit Messgrößen wie Ladung und Masse längst gelungen ist, und sie sind davon überzeugt, dass dann, wenn sie gelernt haben, Information gleichberechtigt etwa mit Energie in der Beschreibung der Wirklichkeit zu verankern, eine völlig andere Wissenschaft entsteht. Eine solche Wissenschaft wird uns vielleicht auch sagen können, was es mit der beim ersten Hören überraschend klingenden These des Wiener Quantenphysikers Anton Zeilinger auf sich hat, der seit einigen Jahren die radikale Ansicht vertritt „Wirklichkeit und Information sind dasselbe.“",
    "bit": {
      "type": "article",
      "format": "bitmark++",
      "body": "Informationen als über Dinge erfahrbare Daten und Informationen als Mitteilungen zwischen bewussten Akteuren – das Wort Information wird in unterschiedlichen Disziplinen von unterschiedlichen Personen offenbar mit unterschiedlichen Bedeutungen verwendet wird. Zur Zeit kann niemand die Hoheit über den Begriff der Information beanspruchen, und wir sollten ihn deshalb in einer möglichst offenen Weise benutzen, um die kaum übersehbaren Folgen der Anfänge des Informationskonzepts im wissenschaftlichen Denken und technischen Handeln nach dem Zweiten Weltkrieg nachvollziehen zu können.\nEine dieser Folgen besteht darin, dass wir uns seit längerem und voller Stolz als eine informierte Gesellschaft betrachten und beschreiben; wortfindige Sozialwissenschaftler wenden auf unsere alltägliche Lebenswirklichkeit sogar ohne zu zögern den Begriff „Informationsgesellschaft“ an. Und tatsächlich: Wir legen täglich rund um die Uhr die genetische Information von Zellen offen, wir versenden und empfangen Informationen, wir kaufen und verkaufen sie und sind davon so beeindruckt, dass wir etwas übersehen: dass wir derzeit noch nicht in der Lage sind, eine befriedigende Antwort auf die Frage zu geben, „Was ist Information?“.\nDies ist kein Beinbruch: Denn wer meint, dass Wissenschaft mit Begriffen hantiert, die sie genau versteht, befindet sich in einem grundsätzlichen Irrtum. Physiker haben erfolgreich mit Atomen und Elektronen die Welt (oder etwas von ihr) erklärt, ohne zu wissen, was Atome und Elektronen sind. Wir wissen bis heute zum Beispiel nicht, was Energie genau ist, ohne auf diesen Begriff verzichten zu können, und der überlebensgroße Albert Einstein hat am Ende seines Lebens eingeräumt, dass ihm fünfzig Jahre des Nachdenkens keine Antwort auf die Frage gegeben haben, was Licht ist. Es gehört zum Wesen der exakten Naturwissenschaft, dass sich mit ihrem Fortschreiten das Verständnis der Grundbegriffe ändern kann und man eines Tages verschiedene Konzepte unterscheiden oder verweben muss – so wie man im Laufe der Wissenschaftsgeschichte gelernt hat, Kraft und Energie zu trennen und zu verbinden.\nMan kann „Information“ auf ganz verschiedene Weise knapp definieren oder die Bedeutung des Begriffs durch einen Satz ausdrücken (explizieren). Wir führen hier einige Möglichkeiten auf:\n\n• Information ist ein Muster, das mitgeteilt und wahrgenommen werden kann.\n• Information ist etwas, das verstanden wird und dabei Informationen erzeugt.\n• Information ist die Botschaft, die ein Sender einem Empfänger zukommen lässt.\n• Information besteht aus Bits (oder Bytes) und kann quantifiziert werden.\n• Information in der Biologie meint die Reihenfolge (Sequenz) der genetischen Bausteine in einem DNA-Molekül.\n• Information im Gehirn setzt sich aus Frequenzen von Nervenimpulsen und davon ausgelösten chemischen Reaktionen zusammen.\n\nKurz, der Begriff der Information hat so viele Dimensionen, dass man versucht ist, ihn zum Schlüssel einer allgemeinen Welterklärung zu machen.\n\nDas Bibelwort „Im Anfang war das Wort“ aus dem Evangelium des Johannes kann bekanntlich unterschiedlich übersetzt werden. Das griechische „logos“, das meist mit „Wort“ übersetzt wird, erlaubt auch eine Version mit unserem Schlüsselwort:\n\n„Im Anfang war die Information, und die Information war bei Gott, und Gott war die Information. Sie war am Anfang bei Gott. Alle Dinge sind durch sie geworden, und ohne Information ist auch nicht eines geworden, das geworden ist. In ihr war Leben, und das Leben war das Licht für die Menschen.“\n\nUnd vielleicht sollte man auch den „Anfang“ weglassen und durch ein treffenderes Wort ersetzen. Dann würde es heißen:\n\n„Im Urstoff war die Information, und die Information war bei Gott, und Gott war die Information. Sie war im Urstoff bei Gott. Alle Dinge sind durch sie geworden, und ohne Information ist auch nicht eines geworden, das geworden ist. In ihr war Leben, und das Leben war das Licht für die Menschen.“\n\nEs geht beim Begriff der Information, das sollte deutlich geworden sein, nicht um Kleinigkeiten. Entsprechend wird um ihn gerungen. Auf der einen Seite nennt es der Philosoph Peter Janich eine Legende, wenn behauptet wird, „Information ist ein Naturgegenstand“. Er beklagt sogar den „Missstand“ einer „Naturalisierung der Information“, die er in dem Anspruch der (mathematisierten) Naturwissenschaft sieht, allein für die Erforschung der Information zuständig zu sein. Auf der anderen Seite wächst die Zahl der Physiker und anderer Naturforscher, die in dem Satz „Information ist physikalisch“ eine tiefe Wahrheit sehen, durch die sie sich herausgefordert und angespornt fühlen. Sie wollen die Information so in die wissenschaftliche Beschreibung der Natur einfügen, wie es ihnen mit Messgrößen wie Ladung und Masse längst gelungen ist, und sie sind davon überzeugt, dass dann, wenn sie gelernt haben, Information gleichberechtigt etwa mit Energie in der Beschreibung der Wirklichkeit zu verankern, eine völlig andere Wissenschaft entsteht. Eine solche Wissenschaft wird uns vielleicht auch sagen können, was es mit der beim ersten Hören überraschend klingenden These des Wiener Quantenphysikers Anton Zeilinger auf sich hat, der seit einigen Jahren die radikale Ansicht vertritt „Wirklichkeit und Information sind dasselbe.“",
      "id": [
        "145776"
      ],
      "item": "Was ist Information?"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145777]\n[%Die doppelte Herkunft des Wortes „Information“]\n„Information“ hat als Begriff der Wissenschaften seit dem Ende des Zweiten Weltkriegs die moderne Gesellschaft transformiert. In kurzer Zeit haben die Menschen in den technisch entwickelten und mit Wissenschaft operierenden Ländern gelernt, völlig selbstverständlich mit „Information“ umzugehen, und im Rückblick wirkt es ausgesprochen erstaunlich, dass wir jemals ohne dieses gefällige Wort auskommen konnten.\nDie „Information“ debütierte in der Nachkriegsgeschichte zweimal unabhängig voneinander: In den genetischen Lebenswissenschaften und in der Nachrichtentechnik. Darüber wundern dürfen wir uns, dass diesem doppelten Ursprung des Begriffs in der naturwissenschaftlichen Moderne eine Dopplung des antiken Bergriffs der Information entspricht. Der Philosoph Peter Janich schreibt in seinem Buch __Was ist Information?__: „Schon die Anfänge der lateinischen Herkunft von ‚Information’ sind zweigeteilt“. Und weiter: „Zum einen bedeutet ‚informare’ das handwerkliche Formen (etwa eines ‚gewaltigen Rundschilds’ durch den Schmied in der Dichtung des Vergil); und fast gleichzeitig wird Information als Inhalt eines vollständigen Satzes (etwa bei Cicero) verstanden.“\nMit anderen Worten: Sowohl Menschen als auch Materialien können informiert sein. Dieser Doppelcharakter der Information rückt aber erst Mitte der 1940er Jahre ins (fach-)öffentliche Bewusstsein. In einer Zeit, in der sich die Hinwendung der alten Biologie hin zur Molekulargenetik vollzieht und der Transistor erfunden wird – und in der beide Ereignisse „Information“ in den Mittelpunkt erst der Forschung und dann der Gesellschaft rücken.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "„Information“ hat als Begriff der Wissenschaften seit dem Ende des Zweiten Weltkriegs die moderne Gesellschaft transformiert. In kurzer Zeit haben die Menschen in den technisch entwickelten und mit Wissenschaft operierenden Ländern gelernt, völlig selbstverständlich mit „Information“ umzugehen, und im Rückblick wirkt es ausgesprochen erstaunlich, dass wir jemals ohne dieses gefällige Wort auskommen konnten.\nDie „Information“ debütierte in der Nachkriegsgeschichte zweimal unabhängig voneinander: In den genetischen Lebenswissenschaften und in der Nachrichtentechnik. Darüber wundern dürfen wir uns, dass diesem doppelten Ursprung des Begriffs in der naturwissenschaftlichen Moderne eine Dopplung des antiken Bergriffs der Information entspricht. Der Philosoph Peter Janich schreibt in seinem Buch __Was ist Information?__: „Schon die Anfänge der lateinischen Herkunft von ‚Information’ sind zweigeteilt“. Und weiter: „Zum einen bedeutet ‚informare’ das handwerkliche Formen (etwa eines ‚gewaltigen Rundschilds’ durch den Schmied in der Dichtung des Vergil); und fast gleichzeitig wird Information als Inhalt eines vollständigen Satzes (etwa bei Cicero) verstanden.“\nMit anderen Worten: Sowohl Menschen als auch Materialien können informiert sein. Dieser Doppelcharakter der Information rückt aber erst Mitte der 1940er Jahre ins (fach-)öffentliche Bewusstsein. In einer Zeit, in der sich die Hinwendung der alten Biologie hin zur Molekulargenetik vollzieht und der Transistor erfunden wird – und in der beide Ereignisse „Information“ in den Mittelpunkt erst der Forschung und dann der Gesellschaft rücken.",
      "id": [
        "145777"
      ],
      "item": "Die doppelte Herkunft des Wortes „Information“"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145778]\n[%Informationsmengen]\nDie Einheit der Information ist ein Bit – Ja oder Nein, 1 oder 0, An oder Aus. Acht Bits nennt man ein Byte, und diese Einheit dient als Basis für Computer, weil ein solches Byte ausreicht, um alle Buchstaben eines Alphabets (neben den Ziffern und Sonderzeichen) zu kodieren. Die ersten Computer schafften es, 1000 Bytes zu verarbeiten oder 1 Kilobyte (KB). Bald konnte man 1000 KB speichern, und das ist ein Megabyte (MB). 1000 MB werden zu einem Gigabyte (GB), 1000 GB sind ein Terabyte (TB), und 1000 TB ergeben ein Petabyte (PB). Die Internet-Suchmaschine Google verarbeitet in jeder Stunde ein PB. Für die Gesammelten Werke von Shakespeare reichen hingegen 5 MB Speicherplatz.\nViele Hundert Terabyte an Informationen müssen inzwischen zum Beispiel die Astronomen verwalten. Im Jahre 2000 wurde im amerikanischen Bundesstaat Neumexiko ein modern ausgerüstetes Observatorium mit Teleskopen in Betrieb genommen, das die alte Aufgabe der Himmelsvermessung mit den neuen Methoden durchführte, die unser Zeitalter mit seinen Informationstechnologien zur Verfügung stellt. Im Rahmen dieses „Sloan Digital Sky Survey“ sammelten die Astronomen und ihre Geräte – vor allem Computer – in den ersten Wochen mehr Daten ein als sämtliche Sternengucker in der gesamten langen vorausgegangenen Geschichte der Astronomie von den Babyloniern über die Griechen und die Meister des 17. Jahrhunderts – unter anderem Galileo Galilei und Johannes Kepler – bis zu den Zeitgenossen Albert Einsteins und darüber hinaus.\nEin zweites Beispiel: Wenn der neue Beschleuniger mit Namen LHC (Large Hadron Collider) am europäischen Forschungszentrum CERN in Genf in Betrieb ist und die angestrebten Energien erreicht, derentwegen er gebaut worden ist und mit deren Hilfe elementare Teilchen zertrümmert werden, um ihre innere Struktur zu erkunden, dann sammeln die dort arbeitenden Forscher mit ihren Computern jeden Tag (!) so viele Informationen ein, wie in 800 Millionen (!) Büchern mit einem Umfang von jeweils 400 Seiten untergebracht werden kann.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Die Einheit der Information ist ein Bit – Ja oder Nein, 1 oder 0, An oder Aus. Acht Bits nennt man ein Byte, und diese Einheit dient als Basis für Computer, weil ein solches Byte ausreicht, um alle Buchstaben eines Alphabets (neben den Ziffern und Sonderzeichen) zu kodieren. Die ersten Computer schafften es, 1000 Bytes zu verarbeiten oder 1 Kilobyte (KB). Bald konnte man 1000 KB speichern, und das ist ein Megabyte (MB). 1000 MB werden zu einem Gigabyte (GB), 1000 GB sind ein Terabyte (TB), und 1000 TB ergeben ein Petabyte (PB). Die Internet-Suchmaschine Google verarbeitet in jeder Stunde ein PB. Für die Gesammelten Werke von Shakespeare reichen hingegen 5 MB Speicherplatz.\nViele Hundert Terabyte an Informationen müssen inzwischen zum Beispiel die Astronomen verwalten. Im Jahre 2000 wurde im amerikanischen Bundesstaat Neumexiko ein modern ausgerüstetes Observatorium mit Teleskopen in Betrieb genommen, das die alte Aufgabe der Himmelsvermessung mit den neuen Methoden durchführte, die unser Zeitalter mit seinen Informationstechnologien zur Verfügung stellt. Im Rahmen dieses „Sloan Digital Sky Survey“ sammelten die Astronomen und ihre Geräte – vor allem Computer – in den ersten Wochen mehr Daten ein als sämtliche Sternengucker in der gesamten langen vorausgegangenen Geschichte der Astronomie von den Babyloniern über die Griechen und die Meister des 17. Jahrhunderts – unter anderem Galileo Galilei und Johannes Kepler – bis zu den Zeitgenossen Albert Einsteins und darüber hinaus.\nEin zweites Beispiel: Wenn der neue Beschleuniger mit Namen LHC (Large Hadron Collider) am europäischen Forschungszentrum CERN in Genf in Betrieb ist und die angestrebten Energien erreicht, derentwegen er gebaut worden ist und mit deren Hilfe elementare Teilchen zertrümmert werden, um ihre innere Struktur zu erkunden, dann sammeln die dort arbeitenden Forscher mit ihren Computern jeden Tag (!) so viele Informationen ein, wie in 800 Millionen (!) Büchern mit einem Umfang von jeweils 400 Seiten untergebracht werden kann.",
      "id": [
        "145778"
      ],
      "item": "Informationsmengen"
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145779]\n[%1]\n[#Aufbruch; Das Auftauchen des Konzepts Information]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "1",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 1,
      "progress": true,
      "toc": true,
      "id": [
        "145779"
      ],
      "title": "Aufbruch; Das Auftauchen des Konzepts Information"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145780]\n[%Das Konzept Information entsteht]\nDas uns vertraute Reden und Nachdenken über „Information“ fängt kurz nach dem Ende des Zweiten Weltkriegs an. Damals tauchen Überlegungen zu diesem Konzept und bald auch der Begriff selbst erst in der Wissenschaft vom Leben und dann in der Nachrichtentechnik auf. Um die Mitte des 20. Jahrhunderts fügen dann amerikanische Wissenschaftler um den genialen Mathematiker Norbert Wiener am legendären Massachusetts Institute of Technology (MIT) in Boston beide Aspekte zusammen und prognostizieren, dass die industrialisierte Welt sich auf dem Weg in eine Informationsgesellschaft befinde.\nSie beginnen mit dem Entwurf von elektronischen Rechengeräten, die das Beschreiten dieses Wegs erleichtern sollen, und konstruieren Maschinen, die ihre Tätigkeit nach der Information richten können, die ihnen zufließt. Wiener erfindet das, was wir früher noch mit „Rückkopplung“ übersetzten, heute aber auch auf Deutsch „Feedback“ nennen. Er beschreibt sein Vorhaben in dem 1948 zum ersten Mal erschienenen (und seitenweise mit komplizierten mathematischen Formeln vollgestopften) Buch über „Regelung und Nachrichtenübertragung im Lebewesen und in der Maschine“, dem er den Titel __Kybernetik__ (nach dem griechischen Wort für „Steuermann“) gibt. Darin beginnt er, einen Zusammengang von „Information, Sprache und Gesellschaft“ mehr als nur zu skizzieren, nämlich zu konstruieren.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Das uns vertraute Reden und Nachdenken über „Information“ fängt kurz nach dem Ende des Zweiten Weltkriegs an. Damals tauchen Überlegungen zu diesem Konzept und bald auch der Begriff selbst erst in der Wissenschaft vom Leben und dann in der Nachrichtentechnik auf. Um die Mitte des 20. Jahrhunderts fügen dann amerikanische Wissenschaftler um den genialen Mathematiker Norbert Wiener am legendären Massachusetts Institute of Technology (MIT) in Boston beide Aspekte zusammen und prognostizieren, dass die industrialisierte Welt sich auf dem Weg in eine Informationsgesellschaft befinde.\nSie beginnen mit dem Entwurf von elektronischen Rechengeräten, die das Beschreiten dieses Wegs erleichtern sollen, und konstruieren Maschinen, die ihre Tätigkeit nach der Information richten können, die ihnen zufließt. Wiener erfindet das, was wir früher noch mit „Rückkopplung“ übersetzten, heute aber auch auf Deutsch „Feedback“ nennen. Er beschreibt sein Vorhaben in dem 1948 zum ersten Mal erschienenen (und seitenweise mit komplizierten mathematischen Formeln vollgestopften) Buch über „Regelung und Nachrichtenübertragung im Lebewesen und in der Maschine“, dem er den Titel __Kybernetik__ (nach dem griechischen Wort für „Steuermann“) gibt. Darin beginnt er, einen Zusammengang von „Information, Sprache und Gesellschaft“ mehr als nur zu skizzieren, nämlich zu konstruieren.",
      "id": [
        "145780"
      ],
      "item": "Das Konzept Information entsteht"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145781]\n[%Norbert Wiener]\nNorbert Wiener wurde 1894 in Columbia, Missouri geboren. Er studierte in Harvard, Cambridge und Göttingen Naturwissenschaften und Philosophie und arbeitete den größten Teil seines Lebens für das berühmte MIT. Während des Zweiten Weltkriegs war er mit der Zielsteuerung von Geschützen befasst. Auch aufgrund dieser Arbeiten gelangte er zu einer allgemeinen Informationstheorie (Kybernetik), die er auch in Zusammenhang mit älteren philosophischen Konzepten brachte. Wiener starb 1964 in Stockholm.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Norbert Wiener wurde 1894 in Columbia, Missouri geboren. Er studierte in Harvard, Cambridge und Göttingen Naturwissenschaften und Philosophie und arbeitete den größten Teil seines Lebens für das berühmte MIT. Während des Zweiten Weltkriegs war er mit der Zielsteuerung von Geschützen befasst. Auch aufgrund dieser Arbeiten gelangte er zu einer allgemeinen Informationstheorie (Kybernetik), die er auch in Zusammenhang mit älteren philosophischen Konzepten brachte. Wiener starb 1964 in Stockholm.",
      "id": [
        "145781"
      ],
      "item": "Norbert Wiener"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145782]\n[%Eine geheimnisvolle Größe im Hintergrund]\nFür den Mathematiker Wiener und seine Mitstreiter tritt bereits um die Mitte des 20. Jahrhunderts der Informationsgehalt eines Systems – lebendig oder nicht – gleichberechtigt neben seine Energie oder sein Material, und sie wissen auch, ihn zu definieren. Sie betrachten die Information als ein Maß für den Grad, den man an Ordnung in einem System finden kann, wobei man sich die Ordnung naiv denken kann wie etwa das, was man auf seinem Schreibtisch erneut herstellt, nachdem man mit der Arbeit fertig ist.\nMit diesem Gedanken schließt das neue Denken über Information an die ungeheuer erfolgreiche Physik des 19. Jahrhunderts an. Es geht dabei um den Teil der Physik, der heute als Wärmelehre oder Thermodynamik bezeichnet wird. Diese Disziplin war nach 1850 damit beschäftigt, das Wechselspiel von Ordnung und Unordnung zu erkunden, das bei verschiedenen Systemen zu beobachten war und sie zu charakterisieren half. Um einen jeweils vorhandenen Ordnungszustand berechnen oder messen zu können, führten die Physiker den Ausdruck „Entropie“ in ihre Wissenschaft ein.\nEntropie war als ein Kunstwort konzipiert, das so ähnlich wie die Energie klingen sollte. Die Einführung der Entropie in die Physik war unvermeidbar gewesen, weil die Wissenschaftler ohne dieses (experimentell zugängliche) Konzept nicht zu sagen vermochten, welcher Anteil von Energie, die man einer Maschine – zunächst ging es um Dampfmaschinen – etwa in Form von Wärme zuführte, in die Arbeit verwandelt werden kann, um derentwillen man sie gebaut hatte – Heben, Transportieren, Pressen, Bohren, Schneiden –, und welcher Anteil der Energie ungenutzt blieb und nur ihre Bauteile verlustreich aufheizte.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Für den Mathematiker Wiener und seine Mitstreiter tritt bereits um die Mitte des 20. Jahrhunderts der Informationsgehalt eines Systems – lebendig oder nicht – gleichberechtigt neben seine Energie oder sein Material, und sie wissen auch, ihn zu definieren. Sie betrachten die Information als ein Maß für den Grad, den man an Ordnung in einem System finden kann, wobei man sich die Ordnung naiv denken kann wie etwa das, was man auf seinem Schreibtisch erneut herstellt, nachdem man mit der Arbeit fertig ist.\nMit diesem Gedanken schließt das neue Denken über Information an die ungeheuer erfolgreiche Physik des 19. Jahrhunderts an. Es geht dabei um den Teil der Physik, der heute als Wärmelehre oder Thermodynamik bezeichnet wird. Diese Disziplin war nach 1850 damit beschäftigt, das Wechselspiel von Ordnung und Unordnung zu erkunden, das bei verschiedenen Systemen zu beobachten war und sie zu charakterisieren half. Um einen jeweils vorhandenen Ordnungszustand berechnen oder messen zu können, führten die Physiker den Ausdruck „Entropie“ in ihre Wissenschaft ein.\nEntropie war als ein Kunstwort konzipiert, das so ähnlich wie die Energie klingen sollte. Die Einführung der Entropie in die Physik war unvermeidbar gewesen, weil die Wissenschaftler ohne dieses (experimentell zugängliche) Konzept nicht zu sagen vermochten, welcher Anteil von Energie, die man einer Maschine – zunächst ging es um Dampfmaschinen – etwa in Form von Wärme zuführte, in die Arbeit verwandelt werden kann, um derentwillen man sie gebaut hatte – Heben, Transportieren, Pressen, Bohren, Schneiden –, und welcher Anteil der Energie ungenutzt blieb und nur ihre Bauteile verlustreich aufheizte.",
      "id": [
        "145782"
      ],
      "item": "Eine geheimnisvolle Größe im Hintergrund"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145783]\n[%Der ENIAC – „Ursprünge der Zukunft“]\nKurz vor dem Ende des Zweiten Weltkriegs stellte der aus Ungarn stammende Mathematiker John von Neumann das Prinzip eines programmierbaren universellen Rechners vor. Diesen Vorgang kann man gut und gerne als Geburtsstunde des modernen Computers mit einer Programmspeicherung bezeichnen.\nVon Neumann hatte 1944 eine Rechenmaschine entworfen, die er „Electronic Discrete Variable Automatic Computer“ nannte  und EDVAC abkürzte. Die Konstruktion sollte zwar erst 1952 funktionieren, doch mit ihrer Architektur definierte von Neumann genau die Elemente, die einen modernen Computer ausmachen und deren Namen uns vertraut sind: Da gibt es ein Rechenwerk, ein Steuerwerk, einen Speicher und Vorrichtungen für die Ein- und Ausgabe von Daten oder Informationen.\nVon Neumann kam 1930 in die USA und arbeitete seit 1943 am Manhattan-Projekt mit, das die Entwicklung einer Atombombe zum Ziel hatte. Die Arbeit an der Bombe, insbesondere das Verständnis der bei der Explosion ablaufenden Reaktionen, verlangte von den Beteiligten Berechnungen („Computations“) in einem zuvor unbekannten Ausmaß und rief unter ihnen den dringenden Wunsch nach Maschinen hervor, denen man diese Knochenarbeit übergeben könnte. Auch die konventionelle Kriegsführung verlangte ausgedehnte Rechenleistungen wie das dauernde Multiplizieren von zehn- und mehrstelligen Zahlen, um zum Beispiel die Reichweite von Bombern bei schwankenden Windstärken und -richtungen ausfindig zu machen oder um bessere Feuertabellen für die Artillerie anfertigen zu können.\nSo galt es, die Rechengeschwindigkeit in den vorhandenen Geräten und diese selbst zu vergrößern; von 1945 an kam zu diesem Zweck ein Monstrum namens ENIAC zum Einsatz – eine elektronische Rechenanlage, wie der vollständige Name zu erkennen gibt: Electronic Numerical Integrator and Calculator. ENIAC war mit 18.000 Elektronenröhren ausgestattet, die gesamte Konstruktion wog rund 30 Tonnen und nahm eine Standfläche von 140 m2 ein.\nDer maßgeblich von dem Physiker John W. Mauchley und dem Ingenieur John P. Eckert konstruierte ENIAC war 1000-mal schneller als alle anderen Rechenmaschinen ihrer Zeit – schneller auch als der berühmte Prototyp Z3 des in Deutschland tätigen und von seinen Landsleuten lange Zeit ignorierten Ingenieurs Konrad Zuse –, aber heute nötigen uns die stolzen 5000 Additionen und 300 Multiplikationen, die ENIAC pro Sekunde zustande brachte, nur noch ein müdes Lächeln ab.\nMit dem ENIAC „wechselt die Entwicklung elektrischer Rechner von einer Phase der Vorbereitung in jene Hauptphase ein, die durch wissenschaftliche Modellbildung, immer raschere technologische Innovationsschübe und massenhafte Erzeugung und Nutzung gekennzeichnet ist“, schreibt der Technikhistoriker Karl H. Metz in seiner „Geschichte der Technik in der westlichen Zivilisation“, die er __Ursprünge der Zukunft__ nennt. „Während die Leistungsfähigkeit rasch ansteigt“, fährt er fort, „nehmen Größe des Geräts und Preis entsprechend ab. Ursache ist die Systematisierung der Forschung und deren schnelle technische Umsetzung“.\nIn der Tat – von 1948 an fangen die Computer an, zu einem bedeutenden Faktor der Wirtschaft zu werden, obwohl der ENIAC noch rund 10 Millionen Dollar kostete. Es ist im übrigen dieser astronomische Preis, der damals einen unglücklichen Manager des heute längst weltweit Computer verkaufenden Unternehmens IBM zu dem ständig hämisch zitierten Satz veranlasste, der weltweite Bedarf an Computern liege seines Erachtens nach bei fünf bis sechs Stück. Wer konnte damals ahnen, dass IBM und andere Unternehmen in Zukunft einmal Hunderttausende oder gar Millionen von erschwinglichen und tragbaren Computern pro Monat an den Kunden bringen würden?",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Kurz vor dem Ende des Zweiten Weltkriegs stellte der aus Ungarn stammende Mathematiker John von Neumann das Prinzip eines programmierbaren universellen Rechners vor. Diesen Vorgang kann man gut und gerne als Geburtsstunde des modernen Computers mit einer Programmspeicherung bezeichnen.\nVon Neumann hatte 1944 eine Rechenmaschine entworfen, die er „Electronic Discrete Variable Automatic Computer“ nannte  und EDVAC abkürzte. Die Konstruktion sollte zwar erst 1952 funktionieren, doch mit ihrer Architektur definierte von Neumann genau die Elemente, die einen modernen Computer ausmachen und deren Namen uns vertraut sind: Da gibt es ein Rechenwerk, ein Steuerwerk, einen Speicher und Vorrichtungen für die Ein- und Ausgabe von Daten oder Informationen.\nVon Neumann kam 1930 in die USA und arbeitete seit 1943 am Manhattan-Projekt mit, das die Entwicklung einer Atombombe zum Ziel hatte. Die Arbeit an der Bombe, insbesondere das Verständnis der bei der Explosion ablaufenden Reaktionen, verlangte von den Beteiligten Berechnungen („Computations“) in einem zuvor unbekannten Ausmaß und rief unter ihnen den dringenden Wunsch nach Maschinen hervor, denen man diese Knochenarbeit übergeben könnte. Auch die konventionelle Kriegsführung verlangte ausgedehnte Rechenleistungen wie das dauernde Multiplizieren von zehn- und mehrstelligen Zahlen, um zum Beispiel die Reichweite von Bombern bei schwankenden Windstärken und -richtungen ausfindig zu machen oder um bessere Feuertabellen für die Artillerie anfertigen zu können.\nSo galt es, die Rechengeschwindigkeit in den vorhandenen Geräten und diese selbst zu vergrößern; von 1945 an kam zu diesem Zweck ein Monstrum namens ENIAC zum Einsatz – eine elektronische Rechenanlage, wie der vollständige Name zu erkennen gibt: Electronic Numerical Integrator and Calculator. ENIAC war mit 18.000 Elektronenröhren ausgestattet, die gesamte Konstruktion wog rund 30 Tonnen und nahm eine Standfläche von 140 m2 ein.\nDer maßgeblich von dem Physiker John W. Mauchley und dem Ingenieur John P. Eckert konstruierte ENIAC war 1000-mal schneller als alle anderen Rechenmaschinen ihrer Zeit – schneller auch als der berühmte Prototyp Z3 des in Deutschland tätigen und von seinen Landsleuten lange Zeit ignorierten Ingenieurs Konrad Zuse –, aber heute nötigen uns die stolzen 5000 Additionen und 300 Multiplikationen, die ENIAC pro Sekunde zustande brachte, nur noch ein müdes Lächeln ab.\nMit dem ENIAC „wechselt die Entwicklung elektrischer Rechner von einer Phase der Vorbereitung in jene Hauptphase ein, die durch wissenschaftliche Modellbildung, immer raschere technologische Innovationsschübe und massenhafte Erzeugung und Nutzung gekennzeichnet ist“, schreibt der Technikhistoriker Karl H. Metz in seiner „Geschichte der Technik in der westlichen Zivilisation“, die er __Ursprünge der Zukunft__ nennt. „Während die Leistungsfähigkeit rasch ansteigt“, fährt er fort, „nehmen Größe des Geräts und Preis entsprechend ab. Ursache ist die Systematisierung der Forschung und deren schnelle technische Umsetzung“.\nIn der Tat – von 1948 an fangen die Computer an, zu einem bedeutenden Faktor der Wirtschaft zu werden, obwohl der ENIAC noch rund 10 Millionen Dollar kostete. Es ist im übrigen dieser astronomische Preis, der damals einen unglücklichen Manager des heute längst weltweit Computer verkaufenden Unternehmens IBM zu dem ständig hämisch zitierten Satz veranlasste, der weltweite Bedarf an Computern liege seines Erachtens nach bei fünf bis sechs Stück. Wer konnte damals ahnen, dass IBM und andere Unternehmen in Zukunft einmal Hunderttausende oder gar Millionen von erschwinglichen und tragbaren Computern pro Monat an den Kunden bringen würden?",
      "id": [
        "145783"
      ],
      "item": "Der ENIAC – „Ursprünge der Zukunft“"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145784]\n[%John von Neumann]\nJohn von Neumann, einer der bedeutendsten Mathematiker des 20. Jahrhunderts, wurde 1903 in Budapest geboren. Er stammte aus einer jüdischen Bankiersfamilie. Er studierte in Berlin, Zürich und Budapest und lehrte und forschte in Berlin, Hamburg und Göttingen. Er arbeitete zur Quantenmechanik und entwickelte die Spieltheorie, d.h. mathematisch begründete Strategien in sozialen Situationen, etwa bei Gesellschaftsspielen oder im Wirtschaftsleben. 1930 emigrierte von Neumann in die USA. Er starb 1957.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "John von Neumann, einer der bedeutendsten Mathematiker des 20. Jahrhunderts, wurde 1903 in Budapest geboren. Er stammte aus einer jüdischen Bankiersfamilie. Er studierte in Berlin, Zürich und Budapest und lehrte und forschte in Berlin, Hamburg und Göttingen. Er arbeitete zur Quantenmechanik und entwickelte die Spieltheorie, d.h. mathematisch begründete Strategien in sozialen Situationen, etwa bei Gesellschaftsspielen oder im Wirtschaftsleben. 1930 emigrierte von Neumann in die USA. Er starb 1957.",
      "id": [
        "145784"
      ],
      "item": "John von Neumann"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145785]\n[%John William Mauchley und John Presper Eckert]\nJohn William Mauchley (1907-1980) und John Presper Eckert (1919-1995) gründeten nach der Entwicklung des ENIAC eine Firma, die 1951 den ersten kommerziellen Computer in den USA, UNIVAC, auslieferte.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "John William Mauchley (1907-1980) und John Presper Eckert (1919-1995) gründeten nach der Entwicklung des ENIAC eine Firma, die 1951 den ersten kommerziellen Computer in den USA, UNIVAC, auslieferte.",
      "id": [
        "145785"
      ],
      "item": "John William Mauchley und John Presper Eckert"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145786]\n[%Konrad Zuse]\nDer deutsche Computerpionier Konrad Zuse (1910-1995) entwickelte zuerst Rechenmaschinen für die deutsche Luftfahrtindustrie. 1938 stellte er den programmgesteuerten mechanischen Rechner Z1 fertig. Z2 von 1940 arbeitete mit Telefonrelais als Schaltern. Z3 von 1941 gilt als erster funktionstüchtiger Computer überhaupt, und Z4, den Zuse 1949 an die Eidgenössische Technische Hochschule verkaufen konnte, war – vor dem UNIVAC – der erste kommerzielle Computer der Welt.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der deutsche Computerpionier Konrad Zuse (1910-1995) entwickelte zuerst Rechenmaschinen für die deutsche Luftfahrtindustrie. 1938 stellte er den programmgesteuerten mechanischen Rechner Z1 fertig. Z2 von 1940 arbeitete mit Telefonrelais als Schaltern. Z3 von 1941 gilt als erster funktionstüchtiger Computer überhaupt, und Z4, den Zuse 1949 an die Eidgenössische Technische Hochschule verkaufen konnte, war – vor dem UNIVAC – der erste kommerzielle Computer der Welt.",
      "id": [
        "145786"
      ],
      "item": "Konrad Zuse"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145787]\n[%Energie und Entropie – Die Geburt der Information]\n„Energie“ stammt vom altgriechischen Wort „energeia“, das man mit „Wirksamkeit“ oder „Tatkraft“ übersetzen könnte. In der Physik meint Energie die Fähigkeit, Arbeit zu verrichten. Es gibt einen (ersten) Hauptsatz der Thermodynamik, demzufolge die Energie in einem geschlossenen System (etwa dem Weltall) konstant ist. Energie kann in verschiedenen Formen auftreten, aber dabei weder vernichtet noch neu erzeugt zu werden. Sie kann nur verwandelt werden – etwa von Wärme- in Bewegungsenergie. Energiekonzerne schaffen in ihren Kraftwerken keine neue Energie, sondern wandeln zum Beispiel lediglich die in fossilen Brennstoffen gespeicherte Energie in elektrischen Strom um.\n„Entropie“ wurde in Anlehnung an „Energie“ als Kunstwort gebildet, wobei in den beiden Endsilben das griechische „trope“ steckt, also Umwandlung oder Wendung. Entropie handelt von Änderungen eines gegebenen Systems. Bei konstanter Energie nimmt in jedem System – jedenfalls in der Physik – mit jeder Änderung die Unordnung zu. Bei einem Verbrennungsvorgang etwa verwandelt sich die in Kohlenstoffmolekülen organisierte potentielle Energie des fossilen Brennstoffs in Asche und diffuse Wärme. Es gibt einen (zweiten) Hauptsatz der Thermodynamik, der besagt, dass die Entropie wächst und einem Maximum – der völligen Unordnung – zustrebt. Dadurch bekommt die Zeit eine Richtung.\n\n__Eine zweite industrielle Revolution__\nDie Geburt der Information findet also aus dem Geist der Entropie statt, und dies führt in den Jahren nach dem Zweiten Weltkrieg nicht nur zu neuen interdisziplinären Wissenschaften – der Kybernetik und der Molekularbiologie –, es führt auch zu einer neuen Gesellschaft. So sehen es jedenfalls die Forscher um Norbert Wiener, die sich als Nachfolger der Helden der ersten Industriellen Revolution des 18. Jahrhundert, eines James Watt etwa, betrachteten. Damals war es gelungen, Apparate wie Dampfmaschinen zu konstruieren, die den Menschen viele körperliche Arbeiten abnehmen konnten. In der Mitte der 1940er Jahre aber besteht nach Ansicht der Mathematiker und Ingenieure erstmals die Chance, Maschinen zu konstruieren, die für Menschen Berechnungen durchführen und andere intelligente Aufgaben können. Wiener prägt für die Einführung der elektronischen Datenverarbeitung und der dazugehörigen Computer den Ausdruck „Zweite Industrielle Revolution“, und er und seine Zeitgenossen können darin nur einen gesellschaftlichen Fortschritt sehen, denn „eine Zivilisation schreitet durch die Zahl der wichtigen Operationen voran, die [ihre Mitglieder] ausführen können, ohne darüber nachdenken zu müssen“, wie Wiener einen wichtigen Tatbestand ausdrückt. \nTatsächlich schreitet die westliche Welt voran, und in den frühen 1960er Jahren kommt hier der Begriff der Informationsgesellschaft auf. Bald können wir viele Bücher lesen, in denen von einer „informierten Gesellschaft“ die Rede ist, die sich seit 1968 auch eine neue Wissenschaft namens „Informatik“ leistet. Wissenschaft und Technik sowie eine Industrie, die deren Erkenntnisse verwerten, ändern die Gesellschaft in der Tat radikal – ohne dass dies in der Geschichtsschreibung eine allzu deutliche Spur hinterlassen hätte. Offenbar merken viele Historiker nicht oder wollen nicht zur Kenntnis nehmen, was der französische Philosoph Michel Serres 1994 in seinem Vorwort zu den von ihm herausgegebenen __Elementen einer Geschichte der Wissenschaften__ so beschrieben hat: „Weder die Wechselfälle der politischen oder militärischen Verhältnisse noch die Ökonomie können – für sich genommen – hinreichend erklären, wie sich unsere heutigen Lebensweisen durchgesetzt haben; dazu bedarf es einer __Geschichte der Wissenschaften und Techniken__ ... “ – oder zumindest eines Interesses der professionellen Historiker an Wissenschaft und Technik.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "„Energie“ stammt vom altgriechischen Wort „energeia“, das man mit „Wirksamkeit“ oder „Tatkraft“ übersetzen könnte. In der Physik meint Energie die Fähigkeit, Arbeit zu verrichten. Es gibt einen (ersten) Hauptsatz der Thermodynamik, demzufolge die Energie in einem geschlossenen System (etwa dem Weltall) konstant ist. Energie kann in verschiedenen Formen auftreten, aber dabei weder vernichtet noch neu erzeugt zu werden. Sie kann nur verwandelt werden – etwa von Wärme- in Bewegungsenergie. Energiekonzerne schaffen in ihren Kraftwerken keine neue Energie, sondern wandeln zum Beispiel lediglich die in fossilen Brennstoffen gespeicherte Energie in elektrischen Strom um.\n„Entropie“ wurde in Anlehnung an „Energie“ als Kunstwort gebildet, wobei in den beiden Endsilben das griechische „trope“ steckt, also Umwandlung oder Wendung. Entropie handelt von Änderungen eines gegebenen Systems. Bei konstanter Energie nimmt in jedem System – jedenfalls in der Physik – mit jeder Änderung die Unordnung zu. Bei einem Verbrennungsvorgang etwa verwandelt sich die in Kohlenstoffmolekülen organisierte potentielle Energie des fossilen Brennstoffs in Asche und diffuse Wärme. Es gibt einen (zweiten) Hauptsatz der Thermodynamik, der besagt, dass die Entropie wächst und einem Maximum – der völligen Unordnung – zustrebt. Dadurch bekommt die Zeit eine Richtung.\n\n__Eine zweite industrielle Revolution__\nDie Geburt der Information findet also aus dem Geist der Entropie statt, und dies führt in den Jahren nach dem Zweiten Weltkrieg nicht nur zu neuen interdisziplinären Wissenschaften – der Kybernetik und der Molekularbiologie –, es führt auch zu einer neuen Gesellschaft. So sehen es jedenfalls die Forscher um Norbert Wiener, die sich als Nachfolger der Helden der ersten Industriellen Revolution des 18. Jahrhundert, eines James Watt etwa, betrachteten. Damals war es gelungen, Apparate wie Dampfmaschinen zu konstruieren, die den Menschen viele körperliche Arbeiten abnehmen konnten. In der Mitte der 1940er Jahre aber besteht nach Ansicht der Mathematiker und Ingenieure erstmals die Chance, Maschinen zu konstruieren, die für Menschen Berechnungen durchführen und andere intelligente Aufgaben können. Wiener prägt für die Einführung der elektronischen Datenverarbeitung und der dazugehörigen Computer den Ausdruck „Zweite Industrielle Revolution“, und er und seine Zeitgenossen können darin nur einen gesellschaftlichen Fortschritt sehen, denn „eine Zivilisation schreitet durch die Zahl der wichtigen Operationen voran, die [ihre Mitglieder] ausführen können, ohne darüber nachdenken zu müssen“, wie Wiener einen wichtigen Tatbestand ausdrückt. \nTatsächlich schreitet die westliche Welt voran, und in den frühen 1960er Jahren kommt hier der Begriff der Informationsgesellschaft auf. Bald können wir viele Bücher lesen, in denen von einer „informierten Gesellschaft“ die Rede ist, die sich seit 1968 auch eine neue Wissenschaft namens „Informatik“ leistet. Wissenschaft und Technik sowie eine Industrie, die deren Erkenntnisse verwerten, ändern die Gesellschaft in der Tat radikal – ohne dass dies in der Geschichtsschreibung eine allzu deutliche Spur hinterlassen hätte. Offenbar merken viele Historiker nicht oder wollen nicht zur Kenntnis nehmen, was der französische Philosoph Michel Serres 1994 in seinem Vorwort zu den von ihm herausgegebenen __Elementen einer Geschichte der Wissenschaften__ so beschrieben hat: „Weder die Wechselfälle der politischen oder militärischen Verhältnisse noch die Ökonomie können – für sich genommen – hinreichend erklären, wie sich unsere heutigen Lebensweisen durchgesetzt haben; dazu bedarf es einer __Geschichte der Wissenschaften und Techniken__ ... “ – oder zumindest eines Interesses der professionellen Historiker an Wissenschaft und Technik.",
      "id": [
        "145787"
      ],
      "item": "Energie und Entropie – Die Geburt der Information"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145788]\n[%Der Transistor]\nZur Kommerzialisierung des Computers ist es natürlich nicht über Nacht gekommen. Der große Durchbruch war auch der nächste große Computer noch nicht, den die IBM-Ingenieure 1946/47 entwickelten. Sie nannten ihn SSEC – Selective Sequence Electronic Calculator – und bestückten ihn mit 12 500 Röhren und mehr als 20 000 mechanischen Relais. Der Rechner SSEC, der die Größe eines Fußballfeldes einnahm, konnte jedoch immerhin schon, was der Vordenker von Neumann mit seinen getrennten Bauelementen theoretisch anvisiert hatte, nämlich ein neues Rechenprogramm beginnen, bevor das alte abgearbeitet war. Den SSEC kann man daher als Übergang „von einer reinen Rechenvorrichtung zu einem Gerät für die allgemeine Verarbeitung von Information“ betrachten, wie Metz es formuliert. Der SSEC wurde bereits um 1950 eingesetzt, um Mondpositionen zu berechnen. Mit diesen Informationen konnten im Rahmen des berühmten, in den 1960er Jahren von der amerikanischen Weltraumbehörde NASA durchgeführten Apollo-Projekts die Landepositionen der anfänglich noch unbemannten Raumfähren ausgewählt werden. Für jeden Landeplatz mussten Zehntausende von Rechenoperationen vorgenommen und mit umfangreichen Suchanfragen an eine Datenbank gekoppelt werden, die Informationen über die Mondoberfläche gespeichert hatte. Die IBM-Maschine konnte diese Kalkulationen in ein paar Minuten erledigen, und führte ihre Rechenkapazität spektakulär im Beisein von einigen aufgeregten Zuschauern vor.\nHeutige Maschinen brauchen bekanntlich für solche und noch kompliziertere Berechnungen nur noch Bruchteile von Sekunden, was unmittelbar die Frage aufwirft, wodurch die bis heute anhaltend rasante Entwicklung der Rechen- und Speicherkapazitäten von Computern bei gleichzeitig erfolgender Miniaturisierung möglich geworden ist. Wieso kann der kleine Laptop auf meinem schmalen Schreibtisch heute so unendlich viel mehr und unglaublich viel schneller als damals zum Beispiel das Riesending SSEC in seiner Riesenhalle?\nEin wesentlicher Teil der Antwort auf diese Frage liegt darin, dass die voluminösen und reparaturanfälligen Elektronenröhren in den Rechenmaschinen durch kleine und höchst zuverlässige Transistoren abgelöst wurden. Erfunden worden ist das Bauprinzip für diese Wunderdinger im Dezember 1947, und zwar in den Bell-Laboratorien in New Jersey. Hier versuchten drei in den 1950er Jahren mit dem Nobelpreis für ihr Fach ausgezeichnete Physiker – William Shockley, John Bardeen und Walter Brattain – systematisch zu erforschen, was in den Jahren des Zweiten Weltkriegs eher nebenbei erkundet worden war, nämlich die Eigenschaften von Kristallen oder Festkörpern, die man Halbleiter nennt. Was in der Geschichte der Physik erst nur Unverständnis und Langeweile hervorgerufen hatte – was sollte man auch mit Elementen wie Silizium und Germanium anfangen, die manchmal elektrischen Strom leiteten und manchmal nicht? –, war im Rahmen von Arbeiten (mit militärischer Zielsetzung) zur Radartechnik in den Blickpunkt des wissenschaftlichen Interesses gerückt. Man benötigte möglichst empfindliche Empfänger (Detektoren) für oftmals extrem schwache Signale, und eines Tages muss jemand unter den Physikern auf den Gedanken gekommen sein, dass Halbleiter genau dazu dienen konnten. Die Verwandlung vom Isolator zum Leiter setzt nämlich bei einigen Halbleiter-Kristallen höchst plötzlich ein, also schon bei geringsten Änderungen der äußeren Bedingungen – etwa der Temperatur oder der durch Strahlung zugeführten Energie –, und wenn man diese kleinen Schwankungen erkunden und vermessen wollte, konnte man ja Halbleiter als Detektoren einsetzen.\nNach ersten tastenden Bemühungen vor 1945 nahmen die – auch nach Kriegsende noch vom amerikanischen Militär finanzierten – Bell-Laboratorien die Entwicklung von Halbleitern als Schaltern systematisch in Angriff, und im Dezember 1947 gab es den ersten Transistor. Die Bezeichnung Transistor ist ein englisches Kunstwort, das sich aus zwei Teilen zusammensetzt, aus Transfer (Übertragung) und Resistance (Widerstand) nämlich. So ein „Übertragungswiderstand“ konnte – bei geeigneter Bausweise – Strom abblocken oder verstärken. Er lieferte nicht nur das, was die alte Elektronenröhre konnte – er tat dies besser und auch zuverlässiger, und er war darüber hinaus sehr viel kleiner und billiger herzustellen.\nDie zentrale Figur der Transistorentwicklung war John Bardeen. Als er seine Stelle bei den „Bell Labs“ im Oktober 1945 antrat, traf er dort mit dem Experimentalphysiker Walter Brattain zusammen, der hier seit 1929 beschäftigt war. Ihre gemeinsame Aufgabe bestand darin, mit Halbleitern die elektronischen Effekte zu erzielen, die bislang mithilfe Vakuumröhren erreicht wurden, in denen Strom durch geeignete Elemente (Kathode, Anode, Gitter) steuerbar gemacht wurde. Damit konnte man in der Praxis unter anderem Verstärker oder Empfänger konstruieren, also Radiogeräte bauen. Solche Vakuumröhren gab es bereits seit dem 19. Jahrhundert.\nDie Suche nach Alternativen zur Vakuumröhre hatte schon früh die Aufmerksamkeit auf Halbleiter gelenkt, da sie zumindest so beeinflusst werden konnten, dass sie etwa als Gleichrichter agierten und Strom nur in eine Richtung durchließen. Dies wusste man bereits seit dem Ende des 19. Jahrhunderts. In der Folge lernten die Physiker, wie sich Halbleiter nach Wunsch herstellen ließen, und als Bardeen bei Bell anfing, konnte man endlich auch erklären, was in den Halbleitern passierte. Man nutzte zur Erklärung das sogenannte Bändermodell der Festkörperphysik und bemühte sich, mit seiner Hilfe Situationen auszudenken und herzustellen, in denen das Leitungsband eines Halbleiters leicht oder schwer zu füllen war.\nDer Halbleiter, der in Bardeens Tagen in den Bell-Laboratorien am meisten Interesse fand, ist als Silizium bekannt (und wird in den Chemiebüchern mit c als Silicium geschrieben). Silizium findet sich zum Beispiel im Sand, der chemisch vorwiegend aus dem Stoff Siliciumdioxid besteht, das in reiner Form auch als Quarz bekannt ist. Das chemische Element heißt auf Englisch „silicon“, und das berühmte Silicon Valley, das bei San Francisco liegt und in den 1970er Jahren die Wiege der amerikanischen Computerindustrie wurde, trägt seinen Namen (wobei man sich davor hüten muss, aus dem amerikanischen „silicon“ bei einer Übersetzung das deutsche Silikon zu machen).\nWenn man sich das Silizium als Atom vorstellt, um sich seine Bedeutung und Einsatzfähigkeit zu erklären, kommt es auf die vier Elektronen an, die seine äußere Schale ausmachen. In einem Gitter (Kristall) aus Silizium befinden sich diese Elektronen vornehmlich in dem Valenzband, das weit vom Leitungsband entfernt liegt, und so kommt im Normalfall kein Stromfluss zustande. Dies kann man nun entscheidend ändern, indem man ein Siliziumkristall gezielt mit einem Element ausstattet (dotiert), das über fünf Außenelektronen verfügt – zum Beispiel mit Phosphor. Jedes Phosphoratom, das in das ursprüngliche Gitter aus Silizium eingebaut wird, kann ein Elektron freigeben, und dieser Ladungsträger kann das Leitungsband des Kristalls leichter erreichen. Der Kristall wird jetzt als dotiert bezeichnet – in unserem Fall Silizium mit Phosphor dotiert. Umgekehrt kann auch ein Element eingefügt werden, das statt der vier nur drei Außenelektronen hat – etwa Aluminium –, was dazu führt, dass bei dieser Dotierung eine Art Loch entsteht, das sich aber auch bewegen kann. Es verschiebt sich so, wie dies ein leerer Platz in der Mitte einer Sitzreihe tut, wenn die Menschen sich umsetzen oder nachrücken. Wenn ein Elektron zu viel da ist, sprechen die Physiker wegen dessen negativer Ladung von einem n-dotierten Halbleiter, und wenn ein Elektron zu wenig da ist und ein Loch entsteht, sprechen sie von einem p-dotierten Halbleiter. Und wenn auch jeder einzeln für sich nicht gerade als Wunderwerk anzusehen ist, so kann man mit einer geeigneten Kombination aus n- und p-Halbleitern – pnp oder npn zum Beispiel – die Welt verändern. Der Transistor ist nämlich eine solche Kombination, und der Weg zu ihm beginnt im Oktober 1945 sichtbar und konkret zu werden.\nIn diesem Monat nahmen Bardeen und Brattain bei Bell Kontakt mit einem dritten Physiker auf, dem aus London stammenden William Shockley, an dem sich bis heute die Geister scheiden. Für viele gilt Shockley als der Moses von Silicon Valley, der durch seinen unternehmerischen Geist die Grundlagen der amerikanischen Computerindustrie gelegt hat. Andere erinnern sich an seine rassistischen Ergüsse, in denen er in den 1960er Jahren beweisen wollte, dass Afroamerikaner statistisch minder intelligent sind als Amerikaner europäischer Herkunft. Auf jeden Fall war es die Zusammenarbeit von Bardeen und Brattain mit Shockley, die im Dezember 1947 zur Erfindung des Transistors führte, für die alle drei Wissenschaftler neun Jahre später den Nobelpreis erhielten.\nDie einzelnen Schritte, die Bardeen und Brattain im Dezember 1947 unternahmen, und das genaue Design ihres Prototyps, der fachlich korrekt als Spitzentransistor bezeichnet wird, müssen wir hier übergehen, weil deren Darstellung zuviel Platz erfordern würde. Es ginge dabei vor allem um die Verteilung von Ladungen an Oberflächen und ihrem theoretischen Verständnis. Entstanden ist eine Anordnung von dotierten Halbleitern in drei Schichten mit unterschiedlich dotierten Nachbarn, was entweder die Kombination pnp oder die Folge npn ergibt. Wenn alle drei Schichten Anschlüsse für elektrischen Strom haben, kann man die Schaltungen so einrichten, dass ein Signal entweder gestoppt oder verstärkt wird. Das Unterbrechen des Stroms ist einfach zu verstehen und erfolgt dann, wenn die ihn ausmachenden Elektronen auf eine n-dotierte Schicht treffen. Spannender wird es, wenn die Gegenrichtung eingeschlagen wird, das heißt, wenn Elektronen auf Löcher treffen und beide sich zusammenfinden (rekombinieren) können. Dabei kann Energie freiwerden, sogar als sichtbares Licht, was in Leuchtdioden ausgenutzt wird.\nSeine eigentliche (verstärkende) Funktion bekommt der Transistor, wenn etwa in einer npn-Anordnung ein kleiner Strom auf die mittlere Schicht geleitet wird. Sie wird heute als Basis bezeichnet und verbindet die anderen Elemente, die Emitter bzw. Kollektor heißen. Ein in der Basis von außen eintreffender kleiner Strom sorgt im Inneren der Schicht für räumliche Veränderungen (Rekombinationen) der Ladungsträger, die sich als großer Strom auf der Strecke zwischen Emitter und Kollektor bemerkbar machen. Um das auf diese Weise verstärkte Signal geht es in der Physik und der Technik, sein Auftauchen stellte das Ziel der Arbeiten am Transistor dar, und es wurde kurz vor Weihnachten 1947 erreicht.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Zur Kommerzialisierung des Computers ist es natürlich nicht über Nacht gekommen. Der große Durchbruch war auch der nächste große Computer noch nicht, den die IBM-Ingenieure 1946/47 entwickelten. Sie nannten ihn SSEC – Selective Sequence Electronic Calculator – und bestückten ihn mit 12 500 Röhren und mehr als 20 000 mechanischen Relais. Der Rechner SSEC, der die Größe eines Fußballfeldes einnahm, konnte jedoch immerhin schon, was der Vordenker von Neumann mit seinen getrennten Bauelementen theoretisch anvisiert hatte, nämlich ein neues Rechenprogramm beginnen, bevor das alte abgearbeitet war. Den SSEC kann man daher als Übergang „von einer reinen Rechenvorrichtung zu einem Gerät für die allgemeine Verarbeitung von Information“ betrachten, wie Metz es formuliert. Der SSEC wurde bereits um 1950 eingesetzt, um Mondpositionen zu berechnen. Mit diesen Informationen konnten im Rahmen des berühmten, in den 1960er Jahren von der amerikanischen Weltraumbehörde NASA durchgeführten Apollo-Projekts die Landepositionen der anfänglich noch unbemannten Raumfähren ausgewählt werden. Für jeden Landeplatz mussten Zehntausende von Rechenoperationen vorgenommen und mit umfangreichen Suchanfragen an eine Datenbank gekoppelt werden, die Informationen über die Mondoberfläche gespeichert hatte. Die IBM-Maschine konnte diese Kalkulationen in ein paar Minuten erledigen, und führte ihre Rechenkapazität spektakulär im Beisein von einigen aufgeregten Zuschauern vor.\nHeutige Maschinen brauchen bekanntlich für solche und noch kompliziertere Berechnungen nur noch Bruchteile von Sekunden, was unmittelbar die Frage aufwirft, wodurch die bis heute anhaltend rasante Entwicklung der Rechen- und Speicherkapazitäten von Computern bei gleichzeitig erfolgender Miniaturisierung möglich geworden ist. Wieso kann der kleine Laptop auf meinem schmalen Schreibtisch heute so unendlich viel mehr und unglaublich viel schneller als damals zum Beispiel das Riesending SSEC in seiner Riesenhalle?\nEin wesentlicher Teil der Antwort auf diese Frage liegt darin, dass die voluminösen und reparaturanfälligen Elektronenröhren in den Rechenmaschinen durch kleine und höchst zuverlässige Transistoren abgelöst wurden. Erfunden worden ist das Bauprinzip für diese Wunderdinger im Dezember 1947, und zwar in den Bell-Laboratorien in New Jersey. Hier versuchten drei in den 1950er Jahren mit dem Nobelpreis für ihr Fach ausgezeichnete Physiker – William Shockley, John Bardeen und Walter Brattain – systematisch zu erforschen, was in den Jahren des Zweiten Weltkriegs eher nebenbei erkundet worden war, nämlich die Eigenschaften von Kristallen oder Festkörpern, die man Halbleiter nennt. Was in der Geschichte der Physik erst nur Unverständnis und Langeweile hervorgerufen hatte – was sollte man auch mit Elementen wie Silizium und Germanium anfangen, die manchmal elektrischen Strom leiteten und manchmal nicht? –, war im Rahmen von Arbeiten (mit militärischer Zielsetzung) zur Radartechnik in den Blickpunkt des wissenschaftlichen Interesses gerückt. Man benötigte möglichst empfindliche Empfänger (Detektoren) für oftmals extrem schwache Signale, und eines Tages muss jemand unter den Physikern auf den Gedanken gekommen sein, dass Halbleiter genau dazu dienen konnten. Die Verwandlung vom Isolator zum Leiter setzt nämlich bei einigen Halbleiter-Kristallen höchst plötzlich ein, also schon bei geringsten Änderungen der äußeren Bedingungen – etwa der Temperatur oder der durch Strahlung zugeführten Energie –, und wenn man diese kleinen Schwankungen erkunden und vermessen wollte, konnte man ja Halbleiter als Detektoren einsetzen.\nNach ersten tastenden Bemühungen vor 1945 nahmen die – auch nach Kriegsende noch vom amerikanischen Militär finanzierten – Bell-Laboratorien die Entwicklung von Halbleitern als Schaltern systematisch in Angriff, und im Dezember 1947 gab es den ersten Transistor. Die Bezeichnung Transistor ist ein englisches Kunstwort, das sich aus zwei Teilen zusammensetzt, aus Transfer (Übertragung) und Resistance (Widerstand) nämlich. So ein „Übertragungswiderstand“ konnte – bei geeigneter Bausweise – Strom abblocken oder verstärken. Er lieferte nicht nur das, was die alte Elektronenröhre konnte – er tat dies besser und auch zuverlässiger, und er war darüber hinaus sehr viel kleiner und billiger herzustellen.\nDie zentrale Figur der Transistorentwicklung war John Bardeen. Als er seine Stelle bei den „Bell Labs“ im Oktober 1945 antrat, traf er dort mit dem Experimentalphysiker Walter Brattain zusammen, der hier seit 1929 beschäftigt war. Ihre gemeinsame Aufgabe bestand darin, mit Halbleitern die elektronischen Effekte zu erzielen, die bislang mithilfe Vakuumröhren erreicht wurden, in denen Strom durch geeignete Elemente (Kathode, Anode, Gitter) steuerbar gemacht wurde. Damit konnte man in der Praxis unter anderem Verstärker oder Empfänger konstruieren, also Radiogeräte bauen. Solche Vakuumröhren gab es bereits seit dem 19. Jahrhundert.\nDie Suche nach Alternativen zur Vakuumröhre hatte schon früh die Aufmerksamkeit auf Halbleiter gelenkt, da sie zumindest so beeinflusst werden konnten, dass sie etwa als Gleichrichter agierten und Strom nur in eine Richtung durchließen. Dies wusste man bereits seit dem Ende des 19. Jahrhunderts. In der Folge lernten die Physiker, wie sich Halbleiter nach Wunsch herstellen ließen, und als Bardeen bei Bell anfing, konnte man endlich auch erklären, was in den Halbleitern passierte. Man nutzte zur Erklärung das sogenannte Bändermodell der Festkörperphysik und bemühte sich, mit seiner Hilfe Situationen auszudenken und herzustellen, in denen das Leitungsband eines Halbleiters leicht oder schwer zu füllen war.\nDer Halbleiter, der in Bardeens Tagen in den Bell-Laboratorien am meisten Interesse fand, ist als Silizium bekannt (und wird in den Chemiebüchern mit c als Silicium geschrieben). Silizium findet sich zum Beispiel im Sand, der chemisch vorwiegend aus dem Stoff Siliciumdioxid besteht, das in reiner Form auch als Quarz bekannt ist. Das chemische Element heißt auf Englisch „silicon“, und das berühmte Silicon Valley, das bei San Francisco liegt und in den 1970er Jahren die Wiege der amerikanischen Computerindustrie wurde, trägt seinen Namen (wobei man sich davor hüten muss, aus dem amerikanischen „silicon“ bei einer Übersetzung das deutsche Silikon zu machen).\nWenn man sich das Silizium als Atom vorstellt, um sich seine Bedeutung und Einsatzfähigkeit zu erklären, kommt es auf die vier Elektronen an, die seine äußere Schale ausmachen. In einem Gitter (Kristall) aus Silizium befinden sich diese Elektronen vornehmlich in dem Valenzband, das weit vom Leitungsband entfernt liegt, und so kommt im Normalfall kein Stromfluss zustande. Dies kann man nun entscheidend ändern, indem man ein Siliziumkristall gezielt mit einem Element ausstattet (dotiert), das über fünf Außenelektronen verfügt – zum Beispiel mit Phosphor. Jedes Phosphoratom, das in das ursprüngliche Gitter aus Silizium eingebaut wird, kann ein Elektron freigeben, und dieser Ladungsträger kann das Leitungsband des Kristalls leichter erreichen. Der Kristall wird jetzt als dotiert bezeichnet – in unserem Fall Silizium mit Phosphor dotiert. Umgekehrt kann auch ein Element eingefügt werden, das statt der vier nur drei Außenelektronen hat – etwa Aluminium –, was dazu führt, dass bei dieser Dotierung eine Art Loch entsteht, das sich aber auch bewegen kann. Es verschiebt sich so, wie dies ein leerer Platz in der Mitte einer Sitzreihe tut, wenn die Menschen sich umsetzen oder nachrücken. Wenn ein Elektron zu viel da ist, sprechen die Physiker wegen dessen negativer Ladung von einem n-dotierten Halbleiter, und wenn ein Elektron zu wenig da ist und ein Loch entsteht, sprechen sie von einem p-dotierten Halbleiter. Und wenn auch jeder einzeln für sich nicht gerade als Wunderwerk anzusehen ist, so kann man mit einer geeigneten Kombination aus n- und p-Halbleitern – pnp oder npn zum Beispiel – die Welt verändern. Der Transistor ist nämlich eine solche Kombination, und der Weg zu ihm beginnt im Oktober 1945 sichtbar und konkret zu werden.\nIn diesem Monat nahmen Bardeen und Brattain bei Bell Kontakt mit einem dritten Physiker auf, dem aus London stammenden William Shockley, an dem sich bis heute die Geister scheiden. Für viele gilt Shockley als der Moses von Silicon Valley, der durch seinen unternehmerischen Geist die Grundlagen der amerikanischen Computerindustrie gelegt hat. Andere erinnern sich an seine rassistischen Ergüsse, in denen er in den 1960er Jahren beweisen wollte, dass Afroamerikaner statistisch minder intelligent sind als Amerikaner europäischer Herkunft. Auf jeden Fall war es die Zusammenarbeit von Bardeen und Brattain mit Shockley, die im Dezember 1947 zur Erfindung des Transistors führte, für die alle drei Wissenschaftler neun Jahre später den Nobelpreis erhielten.\nDie einzelnen Schritte, die Bardeen und Brattain im Dezember 1947 unternahmen, und das genaue Design ihres Prototyps, der fachlich korrekt als Spitzentransistor bezeichnet wird, müssen wir hier übergehen, weil deren Darstellung zuviel Platz erfordern würde. Es ginge dabei vor allem um die Verteilung von Ladungen an Oberflächen und ihrem theoretischen Verständnis. Entstanden ist eine Anordnung von dotierten Halbleitern in drei Schichten mit unterschiedlich dotierten Nachbarn, was entweder die Kombination pnp oder die Folge npn ergibt. Wenn alle drei Schichten Anschlüsse für elektrischen Strom haben, kann man die Schaltungen so einrichten, dass ein Signal entweder gestoppt oder verstärkt wird. Das Unterbrechen des Stroms ist einfach zu verstehen und erfolgt dann, wenn die ihn ausmachenden Elektronen auf eine n-dotierte Schicht treffen. Spannender wird es, wenn die Gegenrichtung eingeschlagen wird, das heißt, wenn Elektronen auf Löcher treffen und beide sich zusammenfinden (rekombinieren) können. Dabei kann Energie freiwerden, sogar als sichtbares Licht, was in Leuchtdioden ausgenutzt wird.\nSeine eigentliche (verstärkende) Funktion bekommt der Transistor, wenn etwa in einer npn-Anordnung ein kleiner Strom auf die mittlere Schicht geleitet wird. Sie wird heute als Basis bezeichnet und verbindet die anderen Elemente, die Emitter bzw. Kollektor heißen. Ein in der Basis von außen eintreffender kleiner Strom sorgt im Inneren der Schicht für räumliche Veränderungen (Rekombinationen) der Ladungsträger, die sich als großer Strom auf der Strecke zwischen Emitter und Kollektor bemerkbar machen. Um das auf diese Weise verstärkte Signal geht es in der Physik und der Technik, sein Auftauchen stellte das Ziel der Arbeiten am Transistor dar, und es wurde kurz vor Weihnachten 1947 erreicht.",
      "id": [
        "145788"
      ],
      "item": "Der Transistor"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145789]\n[%Bändermodell]\nUnter einem Bändermodell versteht man die Idee, dass Elektronen in Kristallen sogenannten Bändern zugeordnet werden können, und zwar abhängig von ihrer Energie. Dabei lässt sich ein Band mit hoher von einem Band mit geringer Energie unterscheiden, und die beiden heißen in der Fachwelt Leitungsband bzw. Valenzband, wobei der erste Name leicht verständlich ist. Wenn sich nämlich Elektronen im Leitungsband befinden, können sie sich bewegen, und somit leitet der Festkörper Strom, sonst nicht. Zwischen den Bändern können Elektronen nicht existieren.\nEin Metall (wie Kupfer) ist nun dadurch charakterisiert, dass Elektronen leicht aus dem Valenzband, das ihrem gebundenen Grundzustand entspricht, in das Leitungsband springen können, das ihrem beweglichen angeregten Zustand entspricht. Bei einem Isolator (wie Glas) ist die Lücke zu groß, um unter normalen Umständen überwunden zu werden, und so halten sich die Elektronen überwiegend im Valenzband auf. Zwischen diesen beiden genannten Festkörperarten stehen die sogenannten Halbleiter, deren Name korrekt ausdrückt, was sie können, nämlich manchmal einen Strom leiten und manchmal nicht. Bei ihnen hängt die Lücke – die Größe des Quantensprungs – zwischen Leitungs- und Valenzband stark von äußeren Bedingungen (etwa der Temperatur) ab, was zunächst eher störend wirkte, bis man bemerkte, dass diese Flexibilität genutzt werden konnte – vor allem in den Transistoren.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Unter einem Bändermodell versteht man die Idee, dass Elektronen in Kristallen sogenannten Bändern zugeordnet werden können, und zwar abhängig von ihrer Energie. Dabei lässt sich ein Band mit hoher von einem Band mit geringer Energie unterscheiden, und die beiden heißen in der Fachwelt Leitungsband bzw. Valenzband, wobei der erste Name leicht verständlich ist. Wenn sich nämlich Elektronen im Leitungsband befinden, können sie sich bewegen, und somit leitet der Festkörper Strom, sonst nicht. Zwischen den Bändern können Elektronen nicht existieren.\nEin Metall (wie Kupfer) ist nun dadurch charakterisiert, dass Elektronen leicht aus dem Valenzband, das ihrem gebundenen Grundzustand entspricht, in das Leitungsband springen können, das ihrem beweglichen angeregten Zustand entspricht. Bei einem Isolator (wie Glas) ist die Lücke zu groß, um unter normalen Umständen überwunden zu werden, und so halten sich die Elektronen überwiegend im Valenzband auf. Zwischen diesen beiden genannten Festkörperarten stehen die sogenannten Halbleiter, deren Name korrekt ausdrückt, was sie können, nämlich manchmal einen Strom leiten und manchmal nicht. Bei ihnen hängt die Lücke – die Größe des Quantensprungs – zwischen Leitungs- und Valenzband stark von äußeren Bedingungen (etwa der Temperatur) ab, was zunächst eher störend wirkte, bis man bemerkte, dass diese Flexibilität genutzt werden konnte – vor allem in den Transistoren.",
      "id": [
        "145789"
      ],
      "item": "Bändermodell"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145790]\n[%Der Siegeszug zur Informationsgesellschaft]\nDer Transistor lieferte nicht nur das, was eine alte Elektronenröhre konnte, der neue Transistor tat dies besser und zuverlässiger, und er war darüber hinaus sehr viel kleiner und billiger herzustellen.\nBei diesen Qualitäten dauerte es nicht lange, bis der Siegeszug der Transistoren einsetzte, die es bereits 1951 in Hörgeräten gab und mit denen seit 1958 die integrierten Schaltkreise gebaut werden, die wir als Mikrochips kennen und nutzen. \nNicht unerwähnt bleiben sollte, dass die drei Erfinder des Transistors (John Bardeen, Walter Brattain und William Shockley) sich bei ihren Bemühungen an der Quantenmechanik orientierten. Ohne ihre Kenntnis dieser neuartigen Wissenschaft wären sie keinen Schritt vorangekommen. Im 18. Jahrhundert hatte noch jemand eine Dampfmaschine konstruieren und im 19. Jahrhundert noch jemand eine Eisenbahn bauen können, ohne die Gesetze der Thermodynamik zu kennen. In der zweiten Hälfte des 20. Jahrhunderts ging dies nicht mehr. Jetzt reichte es nicht, etwas zu wollen, jetzt musste man zunächst etwas wissen, um etwas grundlegend Neues bauen bzw. konstruieren zu können, wie die Erfindung des Transistors zeigt. Und wer aus diesem ersten kleinen Beispiel in einen großen Trend herauslesen will, könnte sagen, dass sich hier die Transformation zu erkennen gibt, die heute als ausgemacht und zukunftsweisend gilt, nämlich die Wandlung einer Industrie- in eine Informationsgesellschaft.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der Transistor lieferte nicht nur das, was eine alte Elektronenröhre konnte, der neue Transistor tat dies besser und zuverlässiger, und er war darüber hinaus sehr viel kleiner und billiger herzustellen.\nBei diesen Qualitäten dauerte es nicht lange, bis der Siegeszug der Transistoren einsetzte, die es bereits 1951 in Hörgeräten gab und mit denen seit 1958 die integrierten Schaltkreise gebaut werden, die wir als Mikrochips kennen und nutzen. \nNicht unerwähnt bleiben sollte, dass die drei Erfinder des Transistors (John Bardeen, Walter Brattain und William Shockley) sich bei ihren Bemühungen an der Quantenmechanik orientierten. Ohne ihre Kenntnis dieser neuartigen Wissenschaft wären sie keinen Schritt vorangekommen. Im 18. Jahrhundert hatte noch jemand eine Dampfmaschine konstruieren und im 19. Jahrhundert noch jemand eine Eisenbahn bauen können, ohne die Gesetze der Thermodynamik zu kennen. In der zweiten Hälfte des 20. Jahrhunderts ging dies nicht mehr. Jetzt reichte es nicht, etwas zu wollen, jetzt musste man zunächst etwas wissen, um etwas grundlegend Neues bauen bzw. konstruieren zu können, wie die Erfindung des Transistors zeigt. Und wer aus diesem ersten kleinen Beispiel in einen großen Trend herauslesen will, könnte sagen, dass sich hier die Transformation zu erkennen gibt, die heute als ausgemacht und zukunftsweisend gilt, nämlich die Wandlung einer Industrie- in eine Informationsgesellschaft.",
      "id": [
        "145790"
      ],
      "item": "Der Siegeszug zur Informationsgesellschaft"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145791]\n[%Eine Welt voller Informationen]\nDie erste Theorie der Information geht auf das Jahr 1948 zurück, als der Mathematiker Claude Shannon darüber nachdachte, wie sich die Übertragung von Nachrichten besser bewerkstelligen lässt. Shannon gelangte zu der Erkenntnis, dass dieses „besser“ bedeutet, sich nicht auf die inhaltliche Bedeutung der Nachricht zu konzentrieren. Er schränkt sich ein und klammert das Komplexe und Komplizierte erst einmal aus. Shannon schreibt: „ Das fundamentale Problem der Kommunikation besteht darin, an einem Punkt eine Nachricht, die an einem anderen Punkt ausgewählt wurde, exakt oder näherungsweise wiederzugeben. Oft haben Nachrichten eine __Bedeutung__, das heißt, sie beziehen sich auf ein System oder sind korreliert mit einem System, das bestimmte physikalische konzeptuelle Entitäten besitzt. Diese semantischen Aspekte der Kommunikation sind für das technische Problem irrelevant.“ Dank dieser Einschränkung gelingt es Shannon, eine vollständige mathematische Theorie der Information zu entwickeln. Sie bietet den Vorteil, unabhängig davon zu sein, in welcher Form die Information vorliegt – als Schrift, als Muster, als Bild oder wie auch immer. Die Information muss nur durch einen Code auf Nullen und Einsen zurückzuführen sein, und die dabei entstehenden Folgen gilt es dann zu zählen.\nUm die mathematische Darstellbarkeit der Informationen zu erreichen, schlägt er also vor, alle Zeichen in binärer Form darzustellen – also als Folge von 0 und 1 – und die Information einer solchen Zahlengruppe durch die Menge der benötigten Stellen festzulegen. Er sprach von „binary digits“, was als Bit abgekürzt wurde und in dieser Form Einzug in den sprachlichen Alltag hielt.\nDie Idee der binären – zweiwertigen – Darstellung ist uralter Stoff für Mathematiker und auch immer schon für den Bau von Rechenmaschinen im Gespräch gewesen. Bereits im 17. Jahrhundert hat der große Gottfried Wilhelm Leibniz über binäre Codes nachgedacht und die Möglichkeit erwogen, Zahlen dual darzustellen.\nDas Ziel von Shannon (in Kooperation mit Norbert Wiener) lag nicht nur darin, Möglichkeiten zu schaffen, mit denen Informationen gemessen werden konnten. Sie wollten Informationen auch in elektronischen Schaltkreisen als Nachrichten übermitteln, und genau dafür waren die binären Einheiten gut zu gebrauchen: Da floss ein Strom – das zählte als 1 – oder da floss kein Strom – das zählte als 0.\nIn seinen zwei Arbeiten mit dem Originaltitel __A Mathematical Theory of Communication__ von 1948 (in der deutschen Übersetzung __Eine mathematische Theorie der Information__[!]) schlägt Shannon vor, den Informationsgehalt einer Nachricht dadurch zu bestimmen, dass man sie erst binär ausdrückt und dann die Anzahl der Nullen und Einsen ermittelt. Man kann die Ziffern, mit denen wir gewöhnlich rechnen – also 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 –, binär darstellen durch 0, 1, 10, 11, 100, 101, 110, 111, 1000, 1001, 1010. Man auch die Buchstaben, mit denen wir unsere Worte schreiben, binär darstellen, und zwar dadurch, dass man einen Code festlegt, nach dem dies geschieht. Unter einem Code versteht man eine Vorschrift, nach der zum Beispiel ein Zeichen (ein Buchstabe) in ein anderes Zeichen (eine Zahl) verwandelt wird, und den meisten fällt dabei der Morse-Code ein, bei dem aus Buchstaben eine Kombination aus langen und kurzen Impulsen wurde, mit denen telegrafiert werden konnte. In der modernen Computertechnologie wird häufig ein Code eingesetzt, der mit acht Bits operiert, weshalb man diese Einheit der Information aus historischen Gründen als Byte zusammenfasst. Wie sich nämlich herausstellte, reichen acht Bits (also 1 Byte) mit ihren 2 hoch acht, also 256 Möglichkeiten aus, um sämtliche Buchstaben und Zahlen nebst Sonderzeichen der Sprache (Anführungen, Doppelpunkte, …) zu kodieren, und damit können alle denkbaren Informationen einem Computer als elektrische Signale gegeben und von ihm empfangen werden. Damit war der Weg frei, den American Standard Code for Information Interchange – ASCII – zu konzipieren, der ab 1963 entwickelt und von 1967 an zum Standard wurde. (Abb. ASCII-Code). Genauer muss gesagt werden, dass anfänglich nur 128 Zeichen kodiert werden sollten, wofür ein 7-Bit-Code reichte, der aber bald erweitert wurde. Zu den ursprünglichen 128 Zeichen gehörten neben dem Leerzeichen noch folgende Symbole:\n! „ $ & ` ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ?\n@ A B C D E F G H I J K L M N O P Q R S T U\nV W X Y Z [ \\ ] ^ _ ` \ta b c d e f g h i j k l m n o p q \nr s t u v w x y z { | } ~",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Die erste Theorie der Information geht auf das Jahr 1948 zurück, als der Mathematiker Claude Shannon darüber nachdachte, wie sich die Übertragung von Nachrichten besser bewerkstelligen lässt. Shannon gelangte zu der Erkenntnis, dass dieses „besser“ bedeutet, sich nicht auf die inhaltliche Bedeutung der Nachricht zu konzentrieren. Er schränkt sich ein und klammert das Komplexe und Komplizierte erst einmal aus. Shannon schreibt: „ Das fundamentale Problem der Kommunikation besteht darin, an einem Punkt eine Nachricht, die an einem anderen Punkt ausgewählt wurde, exakt oder näherungsweise wiederzugeben. Oft haben Nachrichten eine __Bedeutung__, das heißt, sie beziehen sich auf ein System oder sind korreliert mit einem System, das bestimmte physikalische konzeptuelle Entitäten besitzt. Diese semantischen Aspekte der Kommunikation sind für das technische Problem irrelevant.“ Dank dieser Einschränkung gelingt es Shannon, eine vollständige mathematische Theorie der Information zu entwickeln. Sie bietet den Vorteil, unabhängig davon zu sein, in welcher Form die Information vorliegt – als Schrift, als Muster, als Bild oder wie auch immer. Die Information muss nur durch einen Code auf Nullen und Einsen zurückzuführen sein, und die dabei entstehenden Folgen gilt es dann zu zählen.\nUm die mathematische Darstellbarkeit der Informationen zu erreichen, schlägt er also vor, alle Zeichen in binärer Form darzustellen – also als Folge von 0 und 1 – und die Information einer solchen Zahlengruppe durch die Menge der benötigten Stellen festzulegen. Er sprach von „binary digits“, was als Bit abgekürzt wurde und in dieser Form Einzug in den sprachlichen Alltag hielt.\nDie Idee der binären – zweiwertigen – Darstellung ist uralter Stoff für Mathematiker und auch immer schon für den Bau von Rechenmaschinen im Gespräch gewesen. Bereits im 17. Jahrhundert hat der große Gottfried Wilhelm Leibniz über binäre Codes nachgedacht und die Möglichkeit erwogen, Zahlen dual darzustellen.\nDas Ziel von Shannon (in Kooperation mit Norbert Wiener) lag nicht nur darin, Möglichkeiten zu schaffen, mit denen Informationen gemessen werden konnten. Sie wollten Informationen auch in elektronischen Schaltkreisen als Nachrichten übermitteln, und genau dafür waren die binären Einheiten gut zu gebrauchen: Da floss ein Strom – das zählte als 1 – oder da floss kein Strom – das zählte als 0.\nIn seinen zwei Arbeiten mit dem Originaltitel __A Mathematical Theory of Communication__ von 1948 (in der deutschen Übersetzung __Eine mathematische Theorie der Information__[!]) schlägt Shannon vor, den Informationsgehalt einer Nachricht dadurch zu bestimmen, dass man sie erst binär ausdrückt und dann die Anzahl der Nullen und Einsen ermittelt. Man kann die Ziffern, mit denen wir gewöhnlich rechnen – also 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 –, binär darstellen durch 0, 1, 10, 11, 100, 101, 110, 111, 1000, 1001, 1010. Man auch die Buchstaben, mit denen wir unsere Worte schreiben, binär darstellen, und zwar dadurch, dass man einen Code festlegt, nach dem dies geschieht. Unter einem Code versteht man eine Vorschrift, nach der zum Beispiel ein Zeichen (ein Buchstabe) in ein anderes Zeichen (eine Zahl) verwandelt wird, und den meisten fällt dabei der Morse-Code ein, bei dem aus Buchstaben eine Kombination aus langen und kurzen Impulsen wurde, mit denen telegrafiert werden konnte. In der modernen Computertechnologie wird häufig ein Code eingesetzt, der mit acht Bits operiert, weshalb man diese Einheit der Information aus historischen Gründen als Byte zusammenfasst. Wie sich nämlich herausstellte, reichen acht Bits (also 1 Byte) mit ihren 2 hoch acht, also 256 Möglichkeiten aus, um sämtliche Buchstaben und Zahlen nebst Sonderzeichen der Sprache (Anführungen, Doppelpunkte, …) zu kodieren, und damit können alle denkbaren Informationen einem Computer als elektrische Signale gegeben und von ihm empfangen werden. Damit war der Weg frei, den American Standard Code for Information Interchange – ASCII – zu konzipieren, der ab 1963 entwickelt und von 1967 an zum Standard wurde. (Abb. ASCII-Code). Genauer muss gesagt werden, dass anfänglich nur 128 Zeichen kodiert werden sollten, wofür ein 7-Bit-Code reichte, der aber bald erweitert wurde. Zu den ursprünglichen 128 Zeichen gehörten neben dem Leerzeichen noch folgende Symbole:\n! „ $ & ` ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ?\n@ A B C D E F G H I J K L M N O P Q R S T U\nV W X Y Z [ \\ ] ^ _ ` \ta b c d e f g h i j k l m n o p q \nr s t u v w x y z { | } ~",
      "id": [
        "145791"
      ],
      "item": "Eine Welt voller Informationen"
    },
    "errors": [
      {
        "message": "token recognition error at: '`'",
        "line": 9,
        "column": 8,
        "errorLine": "! „ $ & ` ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ?"
      },
      {
        "message": "token recognition error at: '[ \\'",
        "line": 11,
        "column": 10,
        "errorLine": "V W X Y Z [ \\ ] ^ _ ` \ta b c d e f g h i j k l m n o p q "
      },
      {
        "message": "Unexpected input ' ]'",
        "line": 11,
        "column": 13,
        "errorLine": "V W X Y Z [ \\ ] ^ _ ` \ta b c d e f g h i j k l m n o p q "
      },
      {
        "message": "token recognition error at: '`'",
        "line": 11,
        "column": 20,
        "errorLine": "V W X Y Z [ \\ ] ^ _ ` \ta b c d e f g h i j k l m n o p q "
      },
      {
        "message": "token recognition error at: '~'",
        "line": 12,
        "column": 24,
        "errorLine": "r s t u v w x y z { | } ~"
      }
    ]
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145792]\n[%Claude Shannon]\nClaude Shannon (1916-2001), Mathematiker und Elektronik-Tüftler, gilt als Vater der Informationstheorie. Er war wie manche andere Pioniere des Informationszeitalters an den Bell-Laboratorien tätig.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Claude Shannon (1916-2001), Mathematiker und Elektronik-Tüftler, gilt als Vater der Informationstheorie. Er war wie manche andere Pioniere des Informationszeitalters an den Bell-Laboratorien tätig.",
      "id": [
        "145792"
      ],
      "item": "Claude Shannon"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145793]\n[%Gottfried Wilhelm Leibniz]\nGottfried Wilhelm Leibniz (1646-1716) war einer der bedeutendsten Gelehrten der Barockzeit. Als Mathematiker entwickelte er eine recht komplexe Rechenmaschine und erfand (parallel zu Newton) die Infinitesimalrechnung. Als Philosoph entwickelte er die Theorie von den ausdehnungslosen, aber wirkenden und Informationen aufnehmenden „Monaden“, die sehr viel mit modernen Bits oder auch Elementarteilchen gemein haben.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Gottfried Wilhelm Leibniz (1646-1716) war einer der bedeutendsten Gelehrten der Barockzeit. Als Mathematiker entwickelte er eine recht komplexe Rechenmaschine und erfand (parallel zu Newton) die Infinitesimalrechnung. Als Philosoph entwickelte er die Theorie von den ausdehnungslosen, aber wirkenden und Informationen aufnehmenden „Monaden“, die sehr viel mit modernen Bits oder auch Elementarteilchen gemein haben.",
      "id": [
        "145793"
      ],
      "item": "Gottfried Wilhelm Leibniz"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145794]\n[%Das duale Zahlensystem]\nWenn wir die uns vertrauten Zahlen schreiben – etwa 476 –, dann kürzen wir damit 4 mal 100, 7 mal 10 und 6 mal 1 ab. Das heißt, wir schreiben Zahlen gewöhnlich in einem Zehnersystem, wie man sagt. In diesem System benötigen wir neun Ziffern (1, 2, 3, 4, 5, 6, 7, 8, 9) und die Null. Jede Zahl ergibt sich aus den Einern, den Zehnern, den Hundertern, den Tausendern und so weiter. Wie oben gezeigt: 9305 meint 9 mal 1000, 3 mal 100, 0 mal 10 und 5 mal 1.\nIn einem Zweiersystem müssen wir statt der Zehnerreihe (1, 10, 100, 1000, 10’000, …) die Zweierreihe nutzen, die im Zehnersystem 1, 2, 4, 8, 16, 32, 64 und so weiter lautet. Die arabische 21 kann erfasst werden als einmal Sechzehn, keinmal 8, einmal 4, keinmal 2 und einmal 1, was dann ausgeschrieben 10’101 wird. Das Verfahren wird rasch mühsam, wenn die Zahlen größer werden – so verwandelt sich die 98 im Zehnersystem in die Folge 1’100’010 (64 + 32 + 2). Das heißt, das Dualsystem wird mühsam für unseren Schädel. Aber dafür bauen wir ja den Computer, damit er uns das Rechnen abnimmt. Für ihn ist es mühsam, so zu rechnen, wie wir es tun. Mehr als zwei Zeichen – an oder aus – verwirren ihn.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Wenn wir die uns vertrauten Zahlen schreiben – etwa 476 –, dann kürzen wir damit 4 mal 100, 7 mal 10 und 6 mal 1 ab. Das heißt, wir schreiben Zahlen gewöhnlich in einem Zehnersystem, wie man sagt. In diesem System benötigen wir neun Ziffern (1, 2, 3, 4, 5, 6, 7, 8, 9) und die Null. Jede Zahl ergibt sich aus den Einern, den Zehnern, den Hundertern, den Tausendern und so weiter. Wie oben gezeigt: 9305 meint 9 mal 1000, 3 mal 100, 0 mal 10 und 5 mal 1.\nIn einem Zweiersystem müssen wir statt der Zehnerreihe (1, 10, 100, 1000, 10’000, …) die Zweierreihe nutzen, die im Zehnersystem 1, 2, 4, 8, 16, 32, 64 und so weiter lautet. Die arabische 21 kann erfasst werden als einmal Sechzehn, keinmal 8, einmal 4, keinmal 2 und einmal 1, was dann ausgeschrieben 10’101 wird. Das Verfahren wird rasch mühsam, wenn die Zahlen größer werden – so verwandelt sich die 98 im Zehnersystem in die Folge 1’100’010 (64 + 32 + 2). Das heißt, das Dualsystem wird mühsam für unseren Schädel. Aber dafür bauen wir ja den Computer, damit er uns das Rechnen abnimmt. Für ihn ist es mühsam, so zu rechnen, wie wir es tun. Mehr als zwei Zeichen – an oder aus – verwirren ihn.",
      "id": [
        "145794"
      ],
      "item": "Das duale Zahlensystem"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145795]\n[%Verhältniszahlen]\nDas von Shannon entwickelte System bzw. seine Informationstheorie mit der binären Darstellungsweise von Zeichen bietet den Vorteil, unabhängig davon zu sein, in welcher Form die Information vorliegt – als Schrift, als Muster, als Bild oder wie auch immer. Die Information muss nur durch einen Code auf Nullen und Einsen zurückzuführen sein, und die dabei entstehenden Folgen gilt es dann zu zählen. Bei einem Bild etwa geht es im einfachsten Fall nur darum, ob in einem Raster ein Element an- oder ausgeschaltet ist.\nWarum Shannon dafür die semantischen Aspekte der Kommunikation ausklammern musste, erkennt man auch dann, wenn man den mathematischen Schritt ins Auge fasst, den Shannon gehen musste – tatsächlich: musste –, um die Information so definieren zu können, dass sie sowohl in die Physik und ihre Geschichte passt als auch von einem Nachrichtentechniker bequem benutzt werden kann. Um zu verstehen, was da benötigt wird, stellen wir uns vor, wir verfügten über eine Folge von Nullen und Einsen und wollten wissen, wie viele Nachrichten sich damit übertragen lassen. Bei einer Ziffer können wir zwei Nachrichten senden – 0 oder 1 –, bei zwei Ziffern können wir zwei mal zwei Nachrichten senden – 00, 01, 10 oder 11, also 4 –, bei drei Ziffern können wir zwei mal zwei mal zwei senden – 000, 100, 010, 001, 110, 101, 011 oder 111 – also 8 –, und so wächst diese Menge weiter. So klar dies ist, so rasch wächst die Zahl der möglichen Nachrichten, und Shannon überlegte, wie man sie gut in den Griff bekommen kann.\nDie Antwort der Wissenschaft lautet, „durch den Logarithmus“, wobei das aus dem Griechischen stammende Wort so viel wie „Verhältniszahl“ bedeutet. Logarithmen dienen der Wissenschaft dazu, Dinge ins Verhältnis zueinander zu setzen – etwa die Stärken von Erdbeben auf der Richterskala oder die Helligkeit von Sternen in Größenklassen. Und bei einer logarithmischen Auftragung kann man etwas, das anfänglich sehr schnell abläuft und gegen Ende langsamer wird, in zeitlich und dynamisch vergleichbare Stufen einteilen.\nWie Shannon bald feststellte, lässt sich die Anzahl der Nachrichten, die mit einer beliebigen Folge aus Ziffern (0 oder 1) möglich sind, bequem abzählen und handhaben, wenn man statt der Zahl selbst ihren Logarithmus nimmt. Und so definiert er dann den Informationsgehalt einer bestimmten Nachricht durch den Logarithmus der Anzahl der möglichen Nachrichten, wobei das Bit die Einheit der Information bleibt. Hierfür musste Information rein mathematisch definiert sein, denn von semantischen Inhalten gibt es keine Logarithmen.\nDas mit dem Logarithmus klingt kompliziert. Es wird viel Kopfschütteln hervorrufen und einige Mühe machen, aber es gehört zum Geschäft. Denn tatsächlich kann man nur verstehen, was Shannon im Sinn hatte, wenn man sich daran erinnert, was oben gesagt wurde, dass nämlich die Information aus dem Geist der Entropie geboren worden ist. Sie braucht den Logarithmus, um sinnvoll zur Physik und zum Verstehen des Wechselspiels von Ordnung und Unordnung beitragen zu können. Wir können sonst auch nicht verstehen, wie Information physikalisch werden kann.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Das von Shannon entwickelte System bzw. seine Informationstheorie mit der binären Darstellungsweise von Zeichen bietet den Vorteil, unabhängig davon zu sein, in welcher Form die Information vorliegt – als Schrift, als Muster, als Bild oder wie auch immer. Die Information muss nur durch einen Code auf Nullen und Einsen zurückzuführen sein, und die dabei entstehenden Folgen gilt es dann zu zählen. Bei einem Bild etwa geht es im einfachsten Fall nur darum, ob in einem Raster ein Element an- oder ausgeschaltet ist.\nWarum Shannon dafür die semantischen Aspekte der Kommunikation ausklammern musste, erkennt man auch dann, wenn man den mathematischen Schritt ins Auge fasst, den Shannon gehen musste – tatsächlich: musste –, um die Information so definieren zu können, dass sie sowohl in die Physik und ihre Geschichte passt als auch von einem Nachrichtentechniker bequem benutzt werden kann. Um zu verstehen, was da benötigt wird, stellen wir uns vor, wir verfügten über eine Folge von Nullen und Einsen und wollten wissen, wie viele Nachrichten sich damit übertragen lassen. Bei einer Ziffer können wir zwei Nachrichten senden – 0 oder 1 –, bei zwei Ziffern können wir zwei mal zwei Nachrichten senden – 00, 01, 10 oder 11, also 4 –, bei drei Ziffern können wir zwei mal zwei mal zwei senden – 000, 100, 010, 001, 110, 101, 011 oder 111 – also 8 –, und so wächst diese Menge weiter. So klar dies ist, so rasch wächst die Zahl der möglichen Nachrichten, und Shannon überlegte, wie man sie gut in den Griff bekommen kann.\nDie Antwort der Wissenschaft lautet, „durch den Logarithmus“, wobei das aus dem Griechischen stammende Wort so viel wie „Verhältniszahl“ bedeutet. Logarithmen dienen der Wissenschaft dazu, Dinge ins Verhältnis zueinander zu setzen – etwa die Stärken von Erdbeben auf der Richterskala oder die Helligkeit von Sternen in Größenklassen. Und bei einer logarithmischen Auftragung kann man etwas, das anfänglich sehr schnell abläuft und gegen Ende langsamer wird, in zeitlich und dynamisch vergleichbare Stufen einteilen.\nWie Shannon bald feststellte, lässt sich die Anzahl der Nachrichten, die mit einer beliebigen Folge aus Ziffern (0 oder 1) möglich sind, bequem abzählen und handhaben, wenn man statt der Zahl selbst ihren Logarithmus nimmt. Und so definiert er dann den Informationsgehalt einer bestimmten Nachricht durch den Logarithmus der Anzahl der möglichen Nachrichten, wobei das Bit die Einheit der Information bleibt. Hierfür musste Information rein mathematisch definiert sein, denn von semantischen Inhalten gibt es keine Logarithmen.\nDas mit dem Logarithmus klingt kompliziert. Es wird viel Kopfschütteln hervorrufen und einige Mühe machen, aber es gehört zum Geschäft. Denn tatsächlich kann man nur verstehen, was Shannon im Sinn hatte, wenn man sich daran erinnert, was oben gesagt wurde, dass nämlich die Information aus dem Geist der Entropie geboren worden ist. Sie braucht den Logarithmus, um sinnvoll zur Physik und zum Verstehen des Wechselspiels von Ordnung und Unordnung beitragen zu können. Wir können sonst auch nicht verstehen, wie Information physikalisch werden kann.",
      "id": [
        "145795"
      ],
      "item": "Verhältniszahlen"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145796]\n[%Rauschen]\nWir hatten gesagt, dass Shannon mit seiner Theorie der Kommunikation oder Theorie der Information verstehen wollte, wie genau ein Eingangssignal (Sender) mit dem Ausgangssignal (beim Empfänger) übereinstimmt. Dazu untersuchte er auch, welche Störungen auftreten können, wobei ihn vor allem die unvermeidlichen Störungen beschäftigten, die mit Zufälligkeiten und den Abweichungen jeder Realität vom theoretischen Idealzustand zusammenhängen. Wir alle kennen das Rauschen in Leitungen oder das Schneegestöber auf einem Fernsehschirm, und Shannon wollte wissen, wie diese Signale – er nannte sie __noise__ – die übermittelte Information beeinflussen oder beeinträchtigen. Er definierte die Stärke eine Signals (S), verglich sie mit dem möglichen Rauschen (N für noise) bildete das Verhältnis S/N und seinen Logarithmus und konnte so ableiten, wie viel Information ein verrauschter Kommunikationskanal übertragen kann. Shannon leitete eine mathematische Formel für die Informationskapazität von Kanälen her, und damit kam die Telekommunikation zurecht.\nWer dies liest, denkt, das Rauschen sei der Feind der Information. Ein zu sendendes Signal geht im Rauschen unter, wie man es früher noch oft selbst erlebt hat, wenn eine Schallplatte abgespielt werden sollte und dabei vor allem ein Kratzen zu hören war. Doch Vorsicht! Inzwischen dreht sich das Denken um. Wir bekommen oftmals eine Information erst dann, wenn wir ein System stören – etwa wenn wir auf ein Barometer klopfen, um es anschließend ablesen zu können. Und wir wissen, dass unser Auge zwar zittert (und also verrauschte Signale empfängt), dass wir aber offenbar deshalb erst scharf sehen können. Der Physiker Hans Christian formuliert: „Der traditionelle Feind der Information wird langsam zu ihrem Partner.“",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Wir hatten gesagt, dass Shannon mit seiner Theorie der Kommunikation oder Theorie der Information verstehen wollte, wie genau ein Eingangssignal (Sender) mit dem Ausgangssignal (beim Empfänger) übereinstimmt. Dazu untersuchte er auch, welche Störungen auftreten können, wobei ihn vor allem die unvermeidlichen Störungen beschäftigten, die mit Zufälligkeiten und den Abweichungen jeder Realität vom theoretischen Idealzustand zusammenhängen. Wir alle kennen das Rauschen in Leitungen oder das Schneegestöber auf einem Fernsehschirm, und Shannon wollte wissen, wie diese Signale – er nannte sie __noise__ – die übermittelte Information beeinflussen oder beeinträchtigen. Er definierte die Stärke eine Signals (S), verglich sie mit dem möglichen Rauschen (N für noise) bildete das Verhältnis S/N und seinen Logarithmus und konnte so ableiten, wie viel Information ein verrauschter Kommunikationskanal übertragen kann. Shannon leitete eine mathematische Formel für die Informationskapazität von Kanälen her, und damit kam die Telekommunikation zurecht.\nWer dies liest, denkt, das Rauschen sei der Feind der Information. Ein zu sendendes Signal geht im Rauschen unter, wie man es früher noch oft selbst erlebt hat, wenn eine Schallplatte abgespielt werden sollte und dabei vor allem ein Kratzen zu hören war. Doch Vorsicht! Inzwischen dreht sich das Denken um. Wir bekommen oftmals eine Information erst dann, wenn wir ein System stören – etwa wenn wir auf ein Barometer klopfen, um es anschließend ablesen zu können. Und wir wissen, dass unser Auge zwar zittert (und also verrauschte Signale empfängt), dass wir aber offenbar deshalb erst scharf sehen können. Der Physiker Hans Christian formuliert: „Der traditionelle Feind der Information wird langsam zu ihrem Partner.“",
      "id": [
        "145796"
      ],
      "item": "Rauschen"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145797]\n[%Was ist Leben?]\nBiologie gibt es als Fach seit 1800, und die Botaniker und Zoologen und ihre Kollegen aus den Naturkundemuseen haben über Jahrhunderte hin fein und vorsichtig die Vielfalt des Lebens beschrieben, benannt und katalogisiert, ohne allzu viel davon zu verstehen. In den 1930er Jahren gesellten sich ihnen die ersten Physiker und Chemiker zu, die ihre Methoden und Fragestellungen mitbrachten und genauer wissen wollten, welche Molekülsorten in Zellen gebraucht werden, woraus diese Lebensbausteine aufgebaut sind und ob man erklären könne, was sie gleichzeitig stabil für das Individuum und dynamisch für die Evolution macht. Als neuen Namen für dieses quantitativ vorgehende und präzise werdende Erkunden des Lebens wählten sie in den späten 1930er Jahren die Bezeichnung „Molekularbiologie“. Dies passierte also noch vor dem Zweiten Weltkrieg, der dann aber nahezu allen Forschungsschwung dämpfte. Das Militär war mehr an Maschinen als an Molekülen interessiert, und so lenkten viele Forscher ihr Interesse von den Lebewesen auf die benötigten Maschinen um.\nDas heißt, die genetische Forschung – also das wissenschaftliche Bemühen um das Verständnis des Erbmaterials, für das man inzwischen den Namen Gene benutzte – lief auf Sparflamme weiter. Trotzdem brachten ihre Betreiber bis 1945 ausbaubare Ergebnisse zustande, die historische Weichenstellungen erlaubten. Eines besagt, dass sich bei Genen immer ein Stoff finden läßt, den die Chemiker seit dem 19. Jahrhundert als Nukleinsäuren kannten, und zwar genauer die Sorte, die mit den drei Buchstaben DNA abgekürzt wird, hinter denen die englische Variante des komplizierten Ausdrucks Desoxyribonukleinsäure (daher auch DNS) steckt. Bis heute ist daraus die berühmteste Abkürzung der Wissenschaft geworden, und zwar deshalb, weil 1953 – in dem Jahr, in dem Stalin starb, Elisabeth II. Königin von England wurde und zum ersten Mal Menschen auf dem Mount Everest standen – erkannt wurde, wie herrlich gebaut diese DNA ist, nämlich als Doppelhelix mit einer lange Folge von Bausteinen in der Mitte, in der die genetische Information des Lebens steckt, wie wir heute sagen und wie es damals noch niemand auszudrücken wusste.\nMit „Information“ ist das entscheidende Wort gefallen, ohne das die moderne Biologie unverständlich bliebe; sie hat es sich in derselben Zeit einverleibt, in der Norbert Wiener die Kybernetik der rückgekoppelten Maschinen entwarf und Shannon deren Kommunikation auslotete. Mit diesem Begriff gelangte die Biologie zu einem  weiteren folgenreichen Ergebnis, mit dem sie der Antwort auf die Frage „Was ist Leben?“ einen wichtigen Schritt näher kam.\nDiese Frage ist der Titel des Büchleins eines Nobelpreisträgers für Physik aus dem Jahre 1933. In ihm formuliert der aus Österreich stammende und damals im irischen Exil lebende Erwin Schrödinger, dass es die zentrale Aufgabe der Gene ist, Information zu enthalten und weiterzugeben, um den Ordnungszustand, den das Leben in einem Organismus erreicht hat, in der nächsten Generation wiederentstehen lassen zu können. Dieses bis heute immer wieder aufgelegte Büchlein wurde zunächst zwar nicht von vielen Biologen gelesen. Es erregte aber die Aufmerksamkeit derjenigen, die in den kommenden Jahren für den großen Triumph der jetzt exakt werdenden Molekularbiologie sorgen sollten, eben die Entdeckung der Doppelhelix aus DNA.\nDieser Durchbruch basierte auf der merkwürdigen Kooperation des Briten Francis Crick und des Amerikaners James Watson. Die Lektüre von __Was ist Leben?__ veranlasst Watson, sich auf die Natur der Gene zu konzentrieren, also die Struktur der DNA zu erkunden. 1952 wurde die damals bereits alte Einsicht, dass die DNA zum Erbmaterial gehört, durch die neue Erkenntnis erweitert, dass es mindestens eine Lebensform gibt, bei der ausschließlich DNA das Erbmaterial bildet. Watson sucht – und findet – das Laboratorium, in dem erstens mit der DNA gearbeitet wird und zweitens Strukturbestimmungen der DNA möglich sind, und im Februar 1953 kennt man die Doppelhelix und ihren Trick. Sie speichert ihre Information als Kette von sogenannten Basen, die das Alphabet des Lebens ergeben, wie man bald sagt, weil man jetzt das Biologische als eine Welt des Austauschs von Informationen versteht – wie die Welt der Maschinen, die für uns rechnen und mit denen wir schreiben oder im Internet surfen.\nDarin, dass das Konzept Information gleichzeitig in der Sphäre der Maschinen und in der des Lebens triumphiert, liegt freilich auch eine Gefahr, nämlich dass wir auch andere Aspekte rasch und rücksichtslos von der einen Sphäre in die andere übertragen. Sehr verbreitet ist zum Beispiel die Vorstellung, dass im Leben, wenn es sich entwickelt und Formen annimmt, ein genetisches Programm ablaufe. Schließlich müssen auch die Computer anständig programmiert werden, wenn sie funktionieren sollen. An dieser Stelle wird die Ansicht vertreten, dass es zwar überall Informationen gibt, dass sie aber nicht immer einem Programm gehorchen. Im Leben jedenfalls ist es nicht so. Leben funktioniert nicht wie eine Maschine, es funktioniert eher wie ein Kunstwerk, das sich nach einer Idee nur in der Wechselwirkung zwischen Künstler und Werk entwickeln kann.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Biologie gibt es als Fach seit 1800, und die Botaniker und Zoologen und ihre Kollegen aus den Naturkundemuseen haben über Jahrhunderte hin fein und vorsichtig die Vielfalt des Lebens beschrieben, benannt und katalogisiert, ohne allzu viel davon zu verstehen. In den 1930er Jahren gesellten sich ihnen die ersten Physiker und Chemiker zu, die ihre Methoden und Fragestellungen mitbrachten und genauer wissen wollten, welche Molekülsorten in Zellen gebraucht werden, woraus diese Lebensbausteine aufgebaut sind und ob man erklären könne, was sie gleichzeitig stabil für das Individuum und dynamisch für die Evolution macht. Als neuen Namen für dieses quantitativ vorgehende und präzise werdende Erkunden des Lebens wählten sie in den späten 1930er Jahren die Bezeichnung „Molekularbiologie“. Dies passierte also noch vor dem Zweiten Weltkrieg, der dann aber nahezu allen Forschungsschwung dämpfte. Das Militär war mehr an Maschinen als an Molekülen interessiert, und so lenkten viele Forscher ihr Interesse von den Lebewesen auf die benötigten Maschinen um.\nDas heißt, die genetische Forschung – also das wissenschaftliche Bemühen um das Verständnis des Erbmaterials, für das man inzwischen den Namen Gene benutzte – lief auf Sparflamme weiter. Trotzdem brachten ihre Betreiber bis 1945 ausbaubare Ergebnisse zustande, die historische Weichenstellungen erlaubten. Eines besagt, dass sich bei Genen immer ein Stoff finden läßt, den die Chemiker seit dem 19. Jahrhundert als Nukleinsäuren kannten, und zwar genauer die Sorte, die mit den drei Buchstaben DNA abgekürzt wird, hinter denen die englische Variante des komplizierten Ausdrucks Desoxyribonukleinsäure (daher auch DNS) steckt. Bis heute ist daraus die berühmteste Abkürzung der Wissenschaft geworden, und zwar deshalb, weil 1953 – in dem Jahr, in dem Stalin starb, Elisabeth II. Königin von England wurde und zum ersten Mal Menschen auf dem Mount Everest standen – erkannt wurde, wie herrlich gebaut diese DNA ist, nämlich als Doppelhelix mit einer lange Folge von Bausteinen in der Mitte, in der die genetische Information des Lebens steckt, wie wir heute sagen und wie es damals noch niemand auszudrücken wusste.\nMit „Information“ ist das entscheidende Wort gefallen, ohne das die moderne Biologie unverständlich bliebe; sie hat es sich in derselben Zeit einverleibt, in der Norbert Wiener die Kybernetik der rückgekoppelten Maschinen entwarf und Shannon deren Kommunikation auslotete. Mit diesem Begriff gelangte die Biologie zu einem  weiteren folgenreichen Ergebnis, mit dem sie der Antwort auf die Frage „Was ist Leben?“ einen wichtigen Schritt näher kam.\nDiese Frage ist der Titel des Büchleins eines Nobelpreisträgers für Physik aus dem Jahre 1933. In ihm formuliert der aus Österreich stammende und damals im irischen Exil lebende Erwin Schrödinger, dass es die zentrale Aufgabe der Gene ist, Information zu enthalten und weiterzugeben, um den Ordnungszustand, den das Leben in einem Organismus erreicht hat, in der nächsten Generation wiederentstehen lassen zu können. Dieses bis heute immer wieder aufgelegte Büchlein wurde zunächst zwar nicht von vielen Biologen gelesen. Es erregte aber die Aufmerksamkeit derjenigen, die in den kommenden Jahren für den großen Triumph der jetzt exakt werdenden Molekularbiologie sorgen sollten, eben die Entdeckung der Doppelhelix aus DNA.\nDieser Durchbruch basierte auf der merkwürdigen Kooperation des Briten Francis Crick und des Amerikaners James Watson. Die Lektüre von __Was ist Leben?__ veranlasst Watson, sich auf die Natur der Gene zu konzentrieren, also die Struktur der DNA zu erkunden. 1952 wurde die damals bereits alte Einsicht, dass die DNA zum Erbmaterial gehört, durch die neue Erkenntnis erweitert, dass es mindestens eine Lebensform gibt, bei der ausschließlich DNA das Erbmaterial bildet. Watson sucht – und findet – das Laboratorium, in dem erstens mit der DNA gearbeitet wird und zweitens Strukturbestimmungen der DNA möglich sind, und im Februar 1953 kennt man die Doppelhelix und ihren Trick. Sie speichert ihre Information als Kette von sogenannten Basen, die das Alphabet des Lebens ergeben, wie man bald sagt, weil man jetzt das Biologische als eine Welt des Austauschs von Informationen versteht – wie die Welt der Maschinen, die für uns rechnen und mit denen wir schreiben oder im Internet surfen.\nDarin, dass das Konzept Information gleichzeitig in der Sphäre der Maschinen und in der des Lebens triumphiert, liegt freilich auch eine Gefahr, nämlich dass wir auch andere Aspekte rasch und rücksichtslos von der einen Sphäre in die andere übertragen. Sehr verbreitet ist zum Beispiel die Vorstellung, dass im Leben, wenn es sich entwickelt und Formen annimmt, ein genetisches Programm ablaufe. Schließlich müssen auch die Computer anständig programmiert werden, wenn sie funktionieren sollen. An dieser Stelle wird die Ansicht vertreten, dass es zwar überall Informationen gibt, dass sie aber nicht immer einem Programm gehorchen. Im Leben jedenfalls ist es nicht so. Leben funktioniert nicht wie eine Maschine, es funktioniert eher wie ein Kunstwerk, das sich nach einer Idee nur in der Wechselwirkung zwischen Künstler und Werk entwickeln kann.",
      "id": [
        "145797"
      ],
      "item": "Was ist Leben?"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145798]\n[%Erwin Schrödinger]\nDer Wiener Erwin Schrödinger (1887-1961), Nobelpreisträger von 1933, war einer der ganz großen Physiker des 20. Jahrhunderts.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der Wiener Erwin Schrödinger (1887-1961), Nobelpreisträger von 1933, war einer der ganz großen Physiker des 20. Jahrhunderts.",
      "id": [
        "145798"
      ],
      "item": "Erwin Schrödinger"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145799]\n[%Der Code der Chromosomen]\nDer Physiker Erwin Schrödinger wusste intuitiv, dass Erbanlagen mehr sind als ein Code und dass es ziemliche Mühe macht, ihre ganze Qualität zu verstehen. Er drückt dies mit dem Satz aus: „Der Begriff ‚Code’ [den er für die Struktur der Chromosomen eingeführt hat] ist selbstverständlich zu eng. Die [Erbanlagen] tragen gleichzeitig dazu bei, die Entwicklung, welche sie ahnen lassen, hervorzubringen. Sie sind zugleich Gesetzbuch und ausübende Gewalt, Plan der Architekten und Handwerker des Baumeisters.“\nSchrödinger stellt physikalische Fragen an die biologische Wissenschaft, und sie betreffen zwei Aspekte. Da ist zum einen die Frage, wie es Genen bzw. Chromosomen gelingt, nicht nur von Generation zu Generation, sondern über die Jahrmillionen des evolutionären Werdens von Leben im Wesentlichen stabil zu bleiben und nur wenige Mutationen (Veränderungen) zuzulassen, um Anpassungen vornehmen zu können. Und da ist zum zweiten das Rätsel, das seit Langem „der Menschheit so viel zu schaffen gemacht“ hat, nämlich wie sich ein lebender Organismus dem Zerfall entzieht, den die Physik als Naturgesetz erkannt und als zweiten Hauptsatz der Thermodynamik formuliert hat.\nIm ersten Fall antwortet Schrödinger mit Hilfe der Arbeiten, die der aus Berlin stammende Max Delbrück bereits in der Mitte der 1930er Jahre vorgelegt hatte, als er sich mit dem russischen Genetiker Nikolaj Timofejew-Ressowski und dem deutschen Physiker K.G. Zimmer gemeinsam mit der __Natur der Genmutation und der Genstruktur__ befasste. Das Trio zeigte dabei, dass die damals neue Quantenphysik in der Lage ist, die Stabilität von molekularen Strukturen – von Atomverbänden, wie Delbrück es nannte – zu erklären, was uns direkt zu der wichtigeren Frage leitet, wie es diese Atomverbände als Gene schaffen, sich dem physikalischen Gesetz der zunehmenden Entropie (Unordnung) zu entziehen. Schrödinger meint, dass die Lösung darin liege, dass sich Lebewesen von „negativer Entropie“ ernähren, was man auch weniger paradox durch den Satz ausdrücken kann, dass es einem Organismus gelingt, „sich von der Entropie zu befreien, die er, solange er lebt, erzeugen muss.“",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der Physiker Erwin Schrödinger wusste intuitiv, dass Erbanlagen mehr sind als ein Code und dass es ziemliche Mühe macht, ihre ganze Qualität zu verstehen. Er drückt dies mit dem Satz aus: „Der Begriff ‚Code’ [den er für die Struktur der Chromosomen eingeführt hat] ist selbstverständlich zu eng. Die [Erbanlagen] tragen gleichzeitig dazu bei, die Entwicklung, welche sie ahnen lassen, hervorzubringen. Sie sind zugleich Gesetzbuch und ausübende Gewalt, Plan der Architekten und Handwerker des Baumeisters.“\nSchrödinger stellt physikalische Fragen an die biologische Wissenschaft, und sie betreffen zwei Aspekte. Da ist zum einen die Frage, wie es Genen bzw. Chromosomen gelingt, nicht nur von Generation zu Generation, sondern über die Jahrmillionen des evolutionären Werdens von Leben im Wesentlichen stabil zu bleiben und nur wenige Mutationen (Veränderungen) zuzulassen, um Anpassungen vornehmen zu können. Und da ist zum zweiten das Rätsel, das seit Langem „der Menschheit so viel zu schaffen gemacht“ hat, nämlich wie sich ein lebender Organismus dem Zerfall entzieht, den die Physik als Naturgesetz erkannt und als zweiten Hauptsatz der Thermodynamik formuliert hat.\nIm ersten Fall antwortet Schrödinger mit Hilfe der Arbeiten, die der aus Berlin stammende Max Delbrück bereits in der Mitte der 1930er Jahre vorgelegt hatte, als er sich mit dem russischen Genetiker Nikolaj Timofejew-Ressowski und dem deutschen Physiker K.G. Zimmer gemeinsam mit der __Natur der Genmutation und der Genstruktur__ befasste. Das Trio zeigte dabei, dass die damals neue Quantenphysik in der Lage ist, die Stabilität von molekularen Strukturen – von Atomverbänden, wie Delbrück es nannte – zu erklären, was uns direkt zu der wichtigeren Frage leitet, wie es diese Atomverbände als Gene schaffen, sich dem physikalischen Gesetz der zunehmenden Entropie (Unordnung) zu entziehen. Schrödinger meint, dass die Lösung darin liege, dass sich Lebewesen von „negativer Entropie“ ernähren, was man auch weniger paradox durch den Satz ausdrücken kann, dass es einem Organismus gelingt, „sich von der Entropie zu befreien, die er, solange er lebt, erzeugen muss.“",
      "id": [
        "145799"
      ],
      "item": "Der Code der Chromosomen"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145800]\n[%Frühe Formen der Interdisziplinarität]\nIm Buch __Was ist Leben?__ weist der Physiker Erwin Schrödinger bereits 1945 auf den Gegensatz zwischen den einzelnen Disziplinen der Wissenschaft und dem „Streben nach einem ganzheitlichen, alles umfassenden Wissen“ hin, das die Menschen seit frühesten Zeiten auszeichne. Er hält es für die Pflicht der Forscher, immer wieder den Versuch zu unternehmen, „unser gesamtes Wissensgut zu einer Ganzheit zu verbinden“. Da er ihn selbst unternimmt, weiß er auch, welches Risiko ihn erwartet, wenn er mit „Wissen aus zweiter Hand“ umgeht. Das Risiko besteht darin, „sich lächerlich zu machen“. Doch dies muss man aushalten. \nIn den folgenden Jahren konnte man lernen, wozu Interdisziplinarität in der Lage ist. Die Struktur der DNA, die Ikone der Doppelhelix, verdankt sich allein solch einer Kombination aus Beiträgen von Physikern, klassischen Chemikern, Biochemikern, Kristallographen, Bakteriologen und anderen, und wenn James Watson und Francis Crick der Vorwurf gemacht wird, sie hätten nur zusammengeklaubt, was andere hervorgebracht hätten, dann darf man antworten, dass das Geheimnis der Interdisziplinarität nicht darin besteht, alle Experimente selbst zu machen und alle Fakten selbst zu sammeln. Das Geheimnis besteht vielmehr darin, den Mut und die Geduld zu haben, auf die Daten bzw. Ergebnisse zu warten, die dem Problem angemessen sind, um sie dann in einem Lösungsvorschlag vorzustellen. Dabei kann man sich blamieren, wie Watson und Crick vor ihrem Triumph höchst bitter erfahren mussten. Dabei kann man aber auch das große Los ziehen, wie sie es zuletzt erleben durften. Denn während die anderen weitere Informationen anhäuften, wussten sie, wann ihnen genug Tatsachen bekannt waren, mit denen das Denken beginnen konnte und sich neue Wege finden liessen. Informationen sind also nicht alles. Mit ihnen fängt aber alles an.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Im Buch __Was ist Leben?__ weist der Physiker Erwin Schrödinger bereits 1945 auf den Gegensatz zwischen den einzelnen Disziplinen der Wissenschaft und dem „Streben nach einem ganzheitlichen, alles umfassenden Wissen“ hin, das die Menschen seit frühesten Zeiten auszeichne. Er hält es für die Pflicht der Forscher, immer wieder den Versuch zu unternehmen, „unser gesamtes Wissensgut zu einer Ganzheit zu verbinden“. Da er ihn selbst unternimmt, weiß er auch, welches Risiko ihn erwartet, wenn er mit „Wissen aus zweiter Hand“ umgeht. Das Risiko besteht darin, „sich lächerlich zu machen“. Doch dies muss man aushalten. \nIn den folgenden Jahren konnte man lernen, wozu Interdisziplinarität in der Lage ist. Die Struktur der DNA, die Ikone der Doppelhelix, verdankt sich allein solch einer Kombination aus Beiträgen von Physikern, klassischen Chemikern, Biochemikern, Kristallographen, Bakteriologen und anderen, und wenn James Watson und Francis Crick der Vorwurf gemacht wird, sie hätten nur zusammengeklaubt, was andere hervorgebracht hätten, dann darf man antworten, dass das Geheimnis der Interdisziplinarität nicht darin besteht, alle Experimente selbst zu machen und alle Fakten selbst zu sammeln. Das Geheimnis besteht vielmehr darin, den Mut und die Geduld zu haben, auf die Daten bzw. Ergebnisse zu warten, die dem Problem angemessen sind, um sie dann in einem Lösungsvorschlag vorzustellen. Dabei kann man sich blamieren, wie Watson und Crick vor ihrem Triumph höchst bitter erfahren mussten. Dabei kann man aber auch das große Los ziehen, wie sie es zuletzt erleben durften. Denn während die anderen weitere Informationen anhäuften, wussten sie, wann ihnen genug Tatsachen bekannt waren, mit denen das Denken beginnen konnte und sich neue Wege finden liessen. Informationen sind also nicht alles. Mit ihnen fängt aber alles an.",
      "id": [
        "145800"
      ],
      "item": "Frühe Formen der Interdisziplinarität"
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145801]\n[%2]\n[#Auf dem Weg zum Anfang; Vorgeschichte und Vorläufer]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "2",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 1,
      "progress": true,
      "toc": true,
      "id": [
        "145801"
      ],
      "title": "Auf dem Weg zum Anfang; Vorgeschichte und Vorläufer"
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145802]\n[##Vom Abacus zum Computer; Mechanisches und elektronisches Rechnen]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 2,
      "progress": true,
      "toc": true,
      "id": [
        "145802"
      ],
      "title": "Vom Abacus zum Computer; Mechanisches und elektronisches Rechnen"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145803]\n[%Einleitung]\nWenn das Informationszeitalter auch erst in der Mitte des 20. Jahrhunderts angebrochen ist, so ist es doch sinnvoll zurückzuschauen in seine Vorgeschichte, denn nichts ist voraussetzungslos, und die Entstehungsgeschichte lässt uns das, was da entstanden ist, besser verstehen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Wenn das Informationszeitalter auch erst in der Mitte des 20. Jahrhunderts angebrochen ist, so ist es doch sinnvoll zurückzuschauen in seine Vorgeschichte, denn nichts ist voraussetzungslos, und die Entstehungsgeschichte lässt uns das, was da entstanden ist, besser verstehen.",
      "id": [
        "145803"
      ],
      "item": "Einleitung"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145804]\n[%Das mechanische Rechnen]\nWir können so tief im Brunnen der von Menschen erlebten und gestalteten Vergangenheit beginnen, wie wir wollen, wir werden dabei stets auf Zahlen und den Umgang mit ihnen treffen. Das offenbar in uns angelegte Vermögen zum Zählen führte in den sich entfaltenden menschlichen Kulturen über frühe Rechentafeln erst zu Rechenhilfen, dann zu Rechengeräten und schließlich zu den allgemein informationsverarbeitenden Maschinen.\nFrühe Gesellschaften entwickelten erste Zahlenordnungen zum Beispiel für die Vermessung des Landes und die Erstellung von Steuerlisten. Um Steuererhebung ging es auch dem wichtigsten Mathematiker des europäischen Mittelalters, einem Mann namens Leonardo Fibonacci aus Pisa, der bald nach 1200 ein Rechenbuch – den __Liber abaci__ – schrieb und in ihm die arabischen Ziffern mit der indischen Null in unseren Kulturkreis einführte.\nDiese damals neuen und uns heute vertrauten Zeichen für Zahlen finden sich nicht nur in Leonardos Manuskripten, sondern zum Beispiel auch in dem magischen Quadrat, das Albrecht Dürer im 16. Jahrhundert in seine Melancholia einzeichnet – 1514, wie wir genau angeben können, da der Maler es uns zeigt.\nLeonardo griff nicht zuletzt deshalb auf die indisch-arabischen Ziffern zurück, weil er mit ihrer Hilfe besser die Vermehrung von Kaninchen berechnen konnte, deren zunehmende Anzahl eine Folge ergibt, die heute als Fibonacci-Folge bekannt ist. Mit Hilfe dieser Reihung konnte Leonardo prüfen, ob die von ihm und seinem Vater besuchten Bauern sämtliche Kaninchen zeigten oder einige versteckten, um auf diese Weise an den vorgeschriebenen Steuern zu sparen, die Leonardo erheben sollte.\nMit Hilfe der neuen Schreibweise fiel das Rechnen nicht nur ihm, sondern allen Menschen sehr viel leichter, wovon man sich selbst überzeugen kann, wenn man mit den altertümlichen römischen Ziffern etwa versucht, eine Division durchzuführen, deren Ergebnis nicht offensichtlich ist – MXLII durch IV zum Beispiel. Zugleich spornten die neuen Zahlen zum Bau besserer Rechenbretter an. Solche Rechenbretter waren vor allem bei Kaufleuten beliebt, die einen Preis fest- und Bilanzen anlegen wollten, und es gab sie seit den Tagen der Babylonier. Mit den indisch-arabischen Ziffern und der Zehnerschreibweise ließ sich überhaupt gut und systematisch über neue Apparaturen und mechanische Rechenverfahren nachdenken, und brauchbare Ergebnisse des Tüftelns zeigten sich spätestens im 17. Jahrhundert, als ganz allgemein __Die Geburt der modernen Wissenschaft in Europa__ gefeiert werden konnte, wie sie der italienische Historiker Paolo Rossi in seinem gleichnamigen Buch beschrieben hat.\nIn dieser Zeit fanden tatsächlich nicht nur die blutigen Glaubenskriege statt, von denen unsere Geschichtsbücher berichten. In ihr zeigte sich auch eine eigene und neue Lust an der Rationalisierung, durch die die Menschen angespornt wurden, neben besseren Uhrwerken auch erste Rechenuhren mit ähnlichen Zahnrädern anzufertigen. In Tübingen baute zum Beispiel Wilhelm Schickard, der zu den Freunden von Johannes Kepler zählte, in den Jahren des Dreißigjährigen Krieges – genauer: 1623 – eine Maschine mit siebzehn Zahnrädern, die alle vier Grundrechenarten beherrschte und die Ergebnisse ihrer Operationen auf sechs Stellen angeben konnte. Nach Schickard entwarf auch der Franzose Blaise Pascal eine Additionsmaschine, mit der die Geldsummen zusammengezählt werden sollten, die dem königlichen Steuereintreiber zustanden. Das Addieren stellte (und stellt) eine monotone und ermüdende Tätigkeit dar, von der Pascal seine Mitmenschen mittels einer Apparatur entlasten wollte.\nDen entscheidenden Schritt zur mechanischen Rechenmaschine vollzog dann – Karl H. Metz zufolge – der Universalgelehrte Gottfried Wilhelm Leibniz, der 1763 in London seine „Machina Arithmetica“ vorführte, in der die komplizierten Rechenarten – Multiplizieren und Dividieren – auf die einfachen – Addieren und Subtrahieren – zurückgeführt wurden. Leibniz nutzte dazu die von dem Schotten John Napier im Jahrhundert zuvor entwickelten Logarithmen, die genau diese Verschiebung – und mit ihr den Rechenschieber – ermöglichten und das rasch zu großen Zahlen führende Malnehmen durch das übersichtliche Zusammenzählen ersetzten. Wie alle Konstrukteure von mechanischen Rechenhilfen verfolgte auch Leibniz das Ziel, „den Geist des Menschen frei zu machen für höhere Dinge“, wie er einmal geschrieben hat. Dieses Vorhaben wirkte durch die kommenden Jahrhunderte bis in die Gegenwart hinein und bleibt nach wie vor gültig.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Wir können so tief im Brunnen der von Menschen erlebten und gestalteten Vergangenheit beginnen, wie wir wollen, wir werden dabei stets auf Zahlen und den Umgang mit ihnen treffen. Das offenbar in uns angelegte Vermögen zum Zählen führte in den sich entfaltenden menschlichen Kulturen über frühe Rechentafeln erst zu Rechenhilfen, dann zu Rechengeräten und schließlich zu den allgemein informationsverarbeitenden Maschinen.\nFrühe Gesellschaften entwickelten erste Zahlenordnungen zum Beispiel für die Vermessung des Landes und die Erstellung von Steuerlisten. Um Steuererhebung ging es auch dem wichtigsten Mathematiker des europäischen Mittelalters, einem Mann namens Leonardo Fibonacci aus Pisa, der bald nach 1200 ein Rechenbuch – den __Liber abaci__ – schrieb und in ihm die arabischen Ziffern mit der indischen Null in unseren Kulturkreis einführte.\nDiese damals neuen und uns heute vertrauten Zeichen für Zahlen finden sich nicht nur in Leonardos Manuskripten, sondern zum Beispiel auch in dem magischen Quadrat, das Albrecht Dürer im 16. Jahrhundert in seine Melancholia einzeichnet – 1514, wie wir genau angeben können, da der Maler es uns zeigt.\nLeonardo griff nicht zuletzt deshalb auf die indisch-arabischen Ziffern zurück, weil er mit ihrer Hilfe besser die Vermehrung von Kaninchen berechnen konnte, deren zunehmende Anzahl eine Folge ergibt, die heute als Fibonacci-Folge bekannt ist. Mit Hilfe dieser Reihung konnte Leonardo prüfen, ob die von ihm und seinem Vater besuchten Bauern sämtliche Kaninchen zeigten oder einige versteckten, um auf diese Weise an den vorgeschriebenen Steuern zu sparen, die Leonardo erheben sollte.\nMit Hilfe der neuen Schreibweise fiel das Rechnen nicht nur ihm, sondern allen Menschen sehr viel leichter, wovon man sich selbst überzeugen kann, wenn man mit den altertümlichen römischen Ziffern etwa versucht, eine Division durchzuführen, deren Ergebnis nicht offensichtlich ist – MXLII durch IV zum Beispiel. Zugleich spornten die neuen Zahlen zum Bau besserer Rechenbretter an. Solche Rechenbretter waren vor allem bei Kaufleuten beliebt, die einen Preis fest- und Bilanzen anlegen wollten, und es gab sie seit den Tagen der Babylonier. Mit den indisch-arabischen Ziffern und der Zehnerschreibweise ließ sich überhaupt gut und systematisch über neue Apparaturen und mechanische Rechenverfahren nachdenken, und brauchbare Ergebnisse des Tüftelns zeigten sich spätestens im 17. Jahrhundert, als ganz allgemein __Die Geburt der modernen Wissenschaft in Europa__ gefeiert werden konnte, wie sie der italienische Historiker Paolo Rossi in seinem gleichnamigen Buch beschrieben hat.\nIn dieser Zeit fanden tatsächlich nicht nur die blutigen Glaubenskriege statt, von denen unsere Geschichtsbücher berichten. In ihr zeigte sich auch eine eigene und neue Lust an der Rationalisierung, durch die die Menschen angespornt wurden, neben besseren Uhrwerken auch erste Rechenuhren mit ähnlichen Zahnrädern anzufertigen. In Tübingen baute zum Beispiel Wilhelm Schickard, der zu den Freunden von Johannes Kepler zählte, in den Jahren des Dreißigjährigen Krieges – genauer: 1623 – eine Maschine mit siebzehn Zahnrädern, die alle vier Grundrechenarten beherrschte und die Ergebnisse ihrer Operationen auf sechs Stellen angeben konnte. Nach Schickard entwarf auch der Franzose Blaise Pascal eine Additionsmaschine, mit der die Geldsummen zusammengezählt werden sollten, die dem königlichen Steuereintreiber zustanden. Das Addieren stellte (und stellt) eine monotone und ermüdende Tätigkeit dar, von der Pascal seine Mitmenschen mittels einer Apparatur entlasten wollte.\nDen entscheidenden Schritt zur mechanischen Rechenmaschine vollzog dann – Karl H. Metz zufolge – der Universalgelehrte Gottfried Wilhelm Leibniz, der 1763 in London seine „Machina Arithmetica“ vorführte, in der die komplizierten Rechenarten – Multiplizieren und Dividieren – auf die einfachen – Addieren und Subtrahieren – zurückgeführt wurden. Leibniz nutzte dazu die von dem Schotten John Napier im Jahrhundert zuvor entwickelten Logarithmen, die genau diese Verschiebung – und mit ihr den Rechenschieber – ermöglichten und das rasch zu großen Zahlen führende Malnehmen durch das übersichtliche Zusammenzählen ersetzten. Wie alle Konstrukteure von mechanischen Rechenhilfen verfolgte auch Leibniz das Ziel, „den Geist des Menschen frei zu machen für höhere Dinge“, wie er einmal geschrieben hat. Dieses Vorhaben wirkte durch die kommenden Jahrhunderte bis in die Gegenwart hinein und bleibt nach wie vor gültig.",
      "id": [
        "145804"
      ],
      "item": "Das mechanische Rechnen"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145805]\n[%Fibonacci-Folge]\nFibonacci-Folge wird die Folge von Zahlen genannt, die jeweils die Summe der beiden vorhergehenden Zahlen sind. Die Reihe ist also diese: 0,1,1,2,3,5,8,13,21 usw.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Fibonacci-Folge wird die Folge von Zahlen genannt, die jeweils die Summe der beiden vorhergehenden Zahlen sind. Die Reihe ist also diese: 0,1,1,2,3,5,8,13,21 usw.",
      "id": [
        "145805"
      ],
      "item": "Fibonacci-Folge"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145806]\n[%Blaise Pascal]\nDer Franzose Blaise Pascal (1623-1662), war einer der bedeutendsten Mathematiker und Physiker des 17. Jahrhunderts, außerdem ein brillant schreibender tief religiöser Moralist.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der Franzose Blaise Pascal (1623-1662), war einer der bedeutendsten Mathematiker und Physiker des 17. Jahrhunderts, außerdem ein brillant schreibender tief religiöser Moralist.",
      "id": [
        "145806"
      ],
      "item": "Blaise Pascal"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145807]\n[%Lochkarten]\nIm 19. Jahrhundert findet eine __Verwandlung der Welt__ statt, wie der Historiker Jürgen Osterhammel sein umfangreiches Werk über diese Zeit nennt, und zu den großen Veränderungen, die nach 1800 aufkommen, gehört das statistische Denken, mit dessen Hilfe der Zufall durch Berechnung gebändigt werden sollte. Schriften, die nach der Häufigkeit oder Wahrscheinlichkeit fragten, mit der beim Würfeln bestimmte Zahlen oder Kombinationen fallen oder mit der sich Eigenschaften von Eltern auf ihre Nachkommen vererben, hatte es schon länger gegeben, und sie waren zumeist in der Absicht unternommen worden, bei den aufgeworfenen Themen Gewissheit zu erlangen. Dass diese aber nicht zu erreichen ist, dass es demnach so etwas wie sichere Voraussagen nicht gibt – diese Einsicht tauchte erst am Ende des 18. Jahrhunderts auf, und Benjamin Franklin drückte das dazugehörige Lebensgefühl 1789 in dem berühmten Satz aus, dass nichts in dieser Welt sicher sei außer dem Tod und den Steuern.\nVor diesem Hintergrund begann die Wissenschaft nun ernsthaft, eine Mathematik der Wahrscheinlichkeit zu entwickeln, um den Zufall so berechenbar wie möglich zu machen. Dabei machte sie zwar sofort die Entdeckung, dass statistisches Denken nicht nur von den Dingen selbst abhängt, sondern auch von den Kenntnissen, die wir von den insgesamt viel zu vielen Dingen allenfalls erlangen können. Eine Wahrscheinlichkeit ändert sich mit den Informationen, die man erhält oder erhalten kann – dieser heute wenig überraschende Sachverhalt zeigt, dass es mit der Objektivität in der Naturbeschreibung irgendwo zu Ende geht. Schließlich sind es Subjekte, die Informationen sammeln und über sie verfügen.\nDas Denken in Wahrscheinlichkeiten beginnt im 19. Jahrhundert mit der Berechnung der Sterblichkeit. Warum bleibt rätselhaft, denn es gab andere Phänomene – Feuersbrünste, Schiffbrüche, Epidemien –, die mindestens von gleicher praktischer Bedeutung für das Leben der Menschen waren. Aber die historischen Tatsachen zeigen, dass sich die ersten Statistiker um die Sterbehäufigkeit sorgten. \nSo ging es dem großen Mathematiker Carl Friedrich Gauß darum, zu erfahren und zu berechnen, ob sich im Falle seines frühzeitigen Ablebens noch genügend Geld in der Staatskasse für die Rentenzahlungen an seine Witwe befinden würde. Wie groß war überhaupt die Wahrscheinlichkeit für jemanden, der im Dienste des Staates stand, eine angemessene Rente zu bekommen?\nDank der Berechnung von Wahrscheinlichkeiten konnte das mathematisch unterfütterte Versicherungswesen entstehen. Und das – nicht zuletzt von Gauß – entwickelte statistische Denken mit seinen dazugehörigen rechnerischen Methoden ermöglichte eine neue Physik, die man Wärmelehre oder Thermodynamik nannte und die bei maschinellen Anwendungen in der zweiten Hälfte des 19. Jahrhunderts das Konzept der Entropie hervorbrachte, aus der ihrerseits in der Mitte des 20. Jahrhunderts das der Information hervorgegangen ist.\nEs waren die Versicherungen, die bei den Unternehmen, die sie anbieten wollten, natürlich einen großen Rechenbedarf hervorrufen mussten. Es leuchtet daher ein, wenn man erfährt, dass um 1820 die Serienanfertigung von mechanischen Rechengeräten begann und um die Mitte des Jahrhunderts in zwölf Monaten bereits gut 100 Exemplare von ihnen verkauft wurden – vor allem eben an Versicherer.\nZeitgleich denkt zum ersten Mal ein Gelehrter darüber nach, wie man eine Rechenmaschine nicht nur mit den stets selben Funktionen, sondern steuerbar betreiben kann. Gemeint ist der englische Mathematiker und Erfinder Charles Babbage, der in seiner Jugend Sterbetafeln für Versicherungen auf statistischer Grundlage aufgestellt hat. Er träumt nach 1820 von einer „Difference Engine“, deren Operationen durch Informationen – wie wir heute sagen würden – sowohl festgelegt als auch verändert werden können. Babbage sieht auch technisch einen Weg, wie man einer Maschine Instruktionen zukommen lassen kann, nämlich mit Karten, die geeignet gelocht sind. Mit dem dabei erzeugten Muster kann man den gewünschten Rechenvorgang anstoßen. Die Folge der Zeichen, die den Rechenapparat steuern und ihm seine Aufgaben zukommen lassen, kennt man heute als Programm. Das heißt, wenn wir sagen, dass etwas programmiert abläuft, dann meinen wir, dass es neben dem Vorgang in der Maschine (heute: Hardware) selbst noch etwas Zweites gibt – einen Text, eine Reihe von Mustern aus Löchern, oder, im heutigen Sprachgebrauch: eine Software. Diese Software kann Schritt für Schritt mit dem anvisierten Ablauf verglichen werden kann, weil sie zu ihm isomorph ist, wie man sagt. Das heißt, dass das eine – das Programm – und das andere – der Prozess – eins zu eins aufeinander abgebildet und zueinander in Beziehung gesetzt werden können. Dann und nur dann können wir sagen, dass etwas programmiert abläuft.\nDie Idee zu den Lochkarten hatte Babbage aus Frankreich, wo man mit diesem Verfahren angefangen hatte, Webstühle zu steuern. Und die Weber und ihre Mechaniker wiederum hatten das Schema von den ehrgeizigen Bastlern abgeschaut, die Musikautomaten oder mechanische Tiere zur Schau stellten, in denen es eine Walze gab, auf der das „Verhaltensprogramm“ oder eine Melodie in Form von kleinen Stiften „gespeichert“ vorlag. Sie ergaben ein Muster, das mechanisch übertragen und mit Rädchen und Stangen ausgeführt wurde. Die Lochkarten, aus denen später Lochstreifen wurden, gingen über die durch ihren Umfang beschränkte Walze hinaus, da die neuen Informations- oder Programmträger leicht zu vermehren oder zu verlängern waren. Auf ihnen konnte man bald eingeben und festhalten, was heute Rechenprogramm im Speziellen oder Computerprogramm im Allgemeinen heißt. Die „Difference Engine“ von Babbage sollte – in den Worten von Lady Ada Lovelace, der ersten Programmiererin der Welt – „algebraische Muster weben, so wie ein Webstuhl Blumen und Blätter webt.“\nÜbrigens – wenn man ein programmierbares Verfahren zur Lösung einer Aufgabe, etwa der Berechnung einer bestimmten Zahl, der Lösung einer Gleichung entwerfen und einsetzen kann, dann nennt man diese Handlungsvorschrift einen Algorithmus.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Im 19. Jahrhundert findet eine __Verwandlung der Welt__ statt, wie der Historiker Jürgen Osterhammel sein umfangreiches Werk über diese Zeit nennt, und zu den großen Veränderungen, die nach 1800 aufkommen, gehört das statistische Denken, mit dessen Hilfe der Zufall durch Berechnung gebändigt werden sollte. Schriften, die nach der Häufigkeit oder Wahrscheinlichkeit fragten, mit der beim Würfeln bestimmte Zahlen oder Kombinationen fallen oder mit der sich Eigenschaften von Eltern auf ihre Nachkommen vererben, hatte es schon länger gegeben, und sie waren zumeist in der Absicht unternommen worden, bei den aufgeworfenen Themen Gewissheit zu erlangen. Dass diese aber nicht zu erreichen ist, dass es demnach so etwas wie sichere Voraussagen nicht gibt – diese Einsicht tauchte erst am Ende des 18. Jahrhunderts auf, und Benjamin Franklin drückte das dazugehörige Lebensgefühl 1789 in dem berühmten Satz aus, dass nichts in dieser Welt sicher sei außer dem Tod und den Steuern.\nVor diesem Hintergrund begann die Wissenschaft nun ernsthaft, eine Mathematik der Wahrscheinlichkeit zu entwickeln, um den Zufall so berechenbar wie möglich zu machen. Dabei machte sie zwar sofort die Entdeckung, dass statistisches Denken nicht nur von den Dingen selbst abhängt, sondern auch von den Kenntnissen, die wir von den insgesamt viel zu vielen Dingen allenfalls erlangen können. Eine Wahrscheinlichkeit ändert sich mit den Informationen, die man erhält oder erhalten kann – dieser heute wenig überraschende Sachverhalt zeigt, dass es mit der Objektivität in der Naturbeschreibung irgendwo zu Ende geht. Schließlich sind es Subjekte, die Informationen sammeln und über sie verfügen.\nDas Denken in Wahrscheinlichkeiten beginnt im 19. Jahrhundert mit der Berechnung der Sterblichkeit. Warum bleibt rätselhaft, denn es gab andere Phänomene – Feuersbrünste, Schiffbrüche, Epidemien –, die mindestens von gleicher praktischer Bedeutung für das Leben der Menschen waren. Aber die historischen Tatsachen zeigen, dass sich die ersten Statistiker um die Sterbehäufigkeit sorgten. \nSo ging es dem großen Mathematiker Carl Friedrich Gauß darum, zu erfahren und zu berechnen, ob sich im Falle seines frühzeitigen Ablebens noch genügend Geld in der Staatskasse für die Rentenzahlungen an seine Witwe befinden würde. Wie groß war überhaupt die Wahrscheinlichkeit für jemanden, der im Dienste des Staates stand, eine angemessene Rente zu bekommen?\nDank der Berechnung von Wahrscheinlichkeiten konnte das mathematisch unterfütterte Versicherungswesen entstehen. Und das – nicht zuletzt von Gauß – entwickelte statistische Denken mit seinen dazugehörigen rechnerischen Methoden ermöglichte eine neue Physik, die man Wärmelehre oder Thermodynamik nannte und die bei maschinellen Anwendungen in der zweiten Hälfte des 19. Jahrhunderts das Konzept der Entropie hervorbrachte, aus der ihrerseits in der Mitte des 20. Jahrhunderts das der Information hervorgegangen ist.\nEs waren die Versicherungen, die bei den Unternehmen, die sie anbieten wollten, natürlich einen großen Rechenbedarf hervorrufen mussten. Es leuchtet daher ein, wenn man erfährt, dass um 1820 die Serienanfertigung von mechanischen Rechengeräten begann und um die Mitte des Jahrhunderts in zwölf Monaten bereits gut 100 Exemplare von ihnen verkauft wurden – vor allem eben an Versicherer.\nZeitgleich denkt zum ersten Mal ein Gelehrter darüber nach, wie man eine Rechenmaschine nicht nur mit den stets selben Funktionen, sondern steuerbar betreiben kann. Gemeint ist der englische Mathematiker und Erfinder Charles Babbage, der in seiner Jugend Sterbetafeln für Versicherungen auf statistischer Grundlage aufgestellt hat. Er träumt nach 1820 von einer „Difference Engine“, deren Operationen durch Informationen – wie wir heute sagen würden – sowohl festgelegt als auch verändert werden können. Babbage sieht auch technisch einen Weg, wie man einer Maschine Instruktionen zukommen lassen kann, nämlich mit Karten, die geeignet gelocht sind. Mit dem dabei erzeugten Muster kann man den gewünschten Rechenvorgang anstoßen. Die Folge der Zeichen, die den Rechenapparat steuern und ihm seine Aufgaben zukommen lassen, kennt man heute als Programm. Das heißt, wenn wir sagen, dass etwas programmiert abläuft, dann meinen wir, dass es neben dem Vorgang in der Maschine (heute: Hardware) selbst noch etwas Zweites gibt – einen Text, eine Reihe von Mustern aus Löchern, oder, im heutigen Sprachgebrauch: eine Software. Diese Software kann Schritt für Schritt mit dem anvisierten Ablauf verglichen werden kann, weil sie zu ihm isomorph ist, wie man sagt. Das heißt, dass das eine – das Programm – und das andere – der Prozess – eins zu eins aufeinander abgebildet und zueinander in Beziehung gesetzt werden können. Dann und nur dann können wir sagen, dass etwas programmiert abläuft.\nDie Idee zu den Lochkarten hatte Babbage aus Frankreich, wo man mit diesem Verfahren angefangen hatte, Webstühle zu steuern. Und die Weber und ihre Mechaniker wiederum hatten das Schema von den ehrgeizigen Bastlern abgeschaut, die Musikautomaten oder mechanische Tiere zur Schau stellten, in denen es eine Walze gab, auf der das „Verhaltensprogramm“ oder eine Melodie in Form von kleinen Stiften „gespeichert“ vorlag. Sie ergaben ein Muster, das mechanisch übertragen und mit Rädchen und Stangen ausgeführt wurde. Die Lochkarten, aus denen später Lochstreifen wurden, gingen über die durch ihren Umfang beschränkte Walze hinaus, da die neuen Informations- oder Programmträger leicht zu vermehren oder zu verlängern waren. Auf ihnen konnte man bald eingeben und festhalten, was heute Rechenprogramm im Speziellen oder Computerprogramm im Allgemeinen heißt. Die „Difference Engine“ von Babbage sollte – in den Worten von Lady Ada Lovelace, der ersten Programmiererin der Welt – „algebraische Muster weben, so wie ein Webstuhl Blumen und Blätter webt.“\nÜbrigens – wenn man ein programmierbares Verfahren zur Lösung einer Aufgabe, etwa der Berechnung einer bestimmten Zahl, der Lösung einer Gleichung entwerfen und einsetzen kann, dann nennt man diese Handlungsvorschrift einen Algorithmus.",
      "id": [
        "145807"
      ],
      "item": "Lochkarten"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145808]\n[%Carl Friedrich Gauß]\nCarl Friedrich Gauß (1777-1855) studierte und wirkte in Göttingen. Er war einer der bedeutendsten Mathematiker des 19. Jahrhunderts, außerdem Geodät und Astronom.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Carl Friedrich Gauß (1777-1855) studierte und wirkte in Göttingen. Er war einer der bedeutendsten Mathematiker des 19. Jahrhunderts, außerdem Geodät und Astronom.",
      "id": [
        "145808"
      ],
      "item": "Carl Friedrich Gauß"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145809]\n[%Charles Babbage und Ada Lovelace]\nCharles Babbage, 1791-1871, Mathematiker und Ökonom, entwarf programmierbare mechanische Rechenmaschinen. Babbages Mitarbeiterin Ada Lovelace (1815-1852), eine Tochter des Dichters Lord Byron, erdachte einen Algorithmus für Babbages Maschine; sie kann somit als die erste Programmiererin gelten.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Charles Babbage, 1791-1871, Mathematiker und Ökonom, entwarf programmierbare mechanische Rechenmaschinen. Babbages Mitarbeiterin Ada Lovelace (1815-1852), eine Tochter des Dichters Lord Byron, erdachte einen Algorithmus für Babbages Maschine; sie kann somit als die erste Programmiererin gelten.",
      "id": [
        "145809"
      ],
      "item": "Charles Babbage und Ada Lovelace"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145810]\n[%Auf dem Weg zum Computer]\nCharles Babbages (Mathematiker) und Ada Lovelaces (Programmiererin) Gedanken und Konzeptionen zu programmierbaren mechanischen Rechenmaschinen reichten weiter als die zu ihrer Zeit verfügbare Fertigungstechnik. Die Handwerker und Mechaniker waren vor der Mitte des 19. Jahrhunderts schlicht und einfach überfordert, zum Beispiel 50.000 Ziffernräder von gleicher Qualität zu produzieren und in perfekter Passung zusammenzusetzen. Dies verlangten aber die Pläne für die „Analytical Engine“, die Babbage um 1837 herum konzipierte und für die er schon einen eigenen Speicher vorsah – wie es selbstverständlich zu einem modernen Computer gehört –, eben in Form der vielen Räder, die wie die ganze Apparatur von einer Dampfmaschine angetrieben werden sollten.\nSo sehr die englische Regierung auch den Bau von Rechenautomaten förderte – nicht zuletzt in der Absicht, durch statistische Erhebungen und ihre Auswertungen so viel wie möglich über den Zustand der Gesellschaft etwa bei den Krankenzahlen und Verbrechenshäufigkeiten in Erfahrung zu bringen – und so sehr die Quantifizierbarkeit zur herrschenden Denkweise einer liberal und utilitaristisch eingestellten Gesellschaft wurde, so ließ doch „eine Industrialisierung der geistigen Arbeit, wie Babbage sie erstrebt hatte, ... auf sich warten“. So erklärt der Historiker Karl H. Metz und fährt fort: „Ohne eine Maschine, die Rechenleistung, Datenspeicherung und Programmsteuerung gleichermaßen vollzog, blieb ein Übergang zum Maschinensystem in der Verwaltungsarbeit unmöglich. Dieser Übergang kam erst mit der ‚tabulating machine’, für die Hermann Hollerith ein Patent erhielt und zu deren industrieller Produktion er 1896 eine Firma gründete, die 1924 zur „International Business Machines Corporation“ wurde, dem ersten und weltweit größten Hersteller von Anlagen zur Datenverarbeitung.“\nHolleriths Idee bestand darin, die Lochkarten, auf denen die statistischen Daten gesammelt und eingetragen wurden, nicht länger mechanisch, sondern elektrisch abtasten zu lassen. Er konstruierte zu diesem Zweck Abtaststifte, die auf den Karten nach Löchern suchten und nach dem jeweiligen Durchstechen entweder Stromkreise schlossen oder geöffnet ließen. Und er fügte der Gesamtkonstruktion Zählwerke und Sortierfächer hinzu, in die solche Karten kamen, die es später erneut auszuwerten galt – für eine andere Fragestellung nach einem anderen Programm. Die Hollerith-Maschine vermochte somit die gesammelten und in Form von Lochmustern gespeicherten Informationen nach verschiedenen Merkmalen zu kombinieren, und das heißt, „sie arbeitete bereits mit einem logischen Und-Operator“, und da die Stromkreise verschieden geschaltet werden konnten, „ergab sich eine Programmierbarkeit der Maschine, in der jede Schaltordnung einem bestimmten Programm entsprach“, so Metz.\nHollerith war der Durchbruch für sein Konzept bei der amerikanischen Volkszählung des Jahres 1890 gelungen, in der für die Erfassung der insgesamt 62'622'250 Einwohner der USA mehr als 60 Millionen Lochkarten mit mehr als einer Milliarde Lochungen ausgewertet wurden. Holleriths Maschine erlaubte es, eine ungewöhnliche Kombination von Befragungsmerkmalen auszuwerten und zu verknüpfen, und so revolutionierte sein Lochkartensystem „nicht nur die Weise der Volkszählung, die seit den Tagen Babylons unverändert geblieben war“, es „ermöglichte zugleich auch die Mechanisierung aller Vorgänge, die zahlenmäßig zu erfassen waren.“ \nMit der elektrischen Verarbeitung von auf  Lochkarten gespeicherten Informationen, unabhängig von einem weiteren Eingreifen von Personen, war der Weg geebnet, der zur Abspeicherung elektrischer Signale führt und diesen Vorgang durch ein Programm steuert. Wir reden vom Computer, und wer in einem Lexikon nachschlägt oder sich unter Fachleuten erkundigt, wann das erste Gerät, das diesen Namen im modernen Sinne verdient, zum Einsatz gekommen ist, dem wird mit dem Namen von Konrad Zuse und seinem Modell Z1 geantwortet, das 1937 vorgestellt wurde und gerne als „Chip aus Blech“ bezeichnet wird. Zuse wollte die vielen statischen Berechnungen, die er als Bauingenieur durchführen musste, einer Maschine übertragen, und so entwickelte er sein Gerät, das mit binären Zahlen operierte, das Programme von Lochstreifen ablas und überhaupt gut konzipiert war – nur dass sich die Schaltglieder dauernd verhakten, wenn sie in Betrieb waren, so wie es auch mit den Hebeln von mechanischen Schreibmaschinen passierte.\nNatürlich gibt ein Pionier wie Zuse wegen solcher Schwierigkeiten nicht auf, und er verbesserte seine Konstruktionen auch deshalb, weil es bald größere Aufgaben gab – zum Beispiel die aerodynamischen Berechnungen, die in Kriegszeiten nötig wurden, wenn man Bomber fernsteuern wollte. 1942 brachte Zuse den Rechner Z3 zustande, der das Pech hatte, im November 1943 bei einem Bombenangriff zerstört zu werden. Der Z3 operierte mit Relais (elektrischen Schaltungen); er beherrschte die ungefähre Darstellung von Zahlen (Gleitkomma), was den Umfang der Arbeiten einschränkte und zugleich den Umgang mit Rechenergebnissen erleichterte, er verfügte über ein Lesegerät für Lochstreifen, und er funktionierte als eine getaktete Maschine. Das heißt, es gab einen Elektromotor, der eine Trommel antrieb, die sich etwa fünfmal pro Sekunde drehte und dabei die Steuerung der Relais besorgte, die als Gruppen angeordnet waren, um in dieser Form im Takt wechselnd Aufgaben erledigen zu können – Addieren Subtrahieren, Multiplizieren, Dividieren, Wurzel ziehen, Speichern, Abrufen, Ausgeben und was für Aufgaben sonst noch benötigt wurden.\n„Während in Deutschland die Arbeiten Zuses wenig Beachtung fanden“, so der Historiker Metz, galt in den USA die Entwicklung schneller Rechner als wesentliche Aufgabe, für die „Wissenschaft, Industrie und Militär bzw. Staat eng zusammen wirkten, hier die Universitäten Harvard und Pennsylvania, IBM und die Marine. Dort konstruierte der Mathematik-Professor Howard H. Aitken 1944 mit seiner Arbeitsgruppe den ,Superrechner Mark I’. Mark I war im Vergleich zu Zuses Geräten riesig – nämlich sechzehn Meter lang und zweieinhalb Meter hoch. Mark I bestand aus 700’000 Einzelteilen, enthielt 800 km Draht und brauchte trotzdem sechs Sekunden für eine Multiplikation mit zehnstelligen Zahlen und das Doppelte für eine Division, was ihn deutlich langsamer als den Z3 machte.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Charles Babbages (Mathematiker) und Ada Lovelaces (Programmiererin) Gedanken und Konzeptionen zu programmierbaren mechanischen Rechenmaschinen reichten weiter als die zu ihrer Zeit verfügbare Fertigungstechnik. Die Handwerker und Mechaniker waren vor der Mitte des 19. Jahrhunderts schlicht und einfach überfordert, zum Beispiel 50.000 Ziffernräder von gleicher Qualität zu produzieren und in perfekter Passung zusammenzusetzen. Dies verlangten aber die Pläne für die „Analytical Engine“, die Babbage um 1837 herum konzipierte und für die er schon einen eigenen Speicher vorsah – wie es selbstverständlich zu einem modernen Computer gehört –, eben in Form der vielen Räder, die wie die ganze Apparatur von einer Dampfmaschine angetrieben werden sollten.\nSo sehr die englische Regierung auch den Bau von Rechenautomaten förderte – nicht zuletzt in der Absicht, durch statistische Erhebungen und ihre Auswertungen so viel wie möglich über den Zustand der Gesellschaft etwa bei den Krankenzahlen und Verbrechenshäufigkeiten in Erfahrung zu bringen – und so sehr die Quantifizierbarkeit zur herrschenden Denkweise einer liberal und utilitaristisch eingestellten Gesellschaft wurde, so ließ doch „eine Industrialisierung der geistigen Arbeit, wie Babbage sie erstrebt hatte, ... auf sich warten“. So erklärt der Historiker Karl H. Metz und fährt fort: „Ohne eine Maschine, die Rechenleistung, Datenspeicherung und Programmsteuerung gleichermaßen vollzog, blieb ein Übergang zum Maschinensystem in der Verwaltungsarbeit unmöglich. Dieser Übergang kam erst mit der ‚tabulating machine’, für die Hermann Hollerith ein Patent erhielt und zu deren industrieller Produktion er 1896 eine Firma gründete, die 1924 zur „International Business Machines Corporation“ wurde, dem ersten und weltweit größten Hersteller von Anlagen zur Datenverarbeitung.“\nHolleriths Idee bestand darin, die Lochkarten, auf denen die statistischen Daten gesammelt und eingetragen wurden, nicht länger mechanisch, sondern elektrisch abtasten zu lassen. Er konstruierte zu diesem Zweck Abtaststifte, die auf den Karten nach Löchern suchten und nach dem jeweiligen Durchstechen entweder Stromkreise schlossen oder geöffnet ließen. Und er fügte der Gesamtkonstruktion Zählwerke und Sortierfächer hinzu, in die solche Karten kamen, die es später erneut auszuwerten galt – für eine andere Fragestellung nach einem anderen Programm. Die Hollerith-Maschine vermochte somit die gesammelten und in Form von Lochmustern gespeicherten Informationen nach verschiedenen Merkmalen zu kombinieren, und das heißt, „sie arbeitete bereits mit einem logischen Und-Operator“, und da die Stromkreise verschieden geschaltet werden konnten, „ergab sich eine Programmierbarkeit der Maschine, in der jede Schaltordnung einem bestimmten Programm entsprach“, so Metz.\nHollerith war der Durchbruch für sein Konzept bei der amerikanischen Volkszählung des Jahres 1890 gelungen, in der für die Erfassung der insgesamt 62'622'250 Einwohner der USA mehr als 60 Millionen Lochkarten mit mehr als einer Milliarde Lochungen ausgewertet wurden. Holleriths Maschine erlaubte es, eine ungewöhnliche Kombination von Befragungsmerkmalen auszuwerten und zu verknüpfen, und so revolutionierte sein Lochkartensystem „nicht nur die Weise der Volkszählung, die seit den Tagen Babylons unverändert geblieben war“, es „ermöglichte zugleich auch die Mechanisierung aller Vorgänge, die zahlenmäßig zu erfassen waren.“ \nMit der elektrischen Verarbeitung von auf  Lochkarten gespeicherten Informationen, unabhängig von einem weiteren Eingreifen von Personen, war der Weg geebnet, der zur Abspeicherung elektrischer Signale führt und diesen Vorgang durch ein Programm steuert. Wir reden vom Computer, und wer in einem Lexikon nachschlägt oder sich unter Fachleuten erkundigt, wann das erste Gerät, das diesen Namen im modernen Sinne verdient, zum Einsatz gekommen ist, dem wird mit dem Namen von Konrad Zuse und seinem Modell Z1 geantwortet, das 1937 vorgestellt wurde und gerne als „Chip aus Blech“ bezeichnet wird. Zuse wollte die vielen statischen Berechnungen, die er als Bauingenieur durchführen musste, einer Maschine übertragen, und so entwickelte er sein Gerät, das mit binären Zahlen operierte, das Programme von Lochstreifen ablas und überhaupt gut konzipiert war – nur dass sich die Schaltglieder dauernd verhakten, wenn sie in Betrieb waren, so wie es auch mit den Hebeln von mechanischen Schreibmaschinen passierte.\nNatürlich gibt ein Pionier wie Zuse wegen solcher Schwierigkeiten nicht auf, und er verbesserte seine Konstruktionen auch deshalb, weil es bald größere Aufgaben gab – zum Beispiel die aerodynamischen Berechnungen, die in Kriegszeiten nötig wurden, wenn man Bomber fernsteuern wollte. 1942 brachte Zuse den Rechner Z3 zustande, der das Pech hatte, im November 1943 bei einem Bombenangriff zerstört zu werden. Der Z3 operierte mit Relais (elektrischen Schaltungen); er beherrschte die ungefähre Darstellung von Zahlen (Gleitkomma), was den Umfang der Arbeiten einschränkte und zugleich den Umgang mit Rechenergebnissen erleichterte, er verfügte über ein Lesegerät für Lochstreifen, und er funktionierte als eine getaktete Maschine. Das heißt, es gab einen Elektromotor, der eine Trommel antrieb, die sich etwa fünfmal pro Sekunde drehte und dabei die Steuerung der Relais besorgte, die als Gruppen angeordnet waren, um in dieser Form im Takt wechselnd Aufgaben erledigen zu können – Addieren Subtrahieren, Multiplizieren, Dividieren, Wurzel ziehen, Speichern, Abrufen, Ausgeben und was für Aufgaben sonst noch benötigt wurden.\n„Während in Deutschland die Arbeiten Zuses wenig Beachtung fanden“, so der Historiker Metz, galt in den USA die Entwicklung schneller Rechner als wesentliche Aufgabe, für die „Wissenschaft, Industrie und Militär bzw. Staat eng zusammen wirkten, hier die Universitäten Harvard und Pennsylvania, IBM und die Marine. Dort konstruierte der Mathematik-Professor Howard H. Aitken 1944 mit seiner Arbeitsgruppe den ,Superrechner Mark I’. Mark I war im Vergleich zu Zuses Geräten riesig – nämlich sechzehn Meter lang und zweieinhalb Meter hoch. Mark I bestand aus 700’000 Einzelteilen, enthielt 800 km Draht und brauchte trotzdem sechs Sekunden für eine Multiplikation mit zehnstelligen Zahlen und das Doppelte für eine Division, was ihn deutlich langsamer als den Z3 machte.",
      "id": [
        "145810"
      ],
      "item": "Auf dem Weg zum Computer"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145811]\n[%Und-Operator]\nDer Und-Operator ist eine mathematische Anweisung, die besagt, dass die gegebenen Daten (Operanden) zusammengefügt werden sollen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der Und-Operator ist eine mathematische Anweisung, die besagt, dass die gegebenen Daten (Operanden) zusammengefügt werden sollen.",
      "id": [
        "145811"
      ],
      "item": "Und-Operator"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145812]\n[%Die Röhren und das Universelle]\nMit dem Mark I (ein unter der Leitung von dem Mathematik-Professor Howard H. Aitken 1944 entwickelter riesiger Rechner) nähern wir uns dem ENIAC, der von 1943 an gebaut wurde und 1945 zum Einsatz kam. Damals bemühten sich die Ingenieure vor allem darum, den Arbeitstakt der Computer zu erhöhen. Ein Mitarbeiter von Konrad Zuse (mit Zuse kam gewissermassen der erste Computer, der auch gerne als „Chip aus Blech“ bezeichnet wird, 1937 zum Einsatz), der später nach Brasilien emigrierte Helmut Schreyer, träumte dabei von Millisekunden. Schreyer war auch der Fachmann, der den Einsatz von Elektronenröhren in den Rechenanlagen vorantrieb und noch 1943 ein – im Krieg verloren gegangenes – Gerät mit 15’00 Röhren konzipierte. Die amerikanischen Ingenieure des ENIAC steigerten die Zahl der Röhren dann um mehr als den Faktor Zehn auf 18’000.\nRöhren bestehen aus einem möglichst luftleeren (evakuierten) Glaskolben, in dem sich mehrere Elektroden befinden, zwischen den Strom fließen kann. Die Physiker kennen diese Vorrichtung seit dem 19. Jahrhundert und unterscheiden in ihr die Kathode, aus der Elektronen austreten, und die Anode, von der sie aufgenommen werden. Zu Beginn des 20. Jahrhundert fiel einigen Wissenschaftler auf, dass man den Stromfluss steuern konnte, wenn man Gitter zwischen die negative und die positive Elektrode – zwischen Kathode und Anode – anbrachte. „Steuern“ bedeutet, dass sich Stromsignale ausrichten, verstärken oder auf andere Weise modulieren ließen. Damit standen den Ingenieuren elektronische Schalt- und Bauelemente zur Verfügung, und mit ihnen wuchs der Mut, größere Rechenanlagen zu bauen – eben den ENIAC und den bereits erwähnten SSEC. Diese Anlagen wären immer größer, immer aufwendiger und immer teurer geworden, ohne uns auch nur im Ansatz leisten zu können, was moderne Tischcomputer leisten, wenn es nicht zur Erfindung des Transistors gekommen wäre. Sie erfolgte offenbar genau zum richtigen Zeitpunkt, nämlich als sich die ersten Rechner in der Lage zeigten, relevante Aufgaben zu bewältigen, bei denen es um Zahlen ging. Sie reichten von der Berechnung der Reichweite von Flugzeugen über die Kombination von Linsen in Mikroskopen und Fernrohren bis zu Wettervorhersagen.\nDarüber hinaus schien jetzt die Konstruktion einer intelligenten Maschine in greifbare Nähe zu rücken, die alle lösbaren Probleme auch lösen könnte – die Verwirklichung der Turing-Maschine, eines Denkmodells, das der englische Mathematiker Alan Turing entworfen hatte.\nTuring hatte über ein allgemeines Problem der Mathematik nachgedacht, das als Entscheidungsproblem bekannt ist und in dem es um die Frage geht, ob es in der Mathematik unlösbare Probleme gibt oder nicht. Der knapp 25jährige Turing sorgte 1937 für eine Sensation, als er zeigen konnte, dass es sogar unendlich viele Probleme gibt, die grundsätzlich unlösbar sind. Es gibt – so wies Turing nach – im Rahmen einer gegebenen Anzahl von Axiomen keine Möglichkeit, alle mathematischen Probleme zu lösen. Aber er federte den Schrecken, den sein Beweis auslöste, durch die Beobachtung ab, dass es eine universelle Rechenmaschine geben kann – die heute sogenannte Turing-Maschine –, die in der Lage ist, jede andere Maschine zu ersetzen und jedes berechenbare Problem zu lösen. Mit diesen mathematisch-logischen Entdeckungen bereitete Turing den Grund für das Konzept des elektronischen Gehirns bzw. der denkenden Maschine vor, das er nach dem Krieg verwirklichen wollte.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Mit dem Mark I (ein unter der Leitung von dem Mathematik-Professor Howard H. Aitken 1944 entwickelter riesiger Rechner) nähern wir uns dem ENIAC, der von 1943 an gebaut wurde und 1945 zum Einsatz kam. Damals bemühten sich die Ingenieure vor allem darum, den Arbeitstakt der Computer zu erhöhen. Ein Mitarbeiter von Konrad Zuse (mit Zuse kam gewissermassen der erste Computer, der auch gerne als „Chip aus Blech“ bezeichnet wird, 1937 zum Einsatz), der später nach Brasilien emigrierte Helmut Schreyer, träumte dabei von Millisekunden. Schreyer war auch der Fachmann, der den Einsatz von Elektronenröhren in den Rechenanlagen vorantrieb und noch 1943 ein – im Krieg verloren gegangenes – Gerät mit 15’00 Röhren konzipierte. Die amerikanischen Ingenieure des ENIAC steigerten die Zahl der Röhren dann um mehr als den Faktor Zehn auf 18’000.\nRöhren bestehen aus einem möglichst luftleeren (evakuierten) Glaskolben, in dem sich mehrere Elektroden befinden, zwischen den Strom fließen kann. Die Physiker kennen diese Vorrichtung seit dem 19. Jahrhundert und unterscheiden in ihr die Kathode, aus der Elektronen austreten, und die Anode, von der sie aufgenommen werden. Zu Beginn des 20. Jahrhundert fiel einigen Wissenschaftler auf, dass man den Stromfluss steuern konnte, wenn man Gitter zwischen die negative und die positive Elektrode – zwischen Kathode und Anode – anbrachte. „Steuern“ bedeutet, dass sich Stromsignale ausrichten, verstärken oder auf andere Weise modulieren ließen. Damit standen den Ingenieuren elektronische Schalt- und Bauelemente zur Verfügung, und mit ihnen wuchs der Mut, größere Rechenanlagen zu bauen – eben den ENIAC und den bereits erwähnten SSEC. Diese Anlagen wären immer größer, immer aufwendiger und immer teurer geworden, ohne uns auch nur im Ansatz leisten zu können, was moderne Tischcomputer leisten, wenn es nicht zur Erfindung des Transistors gekommen wäre. Sie erfolgte offenbar genau zum richtigen Zeitpunkt, nämlich als sich die ersten Rechner in der Lage zeigten, relevante Aufgaben zu bewältigen, bei denen es um Zahlen ging. Sie reichten von der Berechnung der Reichweite von Flugzeugen über die Kombination von Linsen in Mikroskopen und Fernrohren bis zu Wettervorhersagen.\nDarüber hinaus schien jetzt die Konstruktion einer intelligenten Maschine in greifbare Nähe zu rücken, die alle lösbaren Probleme auch lösen könnte – die Verwirklichung der Turing-Maschine, eines Denkmodells, das der englische Mathematiker Alan Turing entworfen hatte.\nTuring hatte über ein allgemeines Problem der Mathematik nachgedacht, das als Entscheidungsproblem bekannt ist und in dem es um die Frage geht, ob es in der Mathematik unlösbare Probleme gibt oder nicht. Der knapp 25jährige Turing sorgte 1937 für eine Sensation, als er zeigen konnte, dass es sogar unendlich viele Probleme gibt, die grundsätzlich unlösbar sind. Es gibt – so wies Turing nach – im Rahmen einer gegebenen Anzahl von Axiomen keine Möglichkeit, alle mathematischen Probleme zu lösen. Aber er federte den Schrecken, den sein Beweis auslöste, durch die Beobachtung ab, dass es eine universelle Rechenmaschine geben kann – die heute sogenannte Turing-Maschine –, die in der Lage ist, jede andere Maschine zu ersetzen und jedes berechenbare Problem zu lösen. Mit diesen mathematisch-logischen Entdeckungen bereitete Turing den Grund für das Konzept des elektronischen Gehirns bzw. der denkenden Maschine vor, das er nach dem Krieg verwirklichen wollte.",
      "id": [
        "145812"
      ],
      "item": "Die Röhren und das Universelle"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145813]\n[%Alan Turing]\nDer brillante Mathematiker Alan Turing (1912-1954) war während des Krieges maßgeblich dran beteiligt, den Enigma-Code der deutschen Wehrmacht zu knacken. 1952 wurde Turing wegen praktizierter Homosexualität verurteilt und dadurch schließlich in den Tod getrieben.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der brillante Mathematiker Alan Turing (1912-1954) war während des Krieges maßgeblich dran beteiligt, den Enigma-Code der deutschen Wehrmacht zu knacken. 1952 wurde Turing wegen praktizierter Homosexualität verurteilt und dadurch schließlich in den Tod getrieben.",
      "id": [
        "145813"
      ],
      "item": "Alan Turing"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145814]\n[%Rechnen mit Transistoren]\nDie Nachteile von (Elektronen-)Röhren lassen sich rasch aufzählen: Sie sind leicht zerbrechlich, sie benötigen eine Anwärmzeit, sie verbrauchen viel Strom, sie erzeugen viel Wärme, sie erlauben keine integrierten Schaltkreise, sie verbrauchen viel Platz, sie wiegen zu viel, sie kosten zu viel, und so könnte man fortfahren, um zu verstehen, dass die Erfindung des Transistors die entscheidende Wende zu der Miniaturisierung und der Schnelligkeit der Computer war, von der wir heute profitieren.\nNatürlich haben die Erfinder des Transistors Bardeen, Brattain und Shockley nicht im luftleeren Raum mit den Halbleitern gespielt. Über die Frage, wie man die aufwendige und wenig zuverlässige Röhrentechnik verbessern könnte, wurde vielfach nachgedacht. So gab es in Deutschland Vorarbeiten in den späten 1930er Jahren, bei denen zum Beispiel Robert W. Pohl mit Hilfe von Halbleitern einen Verstärkereffekt erzielte. 1947 gelang dann – wie oben erwähnt – die erste technische Realisierung eines Transistors, und nachdem man gelernt hatte, die industrielle Produktion von Transistoren ohne allzu viel Ausschuss in Gang zu setzen, konnte man sich einem weiteren Schritt annehmen, nämlich dem Bau von Computern, die ausschließlich mit den kristallinen Bauelementen ausgestattet waren. Dabei verringerten sich mit dem schwindenden Raumbedarf auch die Stromwege, was zu erhöhten Rechengeschwindigkeiten beitrug. Diese Geschwindigkeit war zu Beginn der 1960er Jahre von etwas mehr als 1’000 auf mehr als 100’000 Additionen pro Sekunde gestiegen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Die Nachteile von (Elektronen-)Röhren lassen sich rasch aufzählen: Sie sind leicht zerbrechlich, sie benötigen eine Anwärmzeit, sie verbrauchen viel Strom, sie erzeugen viel Wärme, sie erlauben keine integrierten Schaltkreise, sie verbrauchen viel Platz, sie wiegen zu viel, sie kosten zu viel, und so könnte man fortfahren, um zu verstehen, dass die Erfindung des Transistors die entscheidende Wende zu der Miniaturisierung und der Schnelligkeit der Computer war, von der wir heute profitieren.\nNatürlich haben die Erfinder des Transistors Bardeen, Brattain und Shockley nicht im luftleeren Raum mit den Halbleitern gespielt. Über die Frage, wie man die aufwendige und wenig zuverlässige Röhrentechnik verbessern könnte, wurde vielfach nachgedacht. So gab es in Deutschland Vorarbeiten in den späten 1930er Jahren, bei denen zum Beispiel Robert W. Pohl mit Hilfe von Halbleitern einen Verstärkereffekt erzielte. 1947 gelang dann – wie oben erwähnt – die erste technische Realisierung eines Transistors, und nachdem man gelernt hatte, die industrielle Produktion von Transistoren ohne allzu viel Ausschuss in Gang zu setzen, konnte man sich einem weiteren Schritt annehmen, nämlich dem Bau von Computern, die ausschließlich mit den kristallinen Bauelementen ausgestattet waren. Dabei verringerten sich mit dem schwindenden Raumbedarf auch die Stromwege, was zu erhöhten Rechengeschwindigkeiten beitrug. Diese Geschwindigkeit war zu Beginn der 1960er Jahre von etwas mehr als 1’000 auf mehr als 100’000 Additionen pro Sekunde gestiegen.",
      "id": [
        "145814"
      ],
      "item": "Rechnen mit Transistoren"
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145815]\n[##Energie und Information; Maxwells Dämon]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 2,
      "progress": true,
      "toc": true,
      "id": [
        "145815"
      ],
      "title": "Energie und Information; Maxwells Dämon"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145816]\n[%Die drei Hauptsätze der Thermodynamik]\n•. Energie kann nicht erzeugt oder vernichtet werden; sie lässt sich nur umwandeln.\n•. Die Entropie kann nicht abnehmen.\n•. Der absolute Nullpunkt kann nicht erreicht werden.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "•. Energie kann nicht erzeugt oder vernichtet werden; sie lässt sich nur umwandeln.\n•. Die Entropie kann nicht abnehmen.\n•. Der absolute Nullpunkt kann nicht erreicht werden.",
      "id": [
        "145816"
      ],
      "item": "Die drei Hauptsätze der Thermodynamik"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145817]\n[%Einleitung]\nAnfang der 1960er Jahre arbeitete der aus Stuttgart stammende und 1938 aus Deutschland vertriebene Physiker und Informationswissenschaftler Rolf Landauer bei IBM, und ihn beschäftigte neben vielen aktuellen technischen Problemen auch eine alte grundsätzliche Fragestellung, die aus dem Jahre 1871 stammte und die Physiker schon länger nervte. Sie ist seit 1874 unter der Bezeichnung „Maxwells Dämon“ bekannt. Ausgedacht hat sich den Dämon der schottische Physiker James Clerk Maxwell, der einen ungeheuren Einfluss auf die Physik des 20. Jahrhunderts ausgeübt hat. Es könnte sein, dass die Geschichtswissenschaftler in tausend Jahren auf die Frage, was es im 19. Jahrhundert Bedeutungsvolles gegeben hat, mit dem Hinweis auf Maxwells Gleichungen antworten. Denn diese Gleichungen haben die praktische Verwendung von Radiowellen – und damit die ganze moderne Telekommunikation – ermöglicht und philosophisch das neue Denken über Raum und Zeit vorbereitet, das wir Albert Einstein verdanken, der ohne seine Bewunderung von Maxwell nicht zu verstehen ist.\nWir wollen uns hier aber nicht mit diesen Gleichungen befassen, die unter anderem zum ersten Mal erklären konnten, was Licht ist und wie es sich im Universum ausbreiten kann. Wir haben auch nicht mit Maxwells Idee eines Fliehkraftreglers zu tun, mit der er 1868 die Maschine ausrüsten wollte, die ein Schiff steuert, obwohl hier zum ersten Mal das Konzept der Rückkopplung auftaucht, das Norbert Wiener nach 1948 übernahm, um Rückkopplungen in seine Rechenmaschinen einzubauen und die Kybernetik zu begründen.\nEs geht vielmehr um Maxwell als Auslöser der „probabilistischen Revolution in der Physik“, wie es die Historiker genannt haben, die beschrieben haben, wie die Wissenschaft „das Reich des Zufalls“ erkundet und erobert hat. Und sie meinen damit das Folgende: „Maxwell erkannte die Bedeutung der Tatsache, dass sich die Moleküle in einem Gas, da sie ständig zusammenstoßen, keinesfalls mit derselben und schon gar nicht mit konstanter Geschwindigkeit bewegen können. Nur ein statistischer Ansatz kann das Problem der unzähligen Stöße lösen. Da die Moleküle nicht beobachtbar sind, kann diese Statistik nur auf der Basis probabilistischer Annahmen durchgeführt werden. Maxwell hat als erster solche Annahmen in die Beschreibung objektiver physikalischer Systeme (im Unterschied zur Auswertung von Hypothesen oder dergleichen) aufgenommen.“\nMaxwell nahm an, dass die drei Komponenten der Geschwindigkeit eines Moleküls, die durch die drei Richtungen bedingt sind, die ihm im Raum zur Verfügung stehen – rechts und links, vorne und hinten, oben und unten –, unabhängig voneinander sind, und er leitete damit ein Gesetz für die Verteilung der Geschwindigkeit von Molekülen ab. Seine Wahrscheinlichkeitstheorie stellt die Geburt der statistischen Physik dar, deren Qualität er durch einen Dämon testen wollte, den wir jetzt endlich einführen können und der – wie erwähnt – die Welt der Wissenschaft nicht nur bis Landauer, sondern noch Jahrzehnte darüber hinaus beschäftigt hat.\nMaxwells Dämon stellt einen Versuch dar, das, was die Physiker den Zweiten Hauptsatz der Thermodynamik oder Entropie nennen, auf Herz und Nieren zu prüfen (und dabei vielleicht sogar über den Haufen zu werfen). Sein Erfinder hat das Teufelchen dabei als knifflige Aufgabe für die Wissenschaft in die Welt gesetzt. Sie sollte ihm mit ihren Gesetzen erklären, warum es solch einen Dämon nicht geben kann. Es hat über einhundert Jahre gedauert, bis die Physiker darauf die passende Antwort gefunden haben, woraus man schließen kann, wie clever der Dämon konzipiert war; diese Antwort aber hat mit den Informationen zu tun, um die es uns geht.\nBevor wir zu dieser überraschenden Lösung kommen, müssen wir den Gipfel der Wissenschaft ersteigen, den wir als Zweiten Hauptsatz schon benannt haben. Wer diese Bezeichnung hört, wird sich erinnern, dass es daneben einen ersten solchen Hauptsatz gibt, und er wird fragen, ob es darüber hinaus noch einen dritten oder gar vierten Hauptsatz in dieser Reihe gibt. Die korrekte Antwort lautet, es gibt drei solcher Grundaussagen der physikalischen Wissenschaft, die sich mit der Wärme beschäftigt. Im Ersten Hauptsatz wird festgestellt, dass in allen Reaktionen und bei allen Bewegungen die Energie weder vernichtet noch erzeugt wird, sondern vielmehr konstant bleibt. Man spricht daher auch vom Erhaltungssatz der Energie. Im 19. Jahrhundert fühlten sich die Physiker so stark, dass sie es riskierten, daraus eine universelle Aussage zu formulieren: Die Energie der Welt ist konstant.\nDa es hier um Maxwells Dämon und den Zweiten Hauptsatz geht, sei nur noch rasch erwähnt, dass die dritte Grundlegung dieser Art eine Auskunft über den absoluten Nullpunkt der Temperatur gibt. Um dies zu verstehen, muss man nur daran denken, dass Gegenstände schrumpfen, wenn es kälter wird. Sie können aber nicht beliebig klein werden. Es muss deshalb eine Grenze geben, die es ausgeschlossen macht, dass die Temperatur noch weiter absinken kann. Hier ist wortwörtlich alles festgefroren und bewegungslos. Man spricht vom absoluten Nullpunkt der Temperatur, um ihn von relativen Nullpunkten zu unterscheiden, wie wir sie etwa von der Celsius-Skala auf unseren Thermometern kennen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Anfang der 1960er Jahre arbeitete der aus Stuttgart stammende und 1938 aus Deutschland vertriebene Physiker und Informationswissenschaftler Rolf Landauer bei IBM, und ihn beschäftigte neben vielen aktuellen technischen Problemen auch eine alte grundsätzliche Fragestellung, die aus dem Jahre 1871 stammte und die Physiker schon länger nervte. Sie ist seit 1874 unter der Bezeichnung „Maxwells Dämon“ bekannt. Ausgedacht hat sich den Dämon der schottische Physiker James Clerk Maxwell, der einen ungeheuren Einfluss auf die Physik des 20. Jahrhunderts ausgeübt hat. Es könnte sein, dass die Geschichtswissenschaftler in tausend Jahren auf die Frage, was es im 19. Jahrhundert Bedeutungsvolles gegeben hat, mit dem Hinweis auf Maxwells Gleichungen antworten. Denn diese Gleichungen haben die praktische Verwendung von Radiowellen – und damit die ganze moderne Telekommunikation – ermöglicht und philosophisch das neue Denken über Raum und Zeit vorbereitet, das wir Albert Einstein verdanken, der ohne seine Bewunderung von Maxwell nicht zu verstehen ist.\nWir wollen uns hier aber nicht mit diesen Gleichungen befassen, die unter anderem zum ersten Mal erklären konnten, was Licht ist und wie es sich im Universum ausbreiten kann. Wir haben auch nicht mit Maxwells Idee eines Fliehkraftreglers zu tun, mit der er 1868 die Maschine ausrüsten wollte, die ein Schiff steuert, obwohl hier zum ersten Mal das Konzept der Rückkopplung auftaucht, das Norbert Wiener nach 1948 übernahm, um Rückkopplungen in seine Rechenmaschinen einzubauen und die Kybernetik zu begründen.\nEs geht vielmehr um Maxwell als Auslöser der „probabilistischen Revolution in der Physik“, wie es die Historiker genannt haben, die beschrieben haben, wie die Wissenschaft „das Reich des Zufalls“ erkundet und erobert hat. Und sie meinen damit das Folgende: „Maxwell erkannte die Bedeutung der Tatsache, dass sich die Moleküle in einem Gas, da sie ständig zusammenstoßen, keinesfalls mit derselben und schon gar nicht mit konstanter Geschwindigkeit bewegen können. Nur ein statistischer Ansatz kann das Problem der unzähligen Stöße lösen. Da die Moleküle nicht beobachtbar sind, kann diese Statistik nur auf der Basis probabilistischer Annahmen durchgeführt werden. Maxwell hat als erster solche Annahmen in die Beschreibung objektiver physikalischer Systeme (im Unterschied zur Auswertung von Hypothesen oder dergleichen) aufgenommen.“\nMaxwell nahm an, dass die drei Komponenten der Geschwindigkeit eines Moleküls, die durch die drei Richtungen bedingt sind, die ihm im Raum zur Verfügung stehen – rechts und links, vorne und hinten, oben und unten –, unabhängig voneinander sind, und er leitete damit ein Gesetz für die Verteilung der Geschwindigkeit von Molekülen ab. Seine Wahrscheinlichkeitstheorie stellt die Geburt der statistischen Physik dar, deren Qualität er durch einen Dämon testen wollte, den wir jetzt endlich einführen können und der – wie erwähnt – die Welt der Wissenschaft nicht nur bis Landauer, sondern noch Jahrzehnte darüber hinaus beschäftigt hat.\nMaxwells Dämon stellt einen Versuch dar, das, was die Physiker den Zweiten Hauptsatz der Thermodynamik oder Entropie nennen, auf Herz und Nieren zu prüfen (und dabei vielleicht sogar über den Haufen zu werfen). Sein Erfinder hat das Teufelchen dabei als knifflige Aufgabe für die Wissenschaft in die Welt gesetzt. Sie sollte ihm mit ihren Gesetzen erklären, warum es solch einen Dämon nicht geben kann. Es hat über einhundert Jahre gedauert, bis die Physiker darauf die passende Antwort gefunden haben, woraus man schließen kann, wie clever der Dämon konzipiert war; diese Antwort aber hat mit den Informationen zu tun, um die es uns geht.\nBevor wir zu dieser überraschenden Lösung kommen, müssen wir den Gipfel der Wissenschaft ersteigen, den wir als Zweiten Hauptsatz schon benannt haben. Wer diese Bezeichnung hört, wird sich erinnern, dass es daneben einen ersten solchen Hauptsatz gibt, und er wird fragen, ob es darüber hinaus noch einen dritten oder gar vierten Hauptsatz in dieser Reihe gibt. Die korrekte Antwort lautet, es gibt drei solcher Grundaussagen der physikalischen Wissenschaft, die sich mit der Wärme beschäftigt. Im Ersten Hauptsatz wird festgestellt, dass in allen Reaktionen und bei allen Bewegungen die Energie weder vernichtet noch erzeugt wird, sondern vielmehr konstant bleibt. Man spricht daher auch vom Erhaltungssatz der Energie. Im 19. Jahrhundert fühlten sich die Physiker so stark, dass sie es riskierten, daraus eine universelle Aussage zu formulieren: Die Energie der Welt ist konstant.\nDa es hier um Maxwells Dämon und den Zweiten Hauptsatz geht, sei nur noch rasch erwähnt, dass die dritte Grundlegung dieser Art eine Auskunft über den absoluten Nullpunkt der Temperatur gibt. Um dies zu verstehen, muss man nur daran denken, dass Gegenstände schrumpfen, wenn es kälter wird. Sie können aber nicht beliebig klein werden. Es muss deshalb eine Grenze geben, die es ausgeschlossen macht, dass die Temperatur noch weiter absinken kann. Hier ist wortwörtlich alles festgefroren und bewegungslos. Man spricht vom absoluten Nullpunkt der Temperatur, um ihn von relativen Nullpunkten zu unterscheiden, wie wir sie etwa von der Celsius-Skala auf unseren Thermometern kennen.",
      "id": [
        "145817"
      ],
      "item": "Einleitung"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145818]\n[%James Clerk Maxwell]\nJames Clerk Maxwell (1831-1879) begründete nicht nur die Wellentheorie der elektromagnetischen Strahlung, sondern fertigte auch die erste Farbfotografie an. Er war Physiker, Mathematiker und Techniker zugleich.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "James Clerk Maxwell (1831-1879) begründete nicht nur die Wellentheorie der elektromagnetischen Strahlung, sondern fertigte auch die erste Farbfotografie an. Er war Physiker, Mathematiker und Techniker zugleich.",
      "id": [
        "145818"
      ],
      "item": "James Clerk Maxwell"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145819]\n[%Die Richtung der Natur]\nDas große Interesse der Physik und anderer Wissenschaften an der Energie hing im 19. Jahrhundert vor allem mit der Notwendigkeit zusammen, besser zu verstehen, wie Maschinen funktionierten. Es war die Zeit der großen Industrialisierung, und überall wurden Dampf- und Elektromaschinen installiert, um Arbeiten zu verrichten. Die Unternehmen – und nicht nur sie – wollten wissen, wie man mit möglichst geringem Aufwand möglichst viel aus einer Maschine herausholen kann, was physikalisch eine Antwort auf die Frage verlangte, wie viel von der Energie, die man etwa in Form von Kohle oder Strom in eine Maschine einbrachte, in Arbeit umgesetzt wurde.\nKlar war, dass nicht alle Energie nutzbar gemacht werden konnte und viel verloren ging – etwa durch Reibung oder dadurch, dass heiße Teile einer Maschine einfach abkühlten (Dissipation). Um hier genauer Auskunft geben zu können, unterschieden die Physiker zwischen der Gesamtenergie, die sie einem Apparat zuführten, und der freien Energie, die sie in Arbeit umwandeln konnten, wie sie sich etwa im Transport von Lasten zeigte. Bei ihren Versuchen, genauer zu erfassen, was diese freie Energie sein könnte, fiel den Physikern ganz allgemein auf, dass sie dann, wenn sie nur über Energie nachdachten, einen wesentlichen Aspekt sowohl der Naturvorgänge als auch der Abläufe in Maschinen außer Acht ließen und nicht in der Griff bekamen – nämlich die Richtung, in der Prozesse ablaufen. Mit der Richtung ist nicht gemeint, ob eine Kugel nach oben oder unten fliegt oder ob ein Ball umkehrt, nachdem ihn jemand gegen eine Mauer geschossen hat. Mit Richtung ist gemeint, dass zum Beispiel dann, wenn man einen Eiswürfel in ein Wasserglas gibt, die Wärme stets von der warmen Umgebung zum kalten Gefrorenen strömt und niemals die Gegenrichtung einschlägt. Wer ein kühles Glas Wein in einer lauen Sommernacht trinkt, wird merken, dass dessen Temperatur nur ansteigt. Es scheint ausgeschlossen zu sein, dass die warme Luft sich zusätzlich aus der Energie der Flüssigkeit bedient. Das Weinglas wird wärmer, bis es das Niveau der Abendluft erreicht hat. Dann kommt der Vorgang des Energietransports an sein Ende. \nMax Planck stellte in seinen Vorlesungen zur Thermodynamik klar: Ob „Wärmeleitung in die Richtung vom wärmeren zum kälteren Körper erfolgt oder umgekehrt, daraus lässt sich aus dem Energieprinzip allein nicht das mindeste schließen.“ Mit „Energieprinzip“ meinte Planck den Ersten Hauptsatz der Thermodynamik, dem er in seinen berühmten „Vorlesungen über Thermodynamik“ bald den Zweiten an die Seite stellte, den er so formulierte:„In der Natur existiert für jedes Körpersystem eine Größe, welche die Eigenschaft besitzt, bei allen Veränderungen, die das System allein betreffen, entweder konstant zu bleiben oder an Wert zuzunehmen.“ Für diese Größe hatte der Physiker Rudolf Clausius den Namen Entropie (Wirksamkeit und Kraft) eingeführt. Entropie stammt vom griechischen Wort entrepein ab, das „umkehren“ bedeutet. Offenbar gibt es in der Natur Vorgänge, die umkehrbar sind – Wasser kann erst zu Eis gefrieren und dann wieder schmelzen, wenn die Temperatur steigt –, aber die meisten Abläufe der Natur sind unumkehrbar. Und darüber entscheidet nicht die zu- oder abgeführte Energie, sondern die Entropie. Sie steigt an, wenn ein Vorgang nicht umkehrbar ist, und sie kann nur ansteigen und niemals abnehmen. Mit anderen Worten, die Vorgänge der Natur laufen in ihrer überwiegenden Art so ab, dass die Entropie stets zunimmt, was Clausius in selbstbewusster Manier in einer universalen Formulierung zusammenfasste: „Die Entropie der Welt strebt einem Maximum zu“.\nEs ist heute schwer vorstellbar, welches Interesse die Physik des 19. Jahrhunderts in einigen intellektuellen Kreisen fand, vor allem, nachdem sie es riskiert hatte, universale Behauptungen aufzustellen. Bald gab es einfache Deutungen der Entropie, die mit dem anschaulichen Konzept der Ordnung agierten und besagten, dass dem Zweiten Hauptsatz der Thermodynamik zufolge die Unordnung der Welt nur zunehmen könnte. Diesen Sachverhalt kennen wir alle aus dem Alltag. Er besagt, dass die Unordnung eines Zimmers nur wachsen kann, wenn niemand aufräumt. In der Natur gibt es bekanntlich keine Putzhilfe, also läuft es dort ab wie in einer Junggesellenbude, die zuletzt wie ein Saustall aussieht.\nTatsächlich sahen einige Intellektuelle im Zweiten Hauptsatz daher bald einen physikalischen Beweis für den notwendigen Untergang der Kultur. Die Urheber des Zweiten Hauptsatzes aber hatten zunächst ganz andere Probleme, nämlich zu verstehen, was die Entropie tatsächlich erfasst und wie die Naturabläufe mit ihrer Hilfe die Richtung bekommen, die sie haben. Es ist doch keine Frage, dass es gerichtet in der Natur zugeht und etwa ein Tintentropfen in einem Wasserglas nur zerfließt und sich nie wieder zusammenzieht. Und wenn man ein Gefäß, in dem sich ein Gas mit hoher Temperatur befindet, neben ein Gefäß stellt, in dem dasselbe Gas eine niedrigere Temperatur hat, dann wechselt die Energie nur von der warmen auf die kalte Seite – und nicht umgekehrt –, und dieser Austausch hört auf, wenn beide die gleiche Temperatur haben. Die Entropie des Systems hat jetzt ihr Maximum erreicht.\nDie Fachleute hatten auch angefangen, diesen Vorgang präzise zu erfassen, und zwar durch die Annahme, dass die Gase (oder andere physikalische Systeme) aus Atomen bestehen. Hier ist ein wenig Vorsicht geboten, denn so einfach dieser Satz heute klingt, so skeptisch wurde er damals begrüßt. Niemand wusste sicher zu sagen, ob es diese Gebilde gab, die ihren Namen schon in der Antike bekommen hatten. Und erst recht hatte damals niemand auch nur eine vage Idee, wie die Atome aussehen sollten. Trotzdem – als Hypothese darf man sie einführen, und wer das macht, verfügt über die Möglichkeit, der Temperatur eines Gases eine Deutung zu geben. Er denkt sich Atome als kleine, harte und elastische Kügelchen – winzige Flummies, wenn man so will –, die schneller oder langsamer unterwegs sind und zusammenstoßen können. Sind die Atome schnell, ist die Temperatur des Gases, das aus ihnen besteht, hoch. Sind die Atome langsam, ist die Temperatur des Gases, das aus ihnen besteht, niedrig. Wenn schnelle und langsame Kügelchen zusammenstoßen, tauschen sie ihre Energie aus, und da die langsamen Atome dabei vor allem etwas von den schnellen Exemplaren abbekommen, lässt sich jetzt der Zweite Hauptsatz gut verstehen. Er besagt, dass als Folge der Zusammenstöße zuletzt die Geschwindigkeiten aller umhersausenden Flummies gleich sind, und das ist auch genau das, was man beobachtet.\nEs war für Planck und seine Mitstreiter offensichtlich, dass diese mechanische Deutung der Wärme ein befriedigendes Gesamtbild der physikalischen Wirklichkeit abgab, und es störte sie nicht, dass es viele Kritiker dieser Theorie gab, die vor allem darauf hinwiesen, dass die Atome im Zentrum des Verstehens nur sehr unzulänglich beschrieben waren und ihr Wirken im Detail unklar blieb. Dieser Einwand war sicher berechtigt, aber es ist nicht zu erwarten, dass eine gute Hypothese gleich alle Fragen der Physik klärt. Es reicht, wenn sie in einigen Fällen weiterhilft – aber nur, wenn sie nicht zugleich größere Probleme schafft. Genau dies letztere aber schien der Fall zu sein, und zwar infolge des Gedankenexperiments des Schotten Maxwell, das als Maxwells Dämon berühmt geworden ist.\nMaxwell stellte sich die beiden oben erwähnten Gase vor, die er in nur durch eine Scheidewand getrennten Behältern nebeneinander platzierte. Eine Prämisse seiner Arbeit war, dass nicht alle Atome gleich schnell oder langsam sind. Es gibt vielmehr eine Verteilung ihrer Geschwindigkeit – die Maxwellsche Verteilung –, und sie liefert die Grundlage für eine statistische Behandlung von Gasen, was konkret bedeutet, dass es auf der warmen Seite sehr viele schnelle, aber auch ein paar langsame Atome gibt, und entsprechend auf der kalten Seite sehr viel langsame, aber auch ein paar schnelle. Nach der Entfernung der Trennwand käme es zu dem Ausgleich der Temperaturen, wie ihn der Zweite Hauptsatz vorhersagt.\nMaxwell ging nun aber einen Schritt weiter und überlegte, was geschähe, wenn die Gase statt durch eine Scheidewand durch eine bewegliche Klappe getrennt wären. In seinem Gedankenspiel positionierte er an jener Klappe einen Dämon, dem er eine einfache Aufgabe stellte: Er sollte die Atome sortieren. Wenn aus der warmen Kammer ein schnelles Atom kommt, soll er es abweisen; wenn aus der warmen Kammer ein langsames Atom kommt, soll er es durchlassen. Umgekehrt: Wenn aus der kalten Kammer ein schnelles Atom kommt, soll er es durchlassen; wenn aus der kalten Kammer ein langsames Molekül kommt, soll er es abweisen. Das Gedankenexperiment wirft konkret die Frage auf, warum es solch einen Dämon nicht geben kann. Schließlich scheinen solche Sortierprozesse in der Natur nicht vorzukommen, was die Frage aufwirft, wo der Denkfehler bei diesem Gedankenexperiment liegt. Wieso also kann es den Dämon nicht geben?",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Das große Interesse der Physik und anderer Wissenschaften an der Energie hing im 19. Jahrhundert vor allem mit der Notwendigkeit zusammen, besser zu verstehen, wie Maschinen funktionierten. Es war die Zeit der großen Industrialisierung, und überall wurden Dampf- und Elektromaschinen installiert, um Arbeiten zu verrichten. Die Unternehmen – und nicht nur sie – wollten wissen, wie man mit möglichst geringem Aufwand möglichst viel aus einer Maschine herausholen kann, was physikalisch eine Antwort auf die Frage verlangte, wie viel von der Energie, die man etwa in Form von Kohle oder Strom in eine Maschine einbrachte, in Arbeit umgesetzt wurde.\nKlar war, dass nicht alle Energie nutzbar gemacht werden konnte und viel verloren ging – etwa durch Reibung oder dadurch, dass heiße Teile einer Maschine einfach abkühlten (Dissipation). Um hier genauer Auskunft geben zu können, unterschieden die Physiker zwischen der Gesamtenergie, die sie einem Apparat zuführten, und der freien Energie, die sie in Arbeit umwandeln konnten, wie sie sich etwa im Transport von Lasten zeigte. Bei ihren Versuchen, genauer zu erfassen, was diese freie Energie sein könnte, fiel den Physikern ganz allgemein auf, dass sie dann, wenn sie nur über Energie nachdachten, einen wesentlichen Aspekt sowohl der Naturvorgänge als auch der Abläufe in Maschinen außer Acht ließen und nicht in der Griff bekamen – nämlich die Richtung, in der Prozesse ablaufen. Mit der Richtung ist nicht gemeint, ob eine Kugel nach oben oder unten fliegt oder ob ein Ball umkehrt, nachdem ihn jemand gegen eine Mauer geschossen hat. Mit Richtung ist gemeint, dass zum Beispiel dann, wenn man einen Eiswürfel in ein Wasserglas gibt, die Wärme stets von der warmen Umgebung zum kalten Gefrorenen strömt und niemals die Gegenrichtung einschlägt. Wer ein kühles Glas Wein in einer lauen Sommernacht trinkt, wird merken, dass dessen Temperatur nur ansteigt. Es scheint ausgeschlossen zu sein, dass die warme Luft sich zusätzlich aus der Energie der Flüssigkeit bedient. Das Weinglas wird wärmer, bis es das Niveau der Abendluft erreicht hat. Dann kommt der Vorgang des Energietransports an sein Ende. \nMax Planck stellte in seinen Vorlesungen zur Thermodynamik klar: Ob „Wärmeleitung in die Richtung vom wärmeren zum kälteren Körper erfolgt oder umgekehrt, daraus lässt sich aus dem Energieprinzip allein nicht das mindeste schließen.“ Mit „Energieprinzip“ meinte Planck den Ersten Hauptsatz der Thermodynamik, dem er in seinen berühmten „Vorlesungen über Thermodynamik“ bald den Zweiten an die Seite stellte, den er so formulierte:„In der Natur existiert für jedes Körpersystem eine Größe, welche die Eigenschaft besitzt, bei allen Veränderungen, die das System allein betreffen, entweder konstant zu bleiben oder an Wert zuzunehmen.“ Für diese Größe hatte der Physiker Rudolf Clausius den Namen Entropie (Wirksamkeit und Kraft) eingeführt. Entropie stammt vom griechischen Wort entrepein ab, das „umkehren“ bedeutet. Offenbar gibt es in der Natur Vorgänge, die umkehrbar sind – Wasser kann erst zu Eis gefrieren und dann wieder schmelzen, wenn die Temperatur steigt –, aber die meisten Abläufe der Natur sind unumkehrbar. Und darüber entscheidet nicht die zu- oder abgeführte Energie, sondern die Entropie. Sie steigt an, wenn ein Vorgang nicht umkehrbar ist, und sie kann nur ansteigen und niemals abnehmen. Mit anderen Worten, die Vorgänge der Natur laufen in ihrer überwiegenden Art so ab, dass die Entropie stets zunimmt, was Clausius in selbstbewusster Manier in einer universalen Formulierung zusammenfasste: „Die Entropie der Welt strebt einem Maximum zu“.\nEs ist heute schwer vorstellbar, welches Interesse die Physik des 19. Jahrhunderts in einigen intellektuellen Kreisen fand, vor allem, nachdem sie es riskiert hatte, universale Behauptungen aufzustellen. Bald gab es einfache Deutungen der Entropie, die mit dem anschaulichen Konzept der Ordnung agierten und besagten, dass dem Zweiten Hauptsatz der Thermodynamik zufolge die Unordnung der Welt nur zunehmen könnte. Diesen Sachverhalt kennen wir alle aus dem Alltag. Er besagt, dass die Unordnung eines Zimmers nur wachsen kann, wenn niemand aufräumt. In der Natur gibt es bekanntlich keine Putzhilfe, also läuft es dort ab wie in einer Junggesellenbude, die zuletzt wie ein Saustall aussieht.\nTatsächlich sahen einige Intellektuelle im Zweiten Hauptsatz daher bald einen physikalischen Beweis für den notwendigen Untergang der Kultur. Die Urheber des Zweiten Hauptsatzes aber hatten zunächst ganz andere Probleme, nämlich zu verstehen, was die Entropie tatsächlich erfasst und wie die Naturabläufe mit ihrer Hilfe die Richtung bekommen, die sie haben. Es ist doch keine Frage, dass es gerichtet in der Natur zugeht und etwa ein Tintentropfen in einem Wasserglas nur zerfließt und sich nie wieder zusammenzieht. Und wenn man ein Gefäß, in dem sich ein Gas mit hoher Temperatur befindet, neben ein Gefäß stellt, in dem dasselbe Gas eine niedrigere Temperatur hat, dann wechselt die Energie nur von der warmen auf die kalte Seite – und nicht umgekehrt –, und dieser Austausch hört auf, wenn beide die gleiche Temperatur haben. Die Entropie des Systems hat jetzt ihr Maximum erreicht.\nDie Fachleute hatten auch angefangen, diesen Vorgang präzise zu erfassen, und zwar durch die Annahme, dass die Gase (oder andere physikalische Systeme) aus Atomen bestehen. Hier ist ein wenig Vorsicht geboten, denn so einfach dieser Satz heute klingt, so skeptisch wurde er damals begrüßt. Niemand wusste sicher zu sagen, ob es diese Gebilde gab, die ihren Namen schon in der Antike bekommen hatten. Und erst recht hatte damals niemand auch nur eine vage Idee, wie die Atome aussehen sollten. Trotzdem – als Hypothese darf man sie einführen, und wer das macht, verfügt über die Möglichkeit, der Temperatur eines Gases eine Deutung zu geben. Er denkt sich Atome als kleine, harte und elastische Kügelchen – winzige Flummies, wenn man so will –, die schneller oder langsamer unterwegs sind und zusammenstoßen können. Sind die Atome schnell, ist die Temperatur des Gases, das aus ihnen besteht, hoch. Sind die Atome langsam, ist die Temperatur des Gases, das aus ihnen besteht, niedrig. Wenn schnelle und langsame Kügelchen zusammenstoßen, tauschen sie ihre Energie aus, und da die langsamen Atome dabei vor allem etwas von den schnellen Exemplaren abbekommen, lässt sich jetzt der Zweite Hauptsatz gut verstehen. Er besagt, dass als Folge der Zusammenstöße zuletzt die Geschwindigkeiten aller umhersausenden Flummies gleich sind, und das ist auch genau das, was man beobachtet.\nEs war für Planck und seine Mitstreiter offensichtlich, dass diese mechanische Deutung der Wärme ein befriedigendes Gesamtbild der physikalischen Wirklichkeit abgab, und es störte sie nicht, dass es viele Kritiker dieser Theorie gab, die vor allem darauf hinwiesen, dass die Atome im Zentrum des Verstehens nur sehr unzulänglich beschrieben waren und ihr Wirken im Detail unklar blieb. Dieser Einwand war sicher berechtigt, aber es ist nicht zu erwarten, dass eine gute Hypothese gleich alle Fragen der Physik klärt. Es reicht, wenn sie in einigen Fällen weiterhilft – aber nur, wenn sie nicht zugleich größere Probleme schafft. Genau dies letztere aber schien der Fall zu sein, und zwar infolge des Gedankenexperiments des Schotten Maxwell, das als Maxwells Dämon berühmt geworden ist.\nMaxwell stellte sich die beiden oben erwähnten Gase vor, die er in nur durch eine Scheidewand getrennten Behältern nebeneinander platzierte. Eine Prämisse seiner Arbeit war, dass nicht alle Atome gleich schnell oder langsam sind. Es gibt vielmehr eine Verteilung ihrer Geschwindigkeit – die Maxwellsche Verteilung –, und sie liefert die Grundlage für eine statistische Behandlung von Gasen, was konkret bedeutet, dass es auf der warmen Seite sehr viele schnelle, aber auch ein paar langsame Atome gibt, und entsprechend auf der kalten Seite sehr viel langsame, aber auch ein paar schnelle. Nach der Entfernung der Trennwand käme es zu dem Ausgleich der Temperaturen, wie ihn der Zweite Hauptsatz vorhersagt.\nMaxwell ging nun aber einen Schritt weiter und überlegte, was geschähe, wenn die Gase statt durch eine Scheidewand durch eine bewegliche Klappe getrennt wären. In seinem Gedankenspiel positionierte er an jener Klappe einen Dämon, dem er eine einfache Aufgabe stellte: Er sollte die Atome sortieren. Wenn aus der warmen Kammer ein schnelles Atom kommt, soll er es abweisen; wenn aus der warmen Kammer ein langsames Atom kommt, soll er es durchlassen. Umgekehrt: Wenn aus der kalten Kammer ein schnelles Atom kommt, soll er es durchlassen; wenn aus der kalten Kammer ein langsames Molekül kommt, soll er es abweisen. Das Gedankenexperiment wirft konkret die Frage auf, warum es solch einen Dämon nicht geben kann. Schließlich scheinen solche Sortierprozesse in der Natur nicht vorzukommen, was die Frage aufwirft, wo der Denkfehler bei diesem Gedankenexperiment liegt. Wieso also kann es den Dämon nicht geben?",
      "id": [
        "145819"
      ],
      "item": "Die Richtung der Natur"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145820]\n[%Rudolf Clausius]\nRudolf Clausius (1822 – 1888) gilt als Entdecker des Zweiten Hauptsatzes der Thermodynamik. Von ihm stammt der Begriff der Entropie.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Rudolf Clausius (1822 – 1888) gilt als Entdecker des Zweiten Hauptsatzes der Thermodynamik. Von ihm stammt der Begriff der Entropie.",
      "id": [
        "145820"
      ],
      "item": "Rudolf Clausius"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145821]\n[%Eine Frage der Information]\nDer Ausdruck „Maxwellscher Dämon“ stammt von dem britischen Physiker Lord Kelvin, der sich 1874 mit dem Thema befasste und bemerkte, dass in dieser Konstruktion allein deshalb ein tiefes Problem steckte, weil weder der Erste noch der Zweite Hauptsatz der Thermodynamik bewiesen waren. Bei ihnen handelte es sich um die Zusammenfassung von Erfahrungen, zu denen sich ja jederzeit neue gesellen konnten. Und wer konnte schon sicher sein, dass sie stets mit den entsprechenden Aussagen der Physik in Übereinstimmung sein würden? Was wäre, wenn man tatsächlich ein trickreiches technisches System à la Maxwellscher Dämon konstruieren könnte, das die natürliche Richtung von Prozessen umkehrt?\nViele Zeitgenossen von Maxwell, Kelvin, Clausius und Planck bemühten sich, das Teufelchen zu erledigen, aber außer dem eher hilflosen Hinweis, dass es sich hier um ein akademisches Spielchen ohne praktische Folge handelte, ist den Wissenschaftlern lange Zeit nichts von Interesse eingefallen. Mit der langen Zeit sind viele Jahrzehnte gemeint, denn tatsächlich dauerte es bis 1929, bevor auf diesem Gebiet endlich wieder ein Fortschritt zu vermelden war. Damals publizierte der aus Ungarn stammende Physiker Leó Szilárd eine Schrift mit dem nicht gerade übermäßig viel Spannung versprechenden Titel „Über die Entropieverminderung in einem thermodynamischen System bei Eingriffen intelligenter Wesen“.\nWer sich auf die etwas vertrackte, aber wissenschaftlich präzise Sprache einlässt, wird ablesen können, dass Szilárd erstens genau auf Maxwells Dämon anspielt und zweitens den Finger auf die Wunde legt. Der Teufel kann nicht bloß als physikalischer Apparat funktionieren, er muss darüber hinaus auch intelligent sein. Der Dämon muss ja Entscheidungen treffen, und die kann er nur ausführen, wenn er über die dazu nötigen Kenntnisse verfügt. An dieser Stelle kommt ein Konzept ins Spiel, das wir heute ganz selbstverständlich verwenden, mit dem wir höchst vertraut sind und das wir daher nicht weiter definieren, das aber – wie viele große Ideen der Menschheit – erst einmal entdeckt und eingeführt werden musste. Gemeint ist das Konzept der Information.\nMaxwell und Planck mussten – wörtlich verstanden – ohne diese Information auskommen, und deshalb stellte der Dämon für sie tatsächlich ein Problem dar. Mit der Information jedoch wird die Sache übersichtlicher, denn das Teufelchen muss die für seine Entscheidungen nötigen Informationen erst einmal erwerben, was nicht ohne Entropieerzeugung vor sich geht, wie Szilárd grob ausrechnen konnte. Aber was noch wichtiger ist, der Dämon muss die Information auch irgendwo speichern, was verhindert, dass er beliebig klein konstruiert sein kann. Szilárd führte vor, dass die Physik nicht verstanden werden konnte, ohne Konzepte wie Messung, Information und Speicherung mit in ihre Berechnungen einzubeziehen, oder, wenn man seine Lösung für Maxwells Problem einfach ausdrücken will: Die neue Unübersichtlichkeit verwirrt den Dämon so, dass er irgendwann nicht mehr genau genug zwischen schnellen und langsamen Atomen unterscheiden kann. Der Zweite Hauptsatz schien unbeschadet überlebt zu haben.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der Ausdruck „Maxwellscher Dämon“ stammt von dem britischen Physiker Lord Kelvin, der sich 1874 mit dem Thema befasste und bemerkte, dass in dieser Konstruktion allein deshalb ein tiefes Problem steckte, weil weder der Erste noch der Zweite Hauptsatz der Thermodynamik bewiesen waren. Bei ihnen handelte es sich um die Zusammenfassung von Erfahrungen, zu denen sich ja jederzeit neue gesellen konnten. Und wer konnte schon sicher sein, dass sie stets mit den entsprechenden Aussagen der Physik in Übereinstimmung sein würden? Was wäre, wenn man tatsächlich ein trickreiches technisches System à la Maxwellscher Dämon konstruieren könnte, das die natürliche Richtung von Prozessen umkehrt?\nViele Zeitgenossen von Maxwell, Kelvin, Clausius und Planck bemühten sich, das Teufelchen zu erledigen, aber außer dem eher hilflosen Hinweis, dass es sich hier um ein akademisches Spielchen ohne praktische Folge handelte, ist den Wissenschaftlern lange Zeit nichts von Interesse eingefallen. Mit der langen Zeit sind viele Jahrzehnte gemeint, denn tatsächlich dauerte es bis 1929, bevor auf diesem Gebiet endlich wieder ein Fortschritt zu vermelden war. Damals publizierte der aus Ungarn stammende Physiker Leó Szilárd eine Schrift mit dem nicht gerade übermäßig viel Spannung versprechenden Titel „Über die Entropieverminderung in einem thermodynamischen System bei Eingriffen intelligenter Wesen“.\nWer sich auf die etwas vertrackte, aber wissenschaftlich präzise Sprache einlässt, wird ablesen können, dass Szilárd erstens genau auf Maxwells Dämon anspielt und zweitens den Finger auf die Wunde legt. Der Teufel kann nicht bloß als physikalischer Apparat funktionieren, er muss darüber hinaus auch intelligent sein. Der Dämon muss ja Entscheidungen treffen, und die kann er nur ausführen, wenn er über die dazu nötigen Kenntnisse verfügt. An dieser Stelle kommt ein Konzept ins Spiel, das wir heute ganz selbstverständlich verwenden, mit dem wir höchst vertraut sind und das wir daher nicht weiter definieren, das aber – wie viele große Ideen der Menschheit – erst einmal entdeckt und eingeführt werden musste. Gemeint ist das Konzept der Information.\nMaxwell und Planck mussten – wörtlich verstanden – ohne diese Information auskommen, und deshalb stellte der Dämon für sie tatsächlich ein Problem dar. Mit der Information jedoch wird die Sache übersichtlicher, denn das Teufelchen muss die für seine Entscheidungen nötigen Informationen erst einmal erwerben, was nicht ohne Entropieerzeugung vor sich geht, wie Szilárd grob ausrechnen konnte. Aber was noch wichtiger ist, der Dämon muss die Information auch irgendwo speichern, was verhindert, dass er beliebig klein konstruiert sein kann. Szilárd führte vor, dass die Physik nicht verstanden werden konnte, ohne Konzepte wie Messung, Information und Speicherung mit in ihre Berechnungen einzubeziehen, oder, wenn man seine Lösung für Maxwells Problem einfach ausdrücken will: Die neue Unübersichtlichkeit verwirrt den Dämon so, dass er irgendwann nicht mehr genau genug zwischen schnellen und langsamen Atomen unterscheiden kann. Der Zweite Hauptsatz schien unbeschadet überlebt zu haben.",
      "id": [
        "145821"
      ],
      "item": "Eine Frage der Information"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145822]\n[%Leó Szilárd]\nLeó Szilárd wurde 1898 in Budapest geboren. Er studierte und forschte in Berlin, bis er 1938 in die USA emigrierte. Hier arbeitete er am Manhattan-Projekt mit, und 1942 konnte er mit Enrico Fermi die erste kontrollierte Kettenreaktion in einem Atomreaktor auslösen. Szilárd starb 1964.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Leó Szilárd wurde 1898 in Budapest geboren. Er studierte und forschte in Berlin, bis er 1938 in die USA emigrierte. Hier arbeitete er am Manhattan-Projekt mit, und 1942 konnte er mit Enrico Fermi die erste kontrollierte Kettenreaktion in einem Atomreaktor auslösen. Szilárd starb 1964.",
      "id": [
        "145822"
      ],
      "item": "Leó Szilárd"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145823]\n[%Der Preis des Vergessens und die Überwindung des Dämons]\nDie Physiker bewunderten Leó Szilárd, der den „Maxwellschen Dämon“ mit seiner Schrift „Über die Entropieverminderung in einem thermodynamischen System bei Eingriffen intelligenter Wesen“ angriff. Szilárd tat sich im Übrigen kurz nach seinem Angriff auf den Dämon mit Albert Einstein zusammen, um mit ihm gemeinsam den legendären Brief an den amerikanischen Präsidenten Roosevelt zu schreiben, in dessen Folge das Manhattan-Projekt der amerikanischen Atombombe ins Laufen kam. In den unruhigen Zeiten des Zweiten Weltkriegs galt es, andere Dämonen als den von Maxwell zu beseitigen, und so dauerte es bis in die frühen 1950er Jahre, bevor sich einige Physiker erneut Szilárds Lösung vornahmen, unter anderem mit der Absicht, die Wechselwirkung zwischen dem Dämon und den Kügelchen – den alten Atomen – im Rahmen der neuen Physik genauer zu verstehen, die inzwischen mit Quanten operierte. Dabei fiel ihnen unter anderem auf, dass die alte Größe Entropie und die neue Größe Information in der Tiefe zusammenhingen. Die eine war mehr oder weniger das Gegenstück zur anderen, was eine Konsequenz hat, die sich in drei Worten ausdrücken lässt und die uns bis zum Ende dieses Buches immer mehr beschäftigen wird: „Information ist physikalisch.“\nTatsächlich: Information unterliegt den Gesetzen der Physik, und als in den späten 1950er und frühen 1960er Jahren die Konstrukteure von Computern an die Stelle der Konstrukteure der Dampfmaschinen traten, wollten auch sie wissen, an welcher Stelle es zu thermodynamischen Verlusten kommen kann. \nBesonders intensiv kümmerte sich Rolf Landauer bei der Firma IBM um diese Frage, und er versuchte – mit dem Zweiten Hauptsatz der Thermodynamik und Maxwells Dämon im Hinterkopf – genau die Stelle ausfindig zu machen, an der in den Rechenmaschinen Energie in Wärme umgewandelt wird und so für ihren eigentlichen Zweck verloren geht. Im Jahre 1961 hatte er Erfolg, und er konnte ein Prinzip formulieren – das Landauers Prinzip –, mit dem zum ersten Mal wirklich verstanden werden kann, was dem Dämon das Leben schwer macht. Landauer veröffentlichte seinen dazugehörigen Aufsatz – er trug den Titel „Irreversibility and heat generation in the computing process“ – in einem von IBM herausgegebenen Forschungsmagazin, was ihn nicht unbedingt und erst recht nicht sofort unter die Leute brachte.\nWas Landauer mitzuteilen hatte, lief auf Folgendes hinaus: Im Gegensatz zu der traditionell vertretenen Ansicht entstehen thermodynamische Verluste in Computern nicht, wenn Information verarbeitet (aufgenommen und genutzt) wird. Der einzige Schritt, bei dem sich ein elementarer Verlust nicht vermeiden lässt, ist die Zerstörung (das Löschen) von Information – das Vergessen. Und dass der Dämon viel Löschen muss, leuchtet sofort ein, wenn man sich einmal vor Augen hält, was er zu leisten hat: Der Teufel muss ja nicht nur ein oder zwei Atome im Kasten messen und sortieren. Er muss vielmehr gigantische Mengen an Atomen ansehen und prüfen, und das heißt, dass er ein ebenso gigantisches Gedächtnis – unglaublich viel Speicherplatz – benötigt, was ihn sicher bald größer als die ganze Anlage – und damit völlig wertlos – macht. Der Dämon muss also neben seiner Aufgabe der Informationsgewinnung die noch viel wichtigere Aufgabe der Informationsvernichtung betreiben. Er muss seinen Speicher unentwegt löschen, und dafür zahlt er das, was man poetisch den „Preis des Vergessens“ nennen könnte. Er ist es, der vom Zweiten Hauptsatz eingefordert wird, der jetzt tatsächlich endgültig alle Dämonen und ihre Vertreiber souverän überstanden hat.\nAls der amerikanische Physiker Charles Bennett im Jahre 1982 Landauers Prinzip auf das Gedächtnis von Maxwells Dämon anwenden und dabei zeigen konnte, dass auf diese Weise das Gas und seine Atome genau mit der Entropie wieder ausgestattet werden, die der Zweite Hauptsatz verlangt, hatte die Physik endlich ihre innere Ruhe wiedergefunden, die Maxwells Dämon ihr vor mehr als 100 Jahren genommen hatte. Es sei denn, morgen findet jemand einen Aspekt, den wir bislang übersehen haben. Diese Möglichkeit sollten wir nicht vergessen, auch wenn wir dafür mit Entropie zahlen müssen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Die Physiker bewunderten Leó Szilárd, der den „Maxwellschen Dämon“ mit seiner Schrift „Über die Entropieverminderung in einem thermodynamischen System bei Eingriffen intelligenter Wesen“ angriff. Szilárd tat sich im Übrigen kurz nach seinem Angriff auf den Dämon mit Albert Einstein zusammen, um mit ihm gemeinsam den legendären Brief an den amerikanischen Präsidenten Roosevelt zu schreiben, in dessen Folge das Manhattan-Projekt der amerikanischen Atombombe ins Laufen kam. In den unruhigen Zeiten des Zweiten Weltkriegs galt es, andere Dämonen als den von Maxwell zu beseitigen, und so dauerte es bis in die frühen 1950er Jahre, bevor sich einige Physiker erneut Szilárds Lösung vornahmen, unter anderem mit der Absicht, die Wechselwirkung zwischen dem Dämon und den Kügelchen – den alten Atomen – im Rahmen der neuen Physik genauer zu verstehen, die inzwischen mit Quanten operierte. Dabei fiel ihnen unter anderem auf, dass die alte Größe Entropie und die neue Größe Information in der Tiefe zusammenhingen. Die eine war mehr oder weniger das Gegenstück zur anderen, was eine Konsequenz hat, die sich in drei Worten ausdrücken lässt und die uns bis zum Ende dieses Buches immer mehr beschäftigen wird: „Information ist physikalisch.“\nTatsächlich: Information unterliegt den Gesetzen der Physik, und als in den späten 1950er und frühen 1960er Jahren die Konstrukteure von Computern an die Stelle der Konstrukteure der Dampfmaschinen traten, wollten auch sie wissen, an welcher Stelle es zu thermodynamischen Verlusten kommen kann. \nBesonders intensiv kümmerte sich Rolf Landauer bei der Firma IBM um diese Frage, und er versuchte – mit dem Zweiten Hauptsatz der Thermodynamik und Maxwells Dämon im Hinterkopf – genau die Stelle ausfindig zu machen, an der in den Rechenmaschinen Energie in Wärme umgewandelt wird und so für ihren eigentlichen Zweck verloren geht. Im Jahre 1961 hatte er Erfolg, und er konnte ein Prinzip formulieren – das Landauers Prinzip –, mit dem zum ersten Mal wirklich verstanden werden kann, was dem Dämon das Leben schwer macht. Landauer veröffentlichte seinen dazugehörigen Aufsatz – er trug den Titel „Irreversibility and heat generation in the computing process“ – in einem von IBM herausgegebenen Forschungsmagazin, was ihn nicht unbedingt und erst recht nicht sofort unter die Leute brachte.\nWas Landauer mitzuteilen hatte, lief auf Folgendes hinaus: Im Gegensatz zu der traditionell vertretenen Ansicht entstehen thermodynamische Verluste in Computern nicht, wenn Information verarbeitet (aufgenommen und genutzt) wird. Der einzige Schritt, bei dem sich ein elementarer Verlust nicht vermeiden lässt, ist die Zerstörung (das Löschen) von Information – das Vergessen. Und dass der Dämon viel Löschen muss, leuchtet sofort ein, wenn man sich einmal vor Augen hält, was er zu leisten hat: Der Teufel muss ja nicht nur ein oder zwei Atome im Kasten messen und sortieren. Er muss vielmehr gigantische Mengen an Atomen ansehen und prüfen, und das heißt, dass er ein ebenso gigantisches Gedächtnis – unglaublich viel Speicherplatz – benötigt, was ihn sicher bald größer als die ganze Anlage – und damit völlig wertlos – macht. Der Dämon muss also neben seiner Aufgabe der Informationsgewinnung die noch viel wichtigere Aufgabe der Informationsvernichtung betreiben. Er muss seinen Speicher unentwegt löschen, und dafür zahlt er das, was man poetisch den „Preis des Vergessens“ nennen könnte. Er ist es, der vom Zweiten Hauptsatz eingefordert wird, der jetzt tatsächlich endgültig alle Dämonen und ihre Vertreiber souverän überstanden hat.\nAls der amerikanische Physiker Charles Bennett im Jahre 1982 Landauers Prinzip auf das Gedächtnis von Maxwells Dämon anwenden und dabei zeigen konnte, dass auf diese Weise das Gas und seine Atome genau mit der Entropie wieder ausgestattet werden, die der Zweite Hauptsatz verlangt, hatte die Physik endlich ihre innere Ruhe wiedergefunden, die Maxwells Dämon ihr vor mehr als 100 Jahren genommen hatte. Es sei denn, morgen findet jemand einen Aspekt, den wir bislang übersehen haben. Diese Möglichkeit sollten wir nicht vergessen, auch wenn wir dafür mit Entropie zahlen müssen.",
      "id": [
        "145823"
      ],
      "item": "Der Preis des Vergessens und die Überwindung des Dämons"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145824]\n[%Rolf Landauer]\nRolf Landauer (1927-1999) war einer der bedeutendsten Physiker des 20. Jahrhunderts. Von ihm stammt die Formulierung des „Landauer-Prinzips“, demzufolge jedes Löschen von Informationen Energie verbraucht. Landauer wurde in Stuttgart geboren, floh mit seiner jüdischen Familie 1938 in die USA, studierte in Harvard und arbeitete später für IBM.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Rolf Landauer (1927-1999) war einer der bedeutendsten Physiker des 20. Jahrhunderts. Von ihm stammt die Formulierung des „Landauer-Prinzips“, demzufolge jedes Löschen von Informationen Energie verbraucht. Landauer wurde in Stuttgart geboren, floh mit seiner jüdischen Familie 1938 in die USA, studierte in Harvard und arbeitete später für IBM.",
      "id": [
        "145824"
      ],
      "item": "Rolf Landauer"
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145825]\n[##Die Doppelhelix und die genetische Information]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 2,
      "progress": true,
      "toc": true,
      "id": [
        "145825"
      ],
      "title": "Die Doppelhelix und die genetische Information"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145826]\n[%Einführung ins Jahrhundert der Genetik]\nAls das Landauer-Prinzip, mit welchem Rolf Landauer mitteilte, dass thermodynamische Verluste in Computern nicht wegen Informationsverarbeitung sondern wegen deren Zerstörung verursacht wird, formuliert wurde und die Information – wegen den ihr zugrunde liegenden physikalischen Gesetze – physikalisch wurde, da wurde sie auch genetisch. Das heißt, Anfang der 1960er Jahre fingen die Molekularbiologen damit an, den genetischen Code zu entschlüsseln und damit die Weitergabe oder Nutzung von Information zum Thema ihrer Forschung zu machen. Inzwischen ist die Genetik – nach einem Jahrhundert der Vorbereitungen – beim Menschen angekommen. Sie wird sein genetisches Profil bestimmen und ihm eine Diskette oder Gen-Chips mit zahlreichen Erbinformationen in die Hand geben. Mit deren Hilfe wird jeder ein persönliches Rezept ganz für sich allein zugeschnitten bekommen oder zum Beispiel erfahren, von welchem Lebensjahr an ihm Alterserscheinungen wie die Alzheimer-Krankheit Probleme bereiten können. Die Genetik wird eine neue Medizin mit neuen Diagnosemöglichkeiten und neuen Therapien ermöglichen, und wenn sie dies tut, wird sie das erreicht haben, was sie sich vor ziemlich genau einhundert Jahren vorgenommen hat. Genetik im 20. Jahrhundert hat nämlich von Anfang an auf den Menschen geschaut, auch wenn ein flüchtiger Blick auf die Geschichte der Erbforschung den Eindruck entstehen lässt, dass es mehr um Erbsen und Fliegen, Bakterien und Viren gegangen sei.\nEs stimmt natürlich, dass die westliche Wissenschaft ihre ersten Einsichten in die Abläufe der Vererbung durch das Zählen von Erbsen im Garten des österreichischen Augustinermönchs Gregor Mendel bekommen hat. Es trifft weiter zu, dass die klassische Form der Genetik vor allem durch die Kreuzung von Fliegen im Laboratorium des amerikanischen Biologen Thomas H. Morgan entstanden ist. Und es ist ebenfalls richtig, dass der Weg in die moderne Molekularbiologie erst durch Arbeiten mit Viren und Bakterien freigelegt worden ist. Doch bereits ganz zu Beginn des 20. Jahrhunderts ist die Frage aufgetaucht, um die es bis heute geht, und zwar die Frage nach einem wissenschaftlichen Verständnis für die Einzigartigkeit eines jeden Menschen. Zwar sehen unsere Gesichter – von außen betrachtet – alle verschieden aus. Aber was ist mit den Molekülen in unseren Zellen? Mein Cholesterin wird sich höchstens der Menge nach von dem meines Nachbarn unterscheiden.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Als das Landauer-Prinzip, mit welchem Rolf Landauer mitteilte, dass thermodynamische Verluste in Computern nicht wegen Informationsverarbeitung sondern wegen deren Zerstörung verursacht wird, formuliert wurde und die Information – wegen den ihr zugrunde liegenden physikalischen Gesetze – physikalisch wurde, da wurde sie auch genetisch. Das heißt, Anfang der 1960er Jahre fingen die Molekularbiologen damit an, den genetischen Code zu entschlüsseln und damit die Weitergabe oder Nutzung von Information zum Thema ihrer Forschung zu machen. Inzwischen ist die Genetik – nach einem Jahrhundert der Vorbereitungen – beim Menschen angekommen. Sie wird sein genetisches Profil bestimmen und ihm eine Diskette oder Gen-Chips mit zahlreichen Erbinformationen in die Hand geben. Mit deren Hilfe wird jeder ein persönliches Rezept ganz für sich allein zugeschnitten bekommen oder zum Beispiel erfahren, von welchem Lebensjahr an ihm Alterserscheinungen wie die Alzheimer-Krankheit Probleme bereiten können. Die Genetik wird eine neue Medizin mit neuen Diagnosemöglichkeiten und neuen Therapien ermöglichen, und wenn sie dies tut, wird sie das erreicht haben, was sie sich vor ziemlich genau einhundert Jahren vorgenommen hat. Genetik im 20. Jahrhundert hat nämlich von Anfang an auf den Menschen geschaut, auch wenn ein flüchtiger Blick auf die Geschichte der Erbforschung den Eindruck entstehen lässt, dass es mehr um Erbsen und Fliegen, Bakterien und Viren gegangen sei.\nEs stimmt natürlich, dass die westliche Wissenschaft ihre ersten Einsichten in die Abläufe der Vererbung durch das Zählen von Erbsen im Garten des österreichischen Augustinermönchs Gregor Mendel bekommen hat. Es trifft weiter zu, dass die klassische Form der Genetik vor allem durch die Kreuzung von Fliegen im Laboratorium des amerikanischen Biologen Thomas H. Morgan entstanden ist. Und es ist ebenfalls richtig, dass der Weg in die moderne Molekularbiologie erst durch Arbeiten mit Viren und Bakterien freigelegt worden ist. Doch bereits ganz zu Beginn des 20. Jahrhunderts ist die Frage aufgetaucht, um die es bis heute geht, und zwar die Frage nach einem wissenschaftlichen Verständnis für die Einzigartigkeit eines jeden Menschen. Zwar sehen unsere Gesichter – von außen betrachtet – alle verschieden aus. Aber was ist mit den Molekülen in unseren Zellen? Mein Cholesterin wird sich höchstens der Menge nach von dem meines Nachbarn unterscheiden.",
      "id": [
        "145826"
      ],
      "item": "Einführung ins Jahrhundert der Genetik"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145827]\n[%Thomas Hunt Morgan]\nDer Amerikaner Thomas Hunt Morgan (1866-1945) war es, der zuerst Gene auf den Chromosomen lokalisierte und Genkarten anfertigte.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der Amerikaner Thomas Hunt Morgan (1866-1945) war es, der zuerst Gene auf den Chromosomen lokalisierte und Genkarten anfertigte.",
      "id": [
        "145827"
      ],
      "item": "Thomas Hunt Morgan"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145828]\n[%Eine Wissenschaft kommt nur schwer auf die Beine]\nWo bin ich innen qualitativ anders als er? Wo finde ich meine chemische Individualität? Und wie wirkt sie sich auf mein persönliches Leben aus? Die Genetik, die uns interessiert, beginnt mit diesen Fragen, auch wenn uns dies nirgendwo beigebracht wird. In der Schule erfährt man zumeist, dass einige Naturforscher um 1900 die Weitergabe von sichtbaren Variationen etwa in den Blütenfarben von Pflanzen untersucht und dabei dieselben Regelmäßigkeiten bemerkt haben, mit denen Mendel bereits mehr als eine Generation zuvor vertraut war. Die Lehrer sprechen dann von der Wiederentdeckung der Mendelschen Erbgesetze, und niemand weist die Schüler darauf hin, dass dies allein deshalb nicht stimmen kann, weil Mendel bei den Berichten über seine Versuche im Klostergarten weder das Wort Vererbung noch den Begriff des Gesetzes benutzt hat.\nZeitgenössischen Naturforschern ist unklar geblieben, wonach Mendel gesucht hat. Verstanden hat ihn aber der anonyme Berichterstatter des Brünner Tagblatts, der im März 1865 in seiner Zeitung zusammengefasst hat, was Mendel vor der Naturforschenden Gesellschaft der Stadt über seine „Versuche über Pflanzen-Hybriden“ zu berichtet wusste. Im Tagblatt findet sich der Hinweis, dass Mendel betont habe, alle Pflanzen zeigten die Neigung, „zu den Stammformen zurückzukehren“, was man auch so übersetzen kann, dass der „Vater der Genetik“ meinte, dass eine Evolution nicht stattfindet.\nVon einem glatten Start der Genetik kann also keine Rede sein, und bis zu der Frage nach dem Menschen brauchte es noch Jahrzehnte. Und doch bleibt etwas von Mendel zurück, und das hat konkret mit den Objekten unserer heutigen Begierde, den Genen, zu tun. Für ihre Existenz hatte er ein eigentümliches Verständnis, was sich gut verstehen lässt, da Mendel Physik studiert hatte. Er sollte der Klosterlehrer für dieses Fach werden. Doch er schaffte die Examen nicht, und so wurde er Gärtner. Bei dieser Arbeit sammelte er die zahlreichen Variationen („Varietäten“), die durchreisende Züchter anboten, und er kreuzte sie untereinander. Die Resultate betrachtete er mit den Augen eines Physikers, das heißt, er stellte sich vor, dass es im Inneren der lebenden Materie Grundbestandteile gibt, die den Atomen im Inneren der toten Materie entsprechen. Mendel nahm an, dass die vererbbaren Eigenschaften der blühenden Pflanzen durch „lebendige Wechselwirkung“ dieser „Elemente“ zustande kommen. Erbsen, die sich in diesen Atomen des Lebens unterschieden, zeigten unterschiedliche Qualitäten, und zwar von Generation zu Generation. Mit anderen Worten, Mendel hatte die Gene gefunden, und er stellte sie sich tatsächlich so wie Atome vor, nämlich als unteilbare und unangreifbare Größen im Inneren der Körper. Das einzige, was man tun konnte, bestand darin, sie zu zählen, und zu diesem Zweck unternahm er seine Versuche.\nWer sich einmal die Mühe macht, Mendels Originalarbeit zu lesen, wird dabei finden, dass dies dort nicht so klar steht und man viel in den Text hineindeuten muss, um ihn zu verstehen. Viele Sätze bleiben so undurchschaubar wie die Ursprünge der Genetik selbst, die trotzdem nicht von Mendels Arbeit zu trennen sind, allerdings nur über einen Umweg, für den ein englischer Biologe namens William Bateson verantwortlich ist. Bateson war bei Meerestieren und Würmern auf der Suche nach Regelmäßigkeiten bei der Vererbung, als ihm Mendels Arbeit in die Hände fiel. Beim Versuch, sie in seine Sprache zu übertragen, musste er mehr ersetzen als übersetzen. Anders war die Gen-Algebra nicht durchsichtig zu machen, die Mendels Daten lieferten. Doch Bateson verbesserte das Original bis zur Verständlichkeit, und er kam etwas später – im Jahre 1906 – auf die Idee, für die inzwischen ins Laufen gekommene Wissenschaft von der Vererbung den Namen „Genetik“ vorzuschlagen.\nZu dieser Zeit gab es in London einen Wissenschaftler, dessen Bekanntschaft mit der Vererbungslehre besondere Früchte trug. Er hieß Archibald Garrod, und ihm war bei seiner Tätigkeit als Arzt schon länger aufgefallen, dass es Krankheiten mit Familiengeschichten gab. Farbenblindheit etwa konnte bei Vater und Sohn zugleich festgestellt werden, und Stoffwechselstörungen der Großeltern tauchten oft bei den Enkeln wieder auf. Nach der Durchsicht von Batesons Mendel-Übersetzung wurde Garrod schlagartig klar, dass er die Vererbung von Krankheiten beobachtete, von der man inzwischen wusste, dass sie nach klaren Regeln ablief. Die englische Sprache kennt dafür seitdem den Ausdruck der „Mendelian disease“, und die Wissenschaftler haben sich – heute vor allem in den USA – zum Ziel gesetzt, diese Erbkrankheiten zu verstehen (um sie verhindern oder heilen zu können).\nGarrod war ein guter Wissenschaftler, was heißt, dass er sehr sorgfältig beobachtete und sehr vorsichtige Schlüsse zog. So bemerkte er, dass er genau genommen nicht die Vererbung einer Krankheit selbst, sondern nur die Vererbung einer Anlage für diese Krankheit verfolgen konnte. Die auffälligste Anlage war dabei die Anfälligkeit für Infektionskrankheiten wie Schnupfen, Grippe und Lungenentzündung. Er wusste (nicht nur als Arzt), dass Menschen dabei höchst individuelle Unterschiede zeigten, und er fragte sich, ob auch diese Einzigartigkeit den Mendelschen Regeln unterliegt und somit vererbt wird. Als die Antwort „ja!“ lautete, sah Garrod auf einmal eine Chance und eine Aufgabe für die Erforschung der Vererbung: Sie sollte versuchen, die „chemische Individualität“ des Menschen zu erfassen, um mit dieser Kenntnis vorhersagen zu können, wer zum Beispiel von einer Infektion betroffen wird oder wer unter Nebenwirkungen von Arzneimitteln zu leiden hat, die bekanntlich ebenfalls von Mensch zu Mensch verschieden in Erscheinung treten. Die organische Individualität eines Menschen muss in seinen Genen stecken, so vermutete Garrod, und er hoffte am Anfang des vorigen Jahrhunderts, dass den Ärzten die entsprechenden Informationen eines Tages zum Nutzen ihrer Patienten zur Verfügung stehen würden. Dieses Ziel ist heute greifbar nah, denn genau da hat die Genetik hingeführt.\nGarrod fasste seine Einsichten 1908 zusammen, ein Jahr, bevor die „Elemente“ der Vererbung den Namen bekamen, den sie bis heute tragen, nämlich „Gene“. Der Schöpfer des Begriffs „Gen“, der Däne Wilhelm Johannsen, legte dabei Wert auf ein kurzes Wort, und zwar aus zwei Gründen. Es sollte zum einen leicht kombinierbar sein, und es sollte zum zweiten erlauben, in einfacher Weise von „Genen für bestimmte Eigenschaften“ zu sprechen, etwa von „Genen für blaue Augen“.\nWährend sich die erste Idee bewährt hat, zeigt die zweite vor allem Nachteile. Sie macht es nämlich viel zu leicht, Genen etwas in die Schuhe zu schieben, mit dem sie direkt wenig zu tun haben. Mit der sprachlichen Vorgabe hat sich ein inflationärer Gebrauch des Wortes eingebürgert, der von „Genen für Krebs“ bis zu „Genen für Untreue“ reicht und keinesfalls die „Gene für Intelligenz“ auslässt. So spannend dies auch klingen mag, wer so spricht, fällt hinter Mendel zurück. Denn so unklar die Beschreibung seiner Versuche auch bleibt, so klar war ihm doch, dass es nicht die Eigenschaften eines Lebewesens – wie etwa seine Augenfarbe – sind, die durch „Erbelemente“ bestimmt werden. Was festgelegt wird, sind vielmehr die Unterschiede von Eigenschaften, und zwar durch Unterschiede in Genen. Genau hier steckt auch die Individualität, nach der Garrod fragt.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Wo bin ich innen qualitativ anders als er? Wo finde ich meine chemische Individualität? Und wie wirkt sie sich auf mein persönliches Leben aus? Die Genetik, die uns interessiert, beginnt mit diesen Fragen, auch wenn uns dies nirgendwo beigebracht wird. In der Schule erfährt man zumeist, dass einige Naturforscher um 1900 die Weitergabe von sichtbaren Variationen etwa in den Blütenfarben von Pflanzen untersucht und dabei dieselben Regelmäßigkeiten bemerkt haben, mit denen Mendel bereits mehr als eine Generation zuvor vertraut war. Die Lehrer sprechen dann von der Wiederentdeckung der Mendelschen Erbgesetze, und niemand weist die Schüler darauf hin, dass dies allein deshalb nicht stimmen kann, weil Mendel bei den Berichten über seine Versuche im Klostergarten weder das Wort Vererbung noch den Begriff des Gesetzes benutzt hat.\nZeitgenössischen Naturforschern ist unklar geblieben, wonach Mendel gesucht hat. Verstanden hat ihn aber der anonyme Berichterstatter des Brünner Tagblatts, der im März 1865 in seiner Zeitung zusammengefasst hat, was Mendel vor der Naturforschenden Gesellschaft der Stadt über seine „Versuche über Pflanzen-Hybriden“ zu berichtet wusste. Im Tagblatt findet sich der Hinweis, dass Mendel betont habe, alle Pflanzen zeigten die Neigung, „zu den Stammformen zurückzukehren“, was man auch so übersetzen kann, dass der „Vater der Genetik“ meinte, dass eine Evolution nicht stattfindet.\nVon einem glatten Start der Genetik kann also keine Rede sein, und bis zu der Frage nach dem Menschen brauchte es noch Jahrzehnte. Und doch bleibt etwas von Mendel zurück, und das hat konkret mit den Objekten unserer heutigen Begierde, den Genen, zu tun. Für ihre Existenz hatte er ein eigentümliches Verständnis, was sich gut verstehen lässt, da Mendel Physik studiert hatte. Er sollte der Klosterlehrer für dieses Fach werden. Doch er schaffte die Examen nicht, und so wurde er Gärtner. Bei dieser Arbeit sammelte er die zahlreichen Variationen („Varietäten“), die durchreisende Züchter anboten, und er kreuzte sie untereinander. Die Resultate betrachtete er mit den Augen eines Physikers, das heißt, er stellte sich vor, dass es im Inneren der lebenden Materie Grundbestandteile gibt, die den Atomen im Inneren der toten Materie entsprechen. Mendel nahm an, dass die vererbbaren Eigenschaften der blühenden Pflanzen durch „lebendige Wechselwirkung“ dieser „Elemente“ zustande kommen. Erbsen, die sich in diesen Atomen des Lebens unterschieden, zeigten unterschiedliche Qualitäten, und zwar von Generation zu Generation. Mit anderen Worten, Mendel hatte die Gene gefunden, und er stellte sie sich tatsächlich so wie Atome vor, nämlich als unteilbare und unangreifbare Größen im Inneren der Körper. Das einzige, was man tun konnte, bestand darin, sie zu zählen, und zu diesem Zweck unternahm er seine Versuche.\nWer sich einmal die Mühe macht, Mendels Originalarbeit zu lesen, wird dabei finden, dass dies dort nicht so klar steht und man viel in den Text hineindeuten muss, um ihn zu verstehen. Viele Sätze bleiben so undurchschaubar wie die Ursprünge der Genetik selbst, die trotzdem nicht von Mendels Arbeit zu trennen sind, allerdings nur über einen Umweg, für den ein englischer Biologe namens William Bateson verantwortlich ist. Bateson war bei Meerestieren und Würmern auf der Suche nach Regelmäßigkeiten bei der Vererbung, als ihm Mendels Arbeit in die Hände fiel. Beim Versuch, sie in seine Sprache zu übertragen, musste er mehr ersetzen als übersetzen. Anders war die Gen-Algebra nicht durchsichtig zu machen, die Mendels Daten lieferten. Doch Bateson verbesserte das Original bis zur Verständlichkeit, und er kam etwas später – im Jahre 1906 – auf die Idee, für die inzwischen ins Laufen gekommene Wissenschaft von der Vererbung den Namen „Genetik“ vorzuschlagen.\nZu dieser Zeit gab es in London einen Wissenschaftler, dessen Bekanntschaft mit der Vererbungslehre besondere Früchte trug. Er hieß Archibald Garrod, und ihm war bei seiner Tätigkeit als Arzt schon länger aufgefallen, dass es Krankheiten mit Familiengeschichten gab. Farbenblindheit etwa konnte bei Vater und Sohn zugleich festgestellt werden, und Stoffwechselstörungen der Großeltern tauchten oft bei den Enkeln wieder auf. Nach der Durchsicht von Batesons Mendel-Übersetzung wurde Garrod schlagartig klar, dass er die Vererbung von Krankheiten beobachtete, von der man inzwischen wusste, dass sie nach klaren Regeln ablief. Die englische Sprache kennt dafür seitdem den Ausdruck der „Mendelian disease“, und die Wissenschaftler haben sich – heute vor allem in den USA – zum Ziel gesetzt, diese Erbkrankheiten zu verstehen (um sie verhindern oder heilen zu können).\nGarrod war ein guter Wissenschaftler, was heißt, dass er sehr sorgfältig beobachtete und sehr vorsichtige Schlüsse zog. So bemerkte er, dass er genau genommen nicht die Vererbung einer Krankheit selbst, sondern nur die Vererbung einer Anlage für diese Krankheit verfolgen konnte. Die auffälligste Anlage war dabei die Anfälligkeit für Infektionskrankheiten wie Schnupfen, Grippe und Lungenentzündung. Er wusste (nicht nur als Arzt), dass Menschen dabei höchst individuelle Unterschiede zeigten, und er fragte sich, ob auch diese Einzigartigkeit den Mendelschen Regeln unterliegt und somit vererbt wird. Als die Antwort „ja!“ lautete, sah Garrod auf einmal eine Chance und eine Aufgabe für die Erforschung der Vererbung: Sie sollte versuchen, die „chemische Individualität“ des Menschen zu erfassen, um mit dieser Kenntnis vorhersagen zu können, wer zum Beispiel von einer Infektion betroffen wird oder wer unter Nebenwirkungen von Arzneimitteln zu leiden hat, die bekanntlich ebenfalls von Mensch zu Mensch verschieden in Erscheinung treten. Die organische Individualität eines Menschen muss in seinen Genen stecken, so vermutete Garrod, und er hoffte am Anfang des vorigen Jahrhunderts, dass den Ärzten die entsprechenden Informationen eines Tages zum Nutzen ihrer Patienten zur Verfügung stehen würden. Dieses Ziel ist heute greifbar nah, denn genau da hat die Genetik hingeführt.\nGarrod fasste seine Einsichten 1908 zusammen, ein Jahr, bevor die „Elemente“ der Vererbung den Namen bekamen, den sie bis heute tragen, nämlich „Gene“. Der Schöpfer des Begriffs „Gen“, der Däne Wilhelm Johannsen, legte dabei Wert auf ein kurzes Wort, und zwar aus zwei Gründen. Es sollte zum einen leicht kombinierbar sein, und es sollte zum zweiten erlauben, in einfacher Weise von „Genen für bestimmte Eigenschaften“ zu sprechen, etwa von „Genen für blaue Augen“.\nWährend sich die erste Idee bewährt hat, zeigt die zweite vor allem Nachteile. Sie macht es nämlich viel zu leicht, Genen etwas in die Schuhe zu schieben, mit dem sie direkt wenig zu tun haben. Mit der sprachlichen Vorgabe hat sich ein inflationärer Gebrauch des Wortes eingebürgert, der von „Genen für Krebs“ bis zu „Genen für Untreue“ reicht und keinesfalls die „Gene für Intelligenz“ auslässt. So spannend dies auch klingen mag, wer so spricht, fällt hinter Mendel zurück. Denn so unklar die Beschreibung seiner Versuche auch bleibt, so klar war ihm doch, dass es nicht die Eigenschaften eines Lebewesens – wie etwa seine Augenfarbe – sind, die durch „Erbelemente“ bestimmt werden. Was festgelegt wird, sind vielmehr die Unterschiede von Eigenschaften, und zwar durch Unterschiede in Genen. Genau hier steckt auch die Individualität, nach der Garrod fragt.",
      "id": [
        "145828"
      ],
      "item": "Eine Wissenschaft kommt nur schwer auf die Beine"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145829]\n[%Gregor Mendel]\nGregor Mendel (1822 – 1864) ist als Entdecker der Mendelschen Gesetze der Verderbung so etwas wie der Vater der Genetik.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Gregor Mendel (1822 – 1864) ist als Entdecker der Mendelschen Gesetze der Verderbung so etwas wie der Vater der Genetik.",
      "id": [
        "145829"
      ],
      "item": "Gregor Mendel"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145830]\n[%Genetische Karten für die Liebhaberin des Taus]\nAls das Gen dem Sprachschatz der Wissenschaft hinzugefügt wurde, gelang dem Amerikaner Thomas H. Morgan ein historischer Glücksgriff, indem er die kleine Fliege mit dem heute berühmten Namen __Drosophila melanogaster__ als Gegenstand seiner Untersuchung wählte. Er und seine Mitarbeiter waren mit dieser „Liebhaberin des Taus“ auf das ideale Versuchstier gestoßen. Es konnte leicht in großen Mengen im Laboratorium gehalten werden, zeigte große Vielfalt der äußeren Erscheinung und brachte neue Generationen in kurzer Folge hervor. Die Fliege __Drosophila__ wurde in den kommenden Jahren zum bevorzugten Objekt der Genetiker, und Morgan konnte mit ihrer Hilfe in den 1920er Jahren eine erste Theorie der Gene aufstellen – wobei dies allerdings das letzte war, was er anstrebte. Morgan wollte ursprünglich das genaue Gegenteil, nämlich zeigen, dass es so etwas wie Erbelemente oder Gene gar nicht gibt. Als Embryologe mit großer Kenntnis von der Formenvielfalt, die das werdende Leben zeigt, konnte er sich beim besten Willen nicht vorstellen, dass es irgendwelche winzigen Partikelchen in den Zellen geben sollte, die ausreichend differenziert und informativ waren, um dafür verantwortlich sein zu können. Morgan machte sich also mit __Drosophila__ an die Arbeit, um dem Unsinn der stofflichen Gene ein Ende zu bereiten – doch nur um durch die Resultate seiner Experimente bekehrt und ein überzeugter Anhänger der Mendelschen Genetik zu werden.\nDie Experimente, die Morgan sein Damaskus-Erlebnis bereiteten, sind als Kreuzungen bekannt, was konkret bedeutet, dass im Laboratorium zwei Fliegen mit ausgewählten Variationen Gelegenheit bekommen, Nachkommen zu produzieren, und zwar möglichst viele. Die Forscher zählen nun, welche Eigenschaften von Vater und Mutter wie oft und in welcher Kombination auf die Söhne und Töchter übertragen werden, wie oft die hier zusammengeführten Qualitäten sich in den nächsten Generation wieder trennen, und so weiter und so fort. Ein unglaubliche Fleißarbeit, die unendlich langweilig gewesen sein muss und für die sorgfältigste Buchführung erforderlich war, die zuletzt aber ungeheuer spannende Einsichten ermöglicht hat, und zwar in Verbindung mit anderen Beobachtungen, die das Innere der Zellen betrafen.\nParallel zu dem Abzählen hatten die Fliegenforscher nämlich bei ihren vielfach unterschiedenen Fliegen noch das Aussehen der länglichen Zellstrukturen notiert, die Chromosomen heißen. Diese „farbigen Körper“ waren den Biologen seit dem 19. Jahrhundert bekannt, weil sie sich in Zellen leicht mit Hilfe eines Lichtmikroskops erkennen lassen. Der Vergleich beider Beobachtungsreihen lieferte nach vielen mühevollen Jahren eine eindeutige Antwort auf die Frage, wo denn die Gene in einem Lebewesen stecken. Es waren genau die Chromosomen, und zum Entzücken der Fliegenforscher wurde diese Ortung noch mit einem besonderen Sahnehäubchen gekrönt. Es bestand in der Erkenntnis, dass die Gene nicht willkürlich verteilt waren und sich kreuz und quer verteilten. Vielmehr bildeten sie eine Art Perlenkette, das heißt, die Gene waren auf einem Chromosom ordentlich angeordnet, und stets folgte eins nach dem anderen.\nDiese Gradlinigkeit reizte natürlich die Genetiker, die nun alle Kraft daran setzten, die genaue Reihenfolge der Gene herauszubekommen, die sie als genetische Karte bezeichneten. Seit 1915 mühten sie sich damit ab, wobei sie vor allem mit Hilfe der kleinen Fliege __Drosophila__ zum Erfolg kamen. Immer wieder galt es, die vielen genetisch bedingten Eigenschaften – wie Augenfarbe, Beinlänge oder Körpergröße – zu verfolgen und zu notieren, wie sich diese beobachtbaren Qualitäten – und damit deren Gene – in nachfolgenden Generationen auftrennen oder zusammenfinden. Dabei ließen sich nach langen einsamen Monaten im Labor Häufigkeiten für alle möglichen Genkombinationen angeben, die ihrerseits Rückschlüsse auf die Reihenfolge der verantwortlichen Gene erlaubten. Der Konstruktion einer genetischen Karte stand dann nichts mehr im Wege.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Als das Gen dem Sprachschatz der Wissenschaft hinzugefügt wurde, gelang dem Amerikaner Thomas H. Morgan ein historischer Glücksgriff, indem er die kleine Fliege mit dem heute berühmten Namen __Drosophila melanogaster__ als Gegenstand seiner Untersuchung wählte. Er und seine Mitarbeiter waren mit dieser „Liebhaberin des Taus“ auf das ideale Versuchstier gestoßen. Es konnte leicht in großen Mengen im Laboratorium gehalten werden, zeigte große Vielfalt der äußeren Erscheinung und brachte neue Generationen in kurzer Folge hervor. Die Fliege __Drosophila__ wurde in den kommenden Jahren zum bevorzugten Objekt der Genetiker, und Morgan konnte mit ihrer Hilfe in den 1920er Jahren eine erste Theorie der Gene aufstellen – wobei dies allerdings das letzte war, was er anstrebte. Morgan wollte ursprünglich das genaue Gegenteil, nämlich zeigen, dass es so etwas wie Erbelemente oder Gene gar nicht gibt. Als Embryologe mit großer Kenntnis von der Formenvielfalt, die das werdende Leben zeigt, konnte er sich beim besten Willen nicht vorstellen, dass es irgendwelche winzigen Partikelchen in den Zellen geben sollte, die ausreichend differenziert und informativ waren, um dafür verantwortlich sein zu können. Morgan machte sich also mit __Drosophila__ an die Arbeit, um dem Unsinn der stofflichen Gene ein Ende zu bereiten – doch nur um durch die Resultate seiner Experimente bekehrt und ein überzeugter Anhänger der Mendelschen Genetik zu werden.\nDie Experimente, die Morgan sein Damaskus-Erlebnis bereiteten, sind als Kreuzungen bekannt, was konkret bedeutet, dass im Laboratorium zwei Fliegen mit ausgewählten Variationen Gelegenheit bekommen, Nachkommen zu produzieren, und zwar möglichst viele. Die Forscher zählen nun, welche Eigenschaften von Vater und Mutter wie oft und in welcher Kombination auf die Söhne und Töchter übertragen werden, wie oft die hier zusammengeführten Qualitäten sich in den nächsten Generation wieder trennen, und so weiter und so fort. Ein unglaubliche Fleißarbeit, die unendlich langweilig gewesen sein muss und für die sorgfältigste Buchführung erforderlich war, die zuletzt aber ungeheuer spannende Einsichten ermöglicht hat, und zwar in Verbindung mit anderen Beobachtungen, die das Innere der Zellen betrafen.\nParallel zu dem Abzählen hatten die Fliegenforscher nämlich bei ihren vielfach unterschiedenen Fliegen noch das Aussehen der länglichen Zellstrukturen notiert, die Chromosomen heißen. Diese „farbigen Körper“ waren den Biologen seit dem 19. Jahrhundert bekannt, weil sie sich in Zellen leicht mit Hilfe eines Lichtmikroskops erkennen lassen. Der Vergleich beider Beobachtungsreihen lieferte nach vielen mühevollen Jahren eine eindeutige Antwort auf die Frage, wo denn die Gene in einem Lebewesen stecken. Es waren genau die Chromosomen, und zum Entzücken der Fliegenforscher wurde diese Ortung noch mit einem besonderen Sahnehäubchen gekrönt. Es bestand in der Erkenntnis, dass die Gene nicht willkürlich verteilt waren und sich kreuz und quer verteilten. Vielmehr bildeten sie eine Art Perlenkette, das heißt, die Gene waren auf einem Chromosom ordentlich angeordnet, und stets folgte eins nach dem anderen.\nDiese Gradlinigkeit reizte natürlich die Genetiker, die nun alle Kraft daran setzten, die genaue Reihenfolge der Gene herauszubekommen, die sie als genetische Karte bezeichneten. Seit 1915 mühten sie sich damit ab, wobei sie vor allem mit Hilfe der kleinen Fliege __Drosophila__ zum Erfolg kamen. Immer wieder galt es, die vielen genetisch bedingten Eigenschaften – wie Augenfarbe, Beinlänge oder Körpergröße – zu verfolgen und zu notieren, wie sich diese beobachtbaren Qualitäten – und damit deren Gene – in nachfolgenden Generationen auftrennen oder zusammenfinden. Dabei ließen sich nach langen einsamen Monaten im Labor Häufigkeiten für alle möglichen Genkombinationen angeben, die ihrerseits Rückschlüsse auf die Reihenfolge der verantwortlichen Gene erlaubten. Der Konstruktion einer genetischen Karte stand dann nichts mehr im Wege.",
      "id": [
        "145830"
      ],
      "item": "Genetische Karten für die Liebhaberin des Taus"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145831]\n[%Die Natur der Gene]\nWer sich auf ein unbekanntes Gebiet wie das Innere eine Zelle begibt, hat großes Interesse an guten (genetischen) Karten. Entsprechend stand deren Anfertigung im Mittelpunkt vieler genetischer Arbeiten der zwanziger und dreißiger Jahre. So lernte man bald sehr genau viele Orte („Loci“) für zahlreiche Gene in einer Zelle kennen, doch eins blieb den Forschern bei diesem Ansatz verborgen – nämlich die Natur der Erbanlagen selbst. Man wusste zwar, wo die Gene lagen. Man wusste aber nicht, woraus sie bestanden. \nDer Natur der Gene kamen die Wissenschaftler auf die Spur, nachdem einem von ihnen im Jahre 1927 aufgefallen war, dass sich Gene verändern (mutieren) können, wenn sie von Röntgenstrahlen getroffen werden. Gemeint ist Hermann J. Muller, der Genetik bei Thomas Hunt Morgan gelernt hatte und mit dem Nobelpreis ausgezeichnet wurde. So wichtig Mullers Entdeckung für die Geschichte der Genetik war, so seltsam ist das, was er für sich daraus gemacht hat. Muller sah nämlich das menschliche Erbmaterial durch Strahlen bedroht, und er vermutete, dass „gesunde“ und „gute“ Gene nur noch in Genies vorhanden sind, zu denen er Einstein und Lenin zählte. Muller meinte, man müsse versuchen, dem genetischen Erbe (Genpool) der Menschen weitere geschädigte Gene ersparen, und er schlug konkret vor, dass „normale“ Frauen mit dem Samen von genialen Männern – siehe oben – befruchtet werden sollten.\nDer Schluss liegt nahe, dass wissenschaftliches Können nicht unmittelbar zu vernünftigen Handlungsweisen führt, aber unabhängig davon entstand dank Muller die neue Disziplin der Strahlengenetik, und die Gene verwandelten sich in etwas, das mit physikalischen Mitteln getroffen und beeinflusst werden konnte. Die Natur der Gene war damit ein Problem für Physiker geworden, und 1935 wurde vorgeschlagen, sich unter einem Gen einen größeren Verband aus Atomen vorzustellen. Autor dieses Vorschlags war der damals knapp 30jährige Max Delbrück, der den russischen Genetiker Nikolai Timofejew-Ressowski und den deutschen Physiker Karl G. Zimmer in das elterliche Haus in Berlin-Grunewald eingeladen hatte, um hier gemeinsam bei Kaffee und Kuchen das Gen interdisziplinär in den Griff zu bekommen. Die Idee des Atomverbands wurde in der sogenannten Dreimännerarbeit vorgestellt, die das Trio 1935 publizierte. Sie entfaltete ihre historische Wirkung dadurch, dass der berühmte Erwin Schrödinger Delbrücks Idee zehn Jahre später in den Mittelpunkt seiner Vorlesungen zu der Frage „Was ist Leben?“ stellte. Damals kam auch das Wort „Molekularbiologie“ auf, zunächst als Bezeichnung für ein Förderprogramm der Rockefeller Foundation. Ohne das Geld von Rockefeller wäre die Entwicklung der Molekularbiologie wahrscheinlich langsamer vonstatten gegangen – und sicher nicht so schnell beim Menschen angekommen. Dieses Ziel aber wurde in den USA ganz pragmatisch angegangen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Wer sich auf ein unbekanntes Gebiet wie das Innere eine Zelle begibt, hat großes Interesse an guten (genetischen) Karten. Entsprechend stand deren Anfertigung im Mittelpunkt vieler genetischer Arbeiten der zwanziger und dreißiger Jahre. So lernte man bald sehr genau viele Orte („Loci“) für zahlreiche Gene in einer Zelle kennen, doch eins blieb den Forschern bei diesem Ansatz verborgen – nämlich die Natur der Erbanlagen selbst. Man wusste zwar, wo die Gene lagen. Man wusste aber nicht, woraus sie bestanden. \nDer Natur der Gene kamen die Wissenschaftler auf die Spur, nachdem einem von ihnen im Jahre 1927 aufgefallen war, dass sich Gene verändern (mutieren) können, wenn sie von Röntgenstrahlen getroffen werden. Gemeint ist Hermann J. Muller, der Genetik bei Thomas Hunt Morgan gelernt hatte und mit dem Nobelpreis ausgezeichnet wurde. So wichtig Mullers Entdeckung für die Geschichte der Genetik war, so seltsam ist das, was er für sich daraus gemacht hat. Muller sah nämlich das menschliche Erbmaterial durch Strahlen bedroht, und er vermutete, dass „gesunde“ und „gute“ Gene nur noch in Genies vorhanden sind, zu denen er Einstein und Lenin zählte. Muller meinte, man müsse versuchen, dem genetischen Erbe (Genpool) der Menschen weitere geschädigte Gene ersparen, und er schlug konkret vor, dass „normale“ Frauen mit dem Samen von genialen Männern – siehe oben – befruchtet werden sollten.\nDer Schluss liegt nahe, dass wissenschaftliches Können nicht unmittelbar zu vernünftigen Handlungsweisen führt, aber unabhängig davon entstand dank Muller die neue Disziplin der Strahlengenetik, und die Gene verwandelten sich in etwas, das mit physikalischen Mitteln getroffen und beeinflusst werden konnte. Die Natur der Gene war damit ein Problem für Physiker geworden, und 1935 wurde vorgeschlagen, sich unter einem Gen einen größeren Verband aus Atomen vorzustellen. Autor dieses Vorschlags war der damals knapp 30jährige Max Delbrück, der den russischen Genetiker Nikolai Timofejew-Ressowski und den deutschen Physiker Karl G. Zimmer in das elterliche Haus in Berlin-Grunewald eingeladen hatte, um hier gemeinsam bei Kaffee und Kuchen das Gen interdisziplinär in den Griff zu bekommen. Die Idee des Atomverbands wurde in der sogenannten Dreimännerarbeit vorgestellt, die das Trio 1935 publizierte. Sie entfaltete ihre historische Wirkung dadurch, dass der berühmte Erwin Schrödinger Delbrücks Idee zehn Jahre später in den Mittelpunkt seiner Vorlesungen zu der Frage „Was ist Leben?“ stellte. Damals kam auch das Wort „Molekularbiologie“ auf, zunächst als Bezeichnung für ein Förderprogramm der Rockefeller Foundation. Ohne das Geld von Rockefeller wäre die Entwicklung der Molekularbiologie wahrscheinlich langsamer vonstatten gegangen – und sicher nicht so schnell beim Menschen angekommen. Dieses Ziel aber wurde in den USA ganz pragmatisch angegangen.",
      "id": [
        "145831"
      ],
      "item": "Die Natur der Gene"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145832]\n[%Max Delbrück]\nMax Delbrück, geboren 1906 in Berlin, arbeitete zunächst in Göttingen als Quantenphysiker, bis er sich auf Anregung von Niels Bohr der Biologie zuwandte. Er emigrierte 1937 in die USA, wo er als Molekularbiologe arbeitete. Er starb 1981 in Kalifornien.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Max Delbrück, geboren 1906 in Berlin, arbeitete zunächst in Göttingen als Quantenphysiker, bis er sich auf Anregung von Niels Bohr der Biologie zuwandte. Er emigrierte 1937 in die USA, wo er als Molekularbiologe arbeitete. Er starb 1981 in Kalifornien.",
      "id": [
        "145832"
      ],
      "item": "Max Delbrück"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145833]\n[%Der Anschluss der Genetik an die exakten Wissenschaften]\nZu den von der Rockefeller Foundation geförderten Wissenschaftlern gehörte Max Delbrück, dem es mit dem „Atomverband“ gelungen war, die bis dahin eigenständig operierende Genetik an eine der etablierten Wissenschaften anzuschließen – an die Physik nämlich. Die Genetik gehörte nun mit zur Familie der exakten Wissenschaften, und es war zu erwarten, dass es nicht mehr lange dauern konnte, bis sich andere Familienmitglieder zu Wort melden würden. Mit besonderem Nachdruck trat bald die Chemie auf den Plan, die sich der Frage annahm, welche Atome da in welchem Verband zusammenkommen mussten, um ein biologisch wirkendes Genmolekül zu ergeben. Und die erste Antwort gab es noch in den Jahren des Zweites Weltkriegs, wobei sie aus einer Ecke kam, in die bis dahin niemand geschaut hatte. Gemeint ist die medizinische Abteilung der Rockefeller Universität in New York, in der man sich über zwei Formen eines Bakteriums wunderte, das Lungenentzündungen hervorrufen konnte. Lebensbedrohlich wirkte nur einer der Stämme, und der bald 70jährige Oswald Avery wollte von seinem Team 1944 so genau wie möglich wissen, was den Unterschied zwischen beiden ausmachte. Gemeinschaftlich entzogen sie den Bakterien einen Stoff, mit dem sich die ungefährlichen in die gefährliche Variante verwandeln ließen. Dieser Träger des „Transformationsprinzips“, wie es hieß, wurde von den Bakterien sogar vererbt, und Avery kam zu der Einsicht, dass er wissen würde, woraus Gene bestehen, wenn er herausfinden könnte, was die eher harmlosen Bakterien umformt. \nDas Ergebnis der Analyse zeigte, dass man es chemisch gesehen mit Säuren zu tun hatte. Genauer gesagt – Gene lagen konkret als Moleküle aus Desoxyribonukleinsäure (DNS, englisch DNA) vor. Die DNA ist das, was Chemiker ein Makromolekül nennen, und seine Größe kommt durch viele Untereinheiten zustande, von denen die wichtigsten als Basen bekannt sind. Hiervon verwendet eine Zelle vier Stück, wobei es reicht, die Anfangsbuchstaben ihrer vollständigen chemischen Namen zu kennen. Sie lassen sich als Quartett A, T, G und C schreiben, was in der Kombination ATGC fast so wie ein ABC des Lebens klingt.\nDer Stoff mit Namen DNA ist längst weltberühmt, und zwar deshalb, weil er zugleich einfach und elegant gebaut ist, wie heute jeder weiß (oder wissen sollte). Die DNA besitzt eine längliche Form – was ihr den wohlklingenden Namen „Faden des Lebens“ eingetragen hat –, wobei sich dem Betrachter das eigentliche Wunder zeigt, wenn er näher herangeht und die Gestalt im molekularen Detail ins Auge fasst. Er wird dann staunend sehen, wie elegant sie als Doppelhelix konstruiert ist.\nDiese Doppelhelix, die längst als Ikone unserer Zeit verstanden wird, ist zum ersten Mal 1953 beschrieben worden, und zwar durch das legendäre Duo bestehend aus dem Amerikaner James Watson und dem Briten Francis Crick. Ihre gemeinsame Veröffentlichung im März 1953 ist dabei nicht nur wegen des herrlichen Ergebnisses in Form der DNA berühmt, sondern auch wegen eines Satzes, in dem die Autoren gegen Ende ihrer Arbeit lapidar mitteilen, dass etwas ihrer Aufmerksamkeit nicht entgangen sei, nämlich die Tatsache, dass die vorgelegte Doppelhelix unmittelbar einsichtig macht, wie das Leben eine seiner elementaren Aufgaben löst. Sie meinten die Verdopplung des Erbmaterials. Denn wie es aussah, braucht man nur die zwei Hälften zu teilen und anschließend die beiden dabei entstehenden Einzelstränge zu ergänzen und zu jeweils wieder einer Doppelhelix zu machen. Aus einem Erbmolekül hatte die Zelle damit zwei gemacht und somit die elementare Aufgabe allen Lebens – die Vermehrung – bewerkstelligt.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Zu den von der Rockefeller Foundation geförderten Wissenschaftlern gehörte Max Delbrück, dem es mit dem „Atomverband“ gelungen war, die bis dahin eigenständig operierende Genetik an eine der etablierten Wissenschaften anzuschließen – an die Physik nämlich. Die Genetik gehörte nun mit zur Familie der exakten Wissenschaften, und es war zu erwarten, dass es nicht mehr lange dauern konnte, bis sich andere Familienmitglieder zu Wort melden würden. Mit besonderem Nachdruck trat bald die Chemie auf den Plan, die sich der Frage annahm, welche Atome da in welchem Verband zusammenkommen mussten, um ein biologisch wirkendes Genmolekül zu ergeben. Und die erste Antwort gab es noch in den Jahren des Zweites Weltkriegs, wobei sie aus einer Ecke kam, in die bis dahin niemand geschaut hatte. Gemeint ist die medizinische Abteilung der Rockefeller Universität in New York, in der man sich über zwei Formen eines Bakteriums wunderte, das Lungenentzündungen hervorrufen konnte. Lebensbedrohlich wirkte nur einer der Stämme, und der bald 70jährige Oswald Avery wollte von seinem Team 1944 so genau wie möglich wissen, was den Unterschied zwischen beiden ausmachte. Gemeinschaftlich entzogen sie den Bakterien einen Stoff, mit dem sich die ungefährlichen in die gefährliche Variante verwandeln ließen. Dieser Träger des „Transformationsprinzips“, wie es hieß, wurde von den Bakterien sogar vererbt, und Avery kam zu der Einsicht, dass er wissen würde, woraus Gene bestehen, wenn er herausfinden könnte, was die eher harmlosen Bakterien umformt. \nDas Ergebnis der Analyse zeigte, dass man es chemisch gesehen mit Säuren zu tun hatte. Genauer gesagt – Gene lagen konkret als Moleküle aus Desoxyribonukleinsäure (DNS, englisch DNA) vor. Die DNA ist das, was Chemiker ein Makromolekül nennen, und seine Größe kommt durch viele Untereinheiten zustande, von denen die wichtigsten als Basen bekannt sind. Hiervon verwendet eine Zelle vier Stück, wobei es reicht, die Anfangsbuchstaben ihrer vollständigen chemischen Namen zu kennen. Sie lassen sich als Quartett A, T, G und C schreiben, was in der Kombination ATGC fast so wie ein ABC des Lebens klingt.\nDer Stoff mit Namen DNA ist längst weltberühmt, und zwar deshalb, weil er zugleich einfach und elegant gebaut ist, wie heute jeder weiß (oder wissen sollte). Die DNA besitzt eine längliche Form – was ihr den wohlklingenden Namen „Faden des Lebens“ eingetragen hat –, wobei sich dem Betrachter das eigentliche Wunder zeigt, wenn er näher herangeht und die Gestalt im molekularen Detail ins Auge fasst. Er wird dann staunend sehen, wie elegant sie als Doppelhelix konstruiert ist.\nDiese Doppelhelix, die längst als Ikone unserer Zeit verstanden wird, ist zum ersten Mal 1953 beschrieben worden, und zwar durch das legendäre Duo bestehend aus dem Amerikaner James Watson und dem Briten Francis Crick. Ihre gemeinsame Veröffentlichung im März 1953 ist dabei nicht nur wegen des herrlichen Ergebnisses in Form der DNA berühmt, sondern auch wegen eines Satzes, in dem die Autoren gegen Ende ihrer Arbeit lapidar mitteilen, dass etwas ihrer Aufmerksamkeit nicht entgangen sei, nämlich die Tatsache, dass die vorgelegte Doppelhelix unmittelbar einsichtig macht, wie das Leben eine seiner elementaren Aufgaben löst. Sie meinten die Verdopplung des Erbmaterials. Denn wie es aussah, braucht man nur die zwei Hälften zu teilen und anschließend die beiden dabei entstehenden Einzelstränge zu ergänzen und zu jeweils wieder einer Doppelhelix zu machen. Aus einem Erbmolekül hatte die Zelle damit zwei gemacht und somit die elementare Aufgabe allen Lebens – die Vermehrung – bewerkstelligt.",
      "id": [
        "145833"
      ],
      "item": "Der Anschluss der Genetik an die exakten Wissenschaften"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145834]\n[%James D. Watson und Francis Crick]\n1951 lernte der junge Amerikaner James D. Watson (geb. 1928) in Cambridge den Engländer Francis Crick (1916-2004) kennen; von da an arbeiteten sie eng zusammen, um die Struktur der DNA zu entschlüsseln.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "1951 lernte der junge Amerikaner James D. Watson (geb. 1928) in Cambridge den Engländer Francis Crick (1916-2004) kennen; von da an arbeiteten sie eng zusammen, um die Struktur der DNA zu entschlüsseln.",
      "id": [
        "145834"
      ],
      "item": "James D. Watson und Francis Crick"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145835]\n[%Der Trick von Watson und Crick]\nSo leicht die Struktur der DNA als Doppelhelix (bestehend aus den vier Basen A, T, G und C) auf die Funktion dieses Moleküls schließen ließ, so schwer war es gewesen, den Weg zur Doppelhelix zu finden. Anfang der fünfziger Jahre waren viele Wissenschaftler mit der DNA und ähnlichen Substanzen aus der Zelle beschäftigt, und zwar vor allem Kristallographen auf der einen und Chemiker auf der anderen Seite. Die erste Gruppe bemühte sich, Kristalle aus DNA zu züchten, um dann mit Hilfe von Röntgenstrahlen deren Aufbau zu erkunden. Und die zweite Gruppe versuchte, etwas über die Anordnung der Bausteine zu erfahren, die in der DNA steckten.\nBei diesen Bemühungen war der 1905 geborene Erwin Chargaff am weitesten gekommen. Seine Analysen zeigten, dass von den vier Basen (A, T, G und C) jeweils zwei in gleichen Mengen vorlagen. Chargaff wusste also, dass es so viel A wie T und so viel G wie C gab, und an dieser Stelle steht dann in der Zeitung, dass er die Basenpaarung entdeckt habe. Doch genau dies hat er übersehen. Er musste sich vielmehr von James D. Watson und Francis Crick darüber aufklären lassen, was die ermittelten Zahlenverhältnisse bedeuteten. Erst sie haben verstanden, was Chargaff gemessen hatte, der selbst in seinem chemischen Denken stecken geblieben ist. Chargaffs Befangenheit in seiner Disziplin hinderte ihn daran, über den Zaun zu blicken, zum Beispiel dorthin, wo die Kristallographen arbeiteten. Watson und Crick agierten völlig anders. Sie machten nichts anderes, als über den Zaun zu blicken, bis sie hatten, was alle suchten, nämlich ein Strukturmodell der Gene.\nZu verlieren hatten Watson und Crick fast nichts – höchstens den Respekt etablierter Männer der Wissenschaft wie Chargaff. Es hat sie kaum gestört und sie bestenfalls nach ihrem Erfolg noch schneller und stolzer in die Kneipe „Eagle“ rennen lassen, die ihrem Laboratorium in Cambridge gegenüber lag, um hier unüberhörbar laut zu verkünden, dass sie das Rätsel des Lebens gelöst hätten.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "So leicht die Struktur der DNA als Doppelhelix (bestehend aus den vier Basen A, T, G und C) auf die Funktion dieses Moleküls schließen ließ, so schwer war es gewesen, den Weg zur Doppelhelix zu finden. Anfang der fünfziger Jahre waren viele Wissenschaftler mit der DNA und ähnlichen Substanzen aus der Zelle beschäftigt, und zwar vor allem Kristallographen auf der einen und Chemiker auf der anderen Seite. Die erste Gruppe bemühte sich, Kristalle aus DNA zu züchten, um dann mit Hilfe von Röntgenstrahlen deren Aufbau zu erkunden. Und die zweite Gruppe versuchte, etwas über die Anordnung der Bausteine zu erfahren, die in der DNA steckten.\nBei diesen Bemühungen war der 1905 geborene Erwin Chargaff am weitesten gekommen. Seine Analysen zeigten, dass von den vier Basen (A, T, G und C) jeweils zwei in gleichen Mengen vorlagen. Chargaff wusste also, dass es so viel A wie T und so viel G wie C gab, und an dieser Stelle steht dann in der Zeitung, dass er die Basenpaarung entdeckt habe. Doch genau dies hat er übersehen. Er musste sich vielmehr von James D. Watson und Francis Crick darüber aufklären lassen, was die ermittelten Zahlenverhältnisse bedeuteten. Erst sie haben verstanden, was Chargaff gemessen hatte, der selbst in seinem chemischen Denken stecken geblieben ist. Chargaffs Befangenheit in seiner Disziplin hinderte ihn daran, über den Zaun zu blicken, zum Beispiel dorthin, wo die Kristallographen arbeiteten. Watson und Crick agierten völlig anders. Sie machten nichts anderes, als über den Zaun zu blicken, bis sie hatten, was alle suchten, nämlich ein Strukturmodell der Gene.\nZu verlieren hatten Watson und Crick fast nichts – höchstens den Respekt etablierter Männer der Wissenschaft wie Chargaff. Es hat sie kaum gestört und sie bestenfalls nach ihrem Erfolg noch schneller und stolzer in die Kneipe „Eagle“ rennen lassen, die ihrem Laboratorium in Cambridge gegenüber lag, um hier unüberhörbar laut zu verkünden, dass sie das Rätsel des Lebens gelöst hätten.",
      "id": [
        "145835"
      ],
      "item": "Der Trick von Watson und Crick"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145836]\n[%Die Soft- und die Hardware des Lebens]\nDie Lösung des Rätsels des Lebens, das James D. Watson und Francis Crick in der Entdeckung eines Strukturmodells der Gene sahen, ist natürlich bis heute nicht gefunden. Aber ein Triumph war es doch, den es im März 1953 mit Watson und Crick zu feiern gab. Danach, im Verlauf der fünfziger und sechziger Jahre entstand eine ungeheuer dynamische Molekularbiologie, der kein Rätsel der Vererbung verschlossen zu bleiben schien. Dieser Eindruck machte sich jedenfalls rasch und weltweit breit, als zum Beispiel verstanden wurde, wie die Gene wirken und wie ihre Rolle bei Bedarf reguliert und kontrolliert wird. Die DNA wurde als Software des Lebens erkannt, in der biologische Informationen steckten, und zwar die Anleitungen, die eine Zelle benötigt, um die besondere Hardware anzufertigen, die sie braucht, um sich im Leben behaupten zu können. Einen wesentlichen Teil dieser Hardware stellen einige besonders raffiniert gebaute Moleküle dar, die Proteine heißen und unter anderem für den Stoffwechsel zuständig sind.\nAnders ausgedrückt: Man wusste in den sechziger Jahren, dass Gene Proteine herstellen und wie sie dies tun, und man hatte darüber hinaus verstanden, was passiert, wenn ein Gen verändert ist oder nicht funktioniert. In diesem Fall fehlt der Zelle bzw. dem sie tragenden Organismus einfach das entsprechende Protein, und beide werden krank. Mit dieser Einsicht rückte Archibald Garrods Frage, ob auch die Anlage der Anfälligkeit für Infektionskrankheiten nach den Mendelschen Regeln vererbbar ist, wieder in Erinnerung. Man hatte nämlich inzwischen gelernt, dass die von ihm beobachteten Stoffwechselstörungen ihren Ursprung in unzulänglich funktionierenden Proteinen haben, und von denen wusste man nun, dass sie ihrerseits durch ungeeignete Gene bedingt werden. Es war zudem verstanden worden, dass auch die Frage, ob jemand anfällig für Infektionen oder empfindlich für Nebenwirkungen von Medikamenten ist, mit Hilfe dieser molekularen Wunderwerke der Natur zu beantworten ist, die unter dem wenig spektakulären Namen „Protein“ leider viel zu wenig Popularität genießen. Die Proteine entfalten auf wunderbare Weise zur zellulären Wirklichkeit, was in den Genen als Möglichkeit angelegt ist. Wer sagt, dass die Zellen oder gar das Leben durch die Wissenschaft entzaubert worden seien, hat sich noch nie auf die Bekanntschaft mit einem Protein eingelassen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Die Lösung des Rätsels des Lebens, das James D. Watson und Francis Crick in der Entdeckung eines Strukturmodells der Gene sahen, ist natürlich bis heute nicht gefunden. Aber ein Triumph war es doch, den es im März 1953 mit Watson und Crick zu feiern gab. Danach, im Verlauf der fünfziger und sechziger Jahre entstand eine ungeheuer dynamische Molekularbiologie, der kein Rätsel der Vererbung verschlossen zu bleiben schien. Dieser Eindruck machte sich jedenfalls rasch und weltweit breit, als zum Beispiel verstanden wurde, wie die Gene wirken und wie ihre Rolle bei Bedarf reguliert und kontrolliert wird. Die DNA wurde als Software des Lebens erkannt, in der biologische Informationen steckten, und zwar die Anleitungen, die eine Zelle benötigt, um die besondere Hardware anzufertigen, die sie braucht, um sich im Leben behaupten zu können. Einen wesentlichen Teil dieser Hardware stellen einige besonders raffiniert gebaute Moleküle dar, die Proteine heißen und unter anderem für den Stoffwechsel zuständig sind.\nAnders ausgedrückt: Man wusste in den sechziger Jahren, dass Gene Proteine herstellen und wie sie dies tun, und man hatte darüber hinaus verstanden, was passiert, wenn ein Gen verändert ist oder nicht funktioniert. In diesem Fall fehlt der Zelle bzw. dem sie tragenden Organismus einfach das entsprechende Protein, und beide werden krank. Mit dieser Einsicht rückte Archibald Garrods Frage, ob auch die Anlage der Anfälligkeit für Infektionskrankheiten nach den Mendelschen Regeln vererbbar ist, wieder in Erinnerung. Man hatte nämlich inzwischen gelernt, dass die von ihm beobachteten Stoffwechselstörungen ihren Ursprung in unzulänglich funktionierenden Proteinen haben, und von denen wusste man nun, dass sie ihrerseits durch ungeeignete Gene bedingt werden. Es war zudem verstanden worden, dass auch die Frage, ob jemand anfällig für Infektionen oder empfindlich für Nebenwirkungen von Medikamenten ist, mit Hilfe dieser molekularen Wunderwerke der Natur zu beantworten ist, die unter dem wenig spektakulären Namen „Protein“ leider viel zu wenig Popularität genießen. Die Proteine entfalten auf wunderbare Weise zur zellulären Wirklichkeit, was in den Genen als Möglichkeit angelegt ist. Wer sagt, dass die Zellen oder gar das Leben durch die Wissenschaft entzaubert worden seien, hat sich noch nie auf die Bekanntschaft mit einem Protein eingelassen.",
      "id": [
        "145836"
      ],
      "item": "Die Soft- und die Hardware des Lebens"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145837]\n[%Die kurze, aber deutliche Ernüchterung]\nMit der erkannten Verbindung zwischen Genen und Proteinen war Archibald Garrods Problem der Vererbung der Anfälligkeit für Infektionskrankheiten zwar wissenschaftlich genauer erfasst worden, doch eine Lösung war nirgendwo in Sicht. Denn so viel man auch über die Funktionen der Gene sagen konnte, so wenig wusste man über ihren speziellen Aufbau. Natürlich kannten die Genetiker ihre allgemeine Form – nämlich die Doppelhelix –, und sie hatten dabei zugleich herausgefunden, dass sich im Inneren dieser Struktur vier chemische Bausteine abwechselten. Aber die Fragen, welche der vier in einem konkreten Abschnitt vorlagen und in welcher Reihenfolge sie angeordnet waren, konnte in den sechziger Jahren nicht einmal im Ansatz geklärt werden. Dabei wäre es gerade diese Kenntnis gewesen, die für die Forscher den größten Reiz gehabt hätte. Schließlich war zu vermuten, dass die Reihenfolge (Sequenz) der vier Bausteine die biologische Information enthielt, mit der sich die Zelle an den Bau der Proteine machte. Dieser Zusammenhang war durch eine wissenschaftliche Leistung sichtbar geworden, die parallel zur Entdeckung der Doppelhelix zur gleichen Zeit am gleichen Ort gelungen war, und zwar dem britischen Biochemiker Fred Sanger. Sanger hatte in jahrelanger und geduldig durchgeführter Arbeit herausgefunden, wie Insulin aufgebaut ist. Insulin ist seiner Funktion nach ein Hormon und seiner Struktur nach ein Protein. Sangers Arbeit zeigte nun, dass Nukleinsäuren und Proteine nach dem gleichen Bauprinzip konstruiert sind. Beide treten in der Natur als Ketten mit zahlreichen Gliedern in Erscheinung, und diese Übereinstimmung machte unmittelbar klar, dass die Zelle in der Lage sein muss, die Reihenfolge der DNA-Bausteine zu „lesen“ und diese so in die Reihenfolge der Proteinbausteine zu übertragen. Sie tut dies mittels des berühmten und viel zitierten genetischen Codes, der in den frühen sechziger Jahren entschlüsselt werden konnte und den Biologie-Studenten in den modernen Lehrbüchern wie eine Sonne entgegen lacht.\nSo hatte man in den sechziger Jahren verstanden, dass das große Geheimnis der Gene – und die Antwort auf Garrods Frage – in der Sequenz ihrer Bausteine steckte, in ihrer Information. Und doch hatte niemand die geringste Ahnung, wie einer Zelle diese Informationen zu entlocken war. Wie sollte es jemals gelingen, Schritte in diese Richtung zu tun? Wie sollte man ein einzelnes Gen erst aus seinem Chromosom herauslösen, dann in einem Reagenzglas isolieren und anschließend in großen Mengen herstellen? Diese Aufgabe hielten viele Biologen für völlig unlösbar, und als die übrige Welt zum Mond aufbrach, machte sich unter den Molekularbiologen so etwas wie Endzeitstimmung breit. Der Höhepunkt der Genetik schien jedenfalls vorüber zu sein, was zur Folge hatte, dass viele Forscher dieses Gebiet verließen. Und damit machten sie einen großen Fehler.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Mit der erkannten Verbindung zwischen Genen und Proteinen war Archibald Garrods Problem der Vererbung der Anfälligkeit für Infektionskrankheiten zwar wissenschaftlich genauer erfasst worden, doch eine Lösung war nirgendwo in Sicht. Denn so viel man auch über die Funktionen der Gene sagen konnte, so wenig wusste man über ihren speziellen Aufbau. Natürlich kannten die Genetiker ihre allgemeine Form – nämlich die Doppelhelix –, und sie hatten dabei zugleich herausgefunden, dass sich im Inneren dieser Struktur vier chemische Bausteine abwechselten. Aber die Fragen, welche der vier in einem konkreten Abschnitt vorlagen und in welcher Reihenfolge sie angeordnet waren, konnte in den sechziger Jahren nicht einmal im Ansatz geklärt werden. Dabei wäre es gerade diese Kenntnis gewesen, die für die Forscher den größten Reiz gehabt hätte. Schließlich war zu vermuten, dass die Reihenfolge (Sequenz) der vier Bausteine die biologische Information enthielt, mit der sich die Zelle an den Bau der Proteine machte. Dieser Zusammenhang war durch eine wissenschaftliche Leistung sichtbar geworden, die parallel zur Entdeckung der Doppelhelix zur gleichen Zeit am gleichen Ort gelungen war, und zwar dem britischen Biochemiker Fred Sanger. Sanger hatte in jahrelanger und geduldig durchgeführter Arbeit herausgefunden, wie Insulin aufgebaut ist. Insulin ist seiner Funktion nach ein Hormon und seiner Struktur nach ein Protein. Sangers Arbeit zeigte nun, dass Nukleinsäuren und Proteine nach dem gleichen Bauprinzip konstruiert sind. Beide treten in der Natur als Ketten mit zahlreichen Gliedern in Erscheinung, und diese Übereinstimmung machte unmittelbar klar, dass die Zelle in der Lage sein muss, die Reihenfolge der DNA-Bausteine zu „lesen“ und diese so in die Reihenfolge der Proteinbausteine zu übertragen. Sie tut dies mittels des berühmten und viel zitierten genetischen Codes, der in den frühen sechziger Jahren entschlüsselt werden konnte und den Biologie-Studenten in den modernen Lehrbüchern wie eine Sonne entgegen lacht.\nSo hatte man in den sechziger Jahren verstanden, dass das große Geheimnis der Gene – und die Antwort auf Garrods Frage – in der Sequenz ihrer Bausteine steckte, in ihrer Information. Und doch hatte niemand die geringste Ahnung, wie einer Zelle diese Informationen zu entlocken war. Wie sollte es jemals gelingen, Schritte in diese Richtung zu tun? Wie sollte man ein einzelnes Gen erst aus seinem Chromosom herauslösen, dann in einem Reagenzglas isolieren und anschließend in großen Mengen herstellen? Diese Aufgabe hielten viele Biologen für völlig unlösbar, und als die übrige Welt zum Mond aufbrach, machte sich unter den Molekularbiologen so etwas wie Endzeitstimmung breit. Der Höhepunkt der Genetik schien jedenfalls vorüber zu sein, was zur Folge hatte, dass viele Forscher dieses Gebiet verließen. Und damit machten sie einen großen Fehler.",
      "id": [
        "145837"
      ],
      "item": "Die kurze, aber deutliche Ernüchterung"
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145838]\n[%3]\n[#Konsequenzen aus Sequenzen; Die Offenlegung der genetischen Information]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "3",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 1,
      "progress": true,
      "toc": true,
      "id": [
        "145838"
      ],
      "title": "Konsequenzen aus Sequenzen; Die Offenlegung der genetischen Information"
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145839]\n[##Wettlauf Wissenschaftler; Entschlüsselung der Humangenoms]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 2,
      "progress": true,
      "toc": true,
      "id": [
        "145839"
      ],
      "title": "Wettlauf Wissenschaftler; Entschlüsselung der Humangenoms"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145840]\n[%Einleitung]\nWir werden in den folgenden beiden Kapiteln erst die Entwicklung der Genetik als Wissenschaft der Erbinformation nachzeichnen und dann vom Aufkommen der modernen Informationstechnologien erzählen. Wer sich die beiden auf den ersten Blick eher getrennt und unabhängig wirkenden Bereiche des wissenschaftlich-technischen Treibens anschaut, dem fällt spätestens beim zweiten Hinsehen auf, dass sie etwas Gemeinsames hervorgebracht haben, das auf den schönen Namen Chip hört – den Mikrochip bei den Informationstechnologen und den Genchip bei den Genetikern. Natürlich sind Genchips etwas ganz anderes als Mikrochips – die einen dienen der Erkennung von genetischen Informationen, sie bringen Muster hervor, die Diagnosen speziell von Krebs und allgemein von erblich bedingten Krankheiten ermöglichen, während die anderen elektronische Schaltkreise mit zahlreichen Transistoren darstellen, die dicht gedrängt auf einem engsten Stück Raums verwoben (integriert) sind. Aber trotzdem lassen sie einen entscheidenden Zusammenhang erkennen. Er besteht darin, dass die heutige Genetik Informationen sammelt und benötigt, die sie ohne die Hilfe der Computer und ihrer Mikrochips weder bekommen noch verwerten könnte. Genetik handelt inzwischen ganz wesentlich von den Sequenzen des Erbmaterials, das aus DNA besteht. Die spezielle Erbsubstanz einer Zelle oder eines Organismus kann durch die Reihenfolge – die Sequenz – der Basen in der dazugehörigen DNA charakterisiert werden, und in dieser Sequenz steckt das, was wir die biologische oder genetische Information nennen, nach deren Vorgabe sich die Organismen bilden. Die Lebenswissenschaften können diese Sequenzen heute bestimmen, aber sie kennen noch längst nicht alle Konsequenzen, die sich daraus ergeben. Dies hat damit zu tun, dass diese Sequenzen schier endlos lang sind. Die genetische Information in einer menschlichen Zelle besteht aus vielen Milliarden Bausteinen. Wir sind heute in der Lage, diese gigantischen Mengen an biologischen Informationen aus den Zellen zu holen und auf Festplatten in Computern zu schaffen, wo sie der Deutung durch ein Gehirn harren, das sie ohne Hilfe von Computerprogrammen gar nicht lesen oder zur Kenntnis nehmen könnte. In den Datenspeichern, die zugleich größer werden und an Zahl zunehmen, sammeln sich immer mehr sequentielle Informationen, die das Leben in gewisser Weise lesbar machen, und wir wollen in ausgewählten Schritten den Weg nachzeichnen, der uns so weit gebracht hat.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Wir werden in den folgenden beiden Kapiteln erst die Entwicklung der Genetik als Wissenschaft der Erbinformation nachzeichnen und dann vom Aufkommen der modernen Informationstechnologien erzählen. Wer sich die beiden auf den ersten Blick eher getrennt und unabhängig wirkenden Bereiche des wissenschaftlich-technischen Treibens anschaut, dem fällt spätestens beim zweiten Hinsehen auf, dass sie etwas Gemeinsames hervorgebracht haben, das auf den schönen Namen Chip hört – den Mikrochip bei den Informationstechnologen und den Genchip bei den Genetikern. Natürlich sind Genchips etwas ganz anderes als Mikrochips – die einen dienen der Erkennung von genetischen Informationen, sie bringen Muster hervor, die Diagnosen speziell von Krebs und allgemein von erblich bedingten Krankheiten ermöglichen, während die anderen elektronische Schaltkreise mit zahlreichen Transistoren darstellen, die dicht gedrängt auf einem engsten Stück Raums verwoben (integriert) sind. Aber trotzdem lassen sie einen entscheidenden Zusammenhang erkennen. Er besteht darin, dass die heutige Genetik Informationen sammelt und benötigt, die sie ohne die Hilfe der Computer und ihrer Mikrochips weder bekommen noch verwerten könnte. Genetik handelt inzwischen ganz wesentlich von den Sequenzen des Erbmaterials, das aus DNA besteht. Die spezielle Erbsubstanz einer Zelle oder eines Organismus kann durch die Reihenfolge – die Sequenz – der Basen in der dazugehörigen DNA charakterisiert werden, und in dieser Sequenz steckt das, was wir die biologische oder genetische Information nennen, nach deren Vorgabe sich die Organismen bilden. Die Lebenswissenschaften können diese Sequenzen heute bestimmen, aber sie kennen noch längst nicht alle Konsequenzen, die sich daraus ergeben. Dies hat damit zu tun, dass diese Sequenzen schier endlos lang sind. Die genetische Information in einer menschlichen Zelle besteht aus vielen Milliarden Bausteinen. Wir sind heute in der Lage, diese gigantischen Mengen an biologischen Informationen aus den Zellen zu holen und auf Festplatten in Computern zu schaffen, wo sie der Deutung durch ein Gehirn harren, das sie ohne Hilfe von Computerprogrammen gar nicht lesen oder zur Kenntnis nehmen könnte. In den Datenspeichern, die zugleich größer werden und an Zahl zunehmen, sammeln sich immer mehr sequentielle Informationen, die das Leben in gewisser Weise lesbar machen, und wir wollen in ausgewählten Schritten den Weg nachzeichnen, der uns so weit gebracht hat.",
      "id": [
        "145840"
      ],
      "item": "Einleitung"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145841]\n[%Die Korrektur eines Irrtums]\nWer die Geschichte der Genetik oberflächlich betrachtet, kann rasch dem Irrtum verfallen, dass die berühmteste Entdeckung der modernen Wissenschaft – gemeint ist die 1953 publizierte Einsicht in die Struktur des Erbmaterials, die Doppelhelix – eine direkte Konsequenz nach sich gezogen haben sollte, nämlich das Bemühen, die konkrete Reihenfolge (Sequenz) der Basenpaare – die genetische Information – zu bestimmen, wie sie zum Beispiel in einem Bakterium oder in einer Hefezelle vorliegt. Doch davon kann keine Rede sein. So funktioniert Wissenschaft nicht. \nDas ästhetisch befriedigende Modell der DNA musste sich erst einmal in der wissenschaftlichen Praxis bewähren – selbst seine Urheber hatten lange Zeit hindurch die Befürchtung, dass irgendwann die eine oder andere hässliche Tatsache auftauchen könnte, die ihrem schönen Modell (oder wenigstens seinem Universalanspruch) den Garaus machen würde. Außerdem gab es für Biochemiker, die versuchten wollten, die Sequenz eines gegebenen DNA-Moleküls zu bestimmen, zunächst keine Möglichkeit, dieses oder doch wenigstens ein überschaubares DNA-Fragment näher zu untersuchen, etwa unter einem Mikroskop. Die Vertreter der Molekularbiologie mussten rund zwanzig Jahre mit der Doppelhelix vor Augen warten, bis sie das Problem ihrer Sequenz anpacken und technisch lösen konnten. Eine ganz wesentliche Voraussetzung dazu war das unerwartete Erscheinen der Gentechnik, mit deren Hilfe die neue Genetik möglich wurde, die uns heute in Atem hält.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Wer die Geschichte der Genetik oberflächlich betrachtet, kann rasch dem Irrtum verfallen, dass die berühmteste Entdeckung der modernen Wissenschaft – gemeint ist die 1953 publizierte Einsicht in die Struktur des Erbmaterials, die Doppelhelix – eine direkte Konsequenz nach sich gezogen haben sollte, nämlich das Bemühen, die konkrete Reihenfolge (Sequenz) der Basenpaare – die genetische Information – zu bestimmen, wie sie zum Beispiel in einem Bakterium oder in einer Hefezelle vorliegt. Doch davon kann keine Rede sein. So funktioniert Wissenschaft nicht. \nDas ästhetisch befriedigende Modell der DNA musste sich erst einmal in der wissenschaftlichen Praxis bewähren – selbst seine Urheber hatten lange Zeit hindurch die Befürchtung, dass irgendwann die eine oder andere hässliche Tatsache auftauchen könnte, die ihrem schönen Modell (oder wenigstens seinem Universalanspruch) den Garaus machen würde. Außerdem gab es für Biochemiker, die versuchten wollten, die Sequenz eines gegebenen DNA-Moleküls zu bestimmen, zunächst keine Möglichkeit, dieses oder doch wenigstens ein überschaubares DNA-Fragment näher zu untersuchen, etwa unter einem Mikroskop. Die Vertreter der Molekularbiologie mussten rund zwanzig Jahre mit der Doppelhelix vor Augen warten, bis sie das Problem ihrer Sequenz anpacken und technisch lösen konnten. Eine ganz wesentliche Voraussetzung dazu war das unerwartete Erscheinen der Gentechnik, mit deren Hilfe die neue Genetik möglich wurde, die uns heute in Atem hält.",
      "id": [
        "145841"
      ],
      "item": "Die Korrektur eines Irrtums"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145842]\n[%Die Möglichkeit der Gentechnik]\nEin „überschaubare DNA-Fragment“, deren Untersuchung für Biochemiker zunächst nicht möglich war, meint ein Stück DNA, das kurz genug ist, um mit traditionell verfügbaren biochemischen Methoden von einem Genetiker – etwa im Rahmen einer Doktorarbeit – analysiert zu werden. Und „kurz genug“ meint dabei eine Kette mit höchstens ein paar Hundert Bausteinen (Basenpaaren), und hier steckt das Problem. Die DNA-Moleküle, die von und in der Natur hervorgebracht werden, sind ausnahmslos wesentlich länger. Sie erstrecken sich von einigen Tausend bis zu vielen Millionen Basenpaaren, und jeder Versuch, molekulare Fäden solcher Ausmaße mit Pipetten oder im Reagenzglas zu handhaben, führte unweigerlich zu ihrer Zerstörung.\nSo schön sie auch waren – die langen Genmoleküle der Natur zerfielen den Wissenschaftlern bei ihrer Arbeit lange Zeit buchstäblich unter den Händen.\nDoch Ende der sechziger Jahre bemerkten die Molekularbiologen, dass Bakterien in der Lage sind, das Genom einiger Viren zu zerlegen, die sie anzugreifen und zu fressen versuchen. Für diese Viren stellten solche Bakterien eine „restricted area“ dar, wie im Amerikanischen ein Gelände heißt, dessen unerlaubtes Betreten bestraft wird. Diese Ausdrucksweise wurde in die technische Sprache der Wissenschaft übernommen. So hieß es nun, dass Bakterien selbst dafür sorgen, eine verbotene Zone zu sein, und Eindringlinge bestrafen, indem sie deren DNA „restringieren“ (zerlegen). Dazu verwenden sie raffinierte molekulare Werkzeuge, die Restriktionsenzyme heißen und die man sich als molekulare Scheren vorstellen kann. Einige Restriktionsenzyme konnten bald aus Bakterien isoliert werden und wurden interessierten Wissenschaftlern gezielt für die Zerlegung von Genmaterial in handhabbare Streifen zur Verfügung gestellt – und zwar gegen Geld von biochemisch tätigen Firmen, die den Wissenschaftlern diese Präparierarbeit abnahmen und ihnen mehr Zeit für kreative Experimente gaben.\nBald nach den molekularen Schneideinstrumenten wurden auch ihre Gegenstücke entdeckt, nämlich Zellbausteine, die das zerschnippelte Erbmaterial wieder zusammensetzen konnten. Wer beides unternahm – also erst DNA zerlegte und dann im Reagenzglas die Einzelteile neu kombinierte (rekombinierte), der betrieb, was ab 1973 den Namen Gentechnik trug. \nMit der Gentechnik eröffnete sich die Möglichkeit, kurze DNA-Abschnitte erstens herzustellen und zweitens so anzureichern, dass ausreichend Material für biochemische Analysen zur Verfügung stand. Mit anderen Worten, seit der Mitte der siebziger Jahre bestanden die Voraussetzungen dafür, Verfahren zu entwickeln, mit deren Hilfe die Sequenz von DNA-Fragmenten bestimmt werden konnte, und es dauerte nicht lange, bis tatsächlich Wege zur Gensequenzierung, also der Offenlegung der Erbinformation, aufgezeigt wurden. Vor allem zwei Strategien hatten es dabei den Wissenschaftlern angetan. Die erste geht auf den Briten Fred Sanger und die zweite auf den Amerikaner Walter Gilbert zurück, und beide erlauben es mit großer Zuverlässigkeit (bei einem nur anfänglich großen technischem Aufwand), DNA-Fragmente von einigen hundert Basenpaaren zu sequenzieren. Und wie nicht anders zu erwarten, machten sich bald die Mitarbeiter einiger Laboratorien an diese Arbeit.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Ein „überschaubare DNA-Fragment“, deren Untersuchung für Biochemiker zunächst nicht möglich war, meint ein Stück DNA, das kurz genug ist, um mit traditionell verfügbaren biochemischen Methoden von einem Genetiker – etwa im Rahmen einer Doktorarbeit – analysiert zu werden. Und „kurz genug“ meint dabei eine Kette mit höchstens ein paar Hundert Bausteinen (Basenpaaren), und hier steckt das Problem. Die DNA-Moleküle, die von und in der Natur hervorgebracht werden, sind ausnahmslos wesentlich länger. Sie erstrecken sich von einigen Tausend bis zu vielen Millionen Basenpaaren, und jeder Versuch, molekulare Fäden solcher Ausmaße mit Pipetten oder im Reagenzglas zu handhaben, führte unweigerlich zu ihrer Zerstörung.\nSo schön sie auch waren – die langen Genmoleküle der Natur zerfielen den Wissenschaftlern bei ihrer Arbeit lange Zeit buchstäblich unter den Händen.\nDoch Ende der sechziger Jahre bemerkten die Molekularbiologen, dass Bakterien in der Lage sind, das Genom einiger Viren zu zerlegen, die sie anzugreifen und zu fressen versuchen. Für diese Viren stellten solche Bakterien eine „restricted area“ dar, wie im Amerikanischen ein Gelände heißt, dessen unerlaubtes Betreten bestraft wird. Diese Ausdrucksweise wurde in die technische Sprache der Wissenschaft übernommen. So hieß es nun, dass Bakterien selbst dafür sorgen, eine verbotene Zone zu sein, und Eindringlinge bestrafen, indem sie deren DNA „restringieren“ (zerlegen). Dazu verwenden sie raffinierte molekulare Werkzeuge, die Restriktionsenzyme heißen und die man sich als molekulare Scheren vorstellen kann. Einige Restriktionsenzyme konnten bald aus Bakterien isoliert werden und wurden interessierten Wissenschaftlern gezielt für die Zerlegung von Genmaterial in handhabbare Streifen zur Verfügung gestellt – und zwar gegen Geld von biochemisch tätigen Firmen, die den Wissenschaftlern diese Präparierarbeit abnahmen und ihnen mehr Zeit für kreative Experimente gaben.\nBald nach den molekularen Schneideinstrumenten wurden auch ihre Gegenstücke entdeckt, nämlich Zellbausteine, die das zerschnippelte Erbmaterial wieder zusammensetzen konnten. Wer beides unternahm – also erst DNA zerlegte und dann im Reagenzglas die Einzelteile neu kombinierte (rekombinierte), der betrieb, was ab 1973 den Namen Gentechnik trug. \nMit der Gentechnik eröffnete sich die Möglichkeit, kurze DNA-Abschnitte erstens herzustellen und zweitens so anzureichern, dass ausreichend Material für biochemische Analysen zur Verfügung stand. Mit anderen Worten, seit der Mitte der siebziger Jahre bestanden die Voraussetzungen dafür, Verfahren zu entwickeln, mit deren Hilfe die Sequenz von DNA-Fragmenten bestimmt werden konnte, und es dauerte nicht lange, bis tatsächlich Wege zur Gensequenzierung, also der Offenlegung der Erbinformation, aufgezeigt wurden. Vor allem zwei Strategien hatten es dabei den Wissenschaftlern angetan. Die erste geht auf den Briten Fred Sanger und die zweite auf den Amerikaner Walter Gilbert zurück, und beide erlauben es mit großer Zuverlässigkeit (bei einem nur anfänglich großen technischem Aufwand), DNA-Fragmente von einigen hundert Basenpaaren zu sequenzieren. Und wie nicht anders zu erwarten, machten sich bald die Mitarbeiter einiger Laboratorien an diese Arbeit.",
      "id": [
        "145842"
      ],
      "item": "Die Möglichkeit der Gentechnik"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145843]\n[%Gentechnik]\nDer größte Zweig der Gentechnik ist die „grüne“ Gentechnik, der genetische Umbau von Nutzpflanzen. Medizinische Gentechnik wird „rote“ Gentechnik genannt, und solche, die Bakterien für die Beseitigung von speziellen Abfällen verändert, „weiße“.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der größte Zweig der Gentechnik ist die „grüne“ Gentechnik, der genetische Umbau von Nutzpflanzen. Medizinische Gentechnik wird „rote“ Gentechnik genannt, und solche, die Bakterien für die Beseitigung von speziellen Abfällen verändert, „weiße“.",
      "id": [
        "145843"
      ],
      "item": "Gentechnik"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145844]\n[%Die neue Genetik und das Genomprojekt]\nEs sollte betont werden, dass die ersten DNA-Analysen keineswegs im Sinn hatten, komplette Genome zu entziffern. Diese Aufgabe erschien allen Beteiligten angesichts der Millionen und Milliarden Kettenglieder (Basenpaare) der Genome viel zu groß. Und zwar auch deshalb, weil die in den siebziger Jahren verfügbaren Computer – wir befinden uns in den Anfangsjahren von Apple und Microsoft – mit ihren noch langsamen Operationen, den viel zu geringen Speicherkapazitäten und eher bescheidenen Rechenleistungen den Biologen nicht viel Mut machten, sich auf Datenmengen von vielen Millionen Bytes einzulassen.\nDie ersten Wissenschaftler, die sich als Genentzifferer oder Gensequenzierer betätigten, waren mehr an einzelnen Genen als an kompletten Genomen interessiert, und sie konzentrierten sich besonders auf Gene, die mit der Entstehung von Krebs in Verbindung gebracht werden konnten. Die Idee von sogenannten Onkogenen wurde damals etabliert, nachdem erkannt worden war, dass die Produkte dieser Gene in der Lage sind, eine normal wachsende Zelle in eine Tumorzelle zu verwandeln, die sich unkontrolliert vermehrt und dem gesunden Gewebe dabei die Lebensgrundlage entzieht. Nach und nach stellte sich im Rahmen dieser Arbeiten immer mehr die Gewissheit ein, dass Krebs auch – vielleicht sogar meist – eine genetische Krankheit ist. Und Mitte der achtziger Jahre schlug Renato Dulbecco, ein amerikanischer Nobelpreisträger italienischer Abstammung, vor, ernsthafte Konsequenzen aus dieser Erkenntnis zu ziehen.\nDulbeccos einfache Logik war unwiderstehlich: Wenn man Krebs besiegen will, muss man ihn verstehen, und wenn Krebs genetisch bedingt ist, kann man ihn nur verstehen, wenn man die Gene kennt. Also sollte man alles daran setzen, die Gene kennenzulernen, und das heißt nicht mehr und nicht weniger, als sich an die Aufgabe heranzuwagen, das menschliche Genom zu sequenzieren, und zwar komplett – alle drei Milliarden Basenpaare, die das genetische Material einer menschlichen Körperzelle ausmachen, wenn man nur den einfachen (haploiden) Chromosomensatz rechnet – oder eben sechs Milliarden, wenn man den üblicherweise vorhandenen zweifachen (diploiden) Satz rechnet.\nDulbecco konnten diesen verblüffenden und zunächst eher belächelten Vorschlag auch deshalb machen, weil den Biologen damals ein weiterer methodischer Fortschritt gelungen war, der ein solches Genomprojekt tatsächlich praktikabel erscheinen ließ. Auch dieser Fortschritt basierte auf der Gentechnik, genauer auf ihren Werkzeugen, den Restriktionsenzymen. Mit ihrer Hilfe lässt sich – wie erwähnt – das genetische Material einer Zelle (zum Beispiel einer menschlichen) zerstückeln, fragmentieren. Die dabei entstehenden Restriktionsfragmente lassen sich durch traditionelle Methoden der Biochemie trennen und sortieren, wobei die Ergebnisse in Form schöner Streifenmuster (Banden) präsentiert werden können.\nDieser Tatbestand war nicht weiter aufregend, bis einige Molekularbiologen unter der Führung von David Botstein und Ron Davies im Jahre 1980 feststellten, dass die dabei produzierten Schnittmuster erstens von Individuum zu Individuum verschieden sind und zweitens weitervererbt werden. Das Genom eines jeden einzelnen Menschen besaß andere Stärken und Schwächen gegenüber den Restriktionsenzymen als das eines anderen. Wo bei dem einen der DNA-Strang zerschnitten war, blieb er bei einem anderen ungetrennt und ungekehrt. Die einer Person zugehörende Vielgestaltigkeit der Restriktionsfragmente bekam nun den leicht nachvollziehbaren Namen Polymorphismus, und wenn man die beiden Ausdrücke zusammenzieht, entsteht ein Wortungetüm, das es in sich hat und das man erst üben muss, bevor es einem leicht über die Lippen geht. Die Rede ist von dem Restriktionsfragmentlängenpolymorphismus, den die Biologen rasch RFLP abkürzten und „Riflip“ aussprachen. Mit diesem Phänomen hatten Botstein und Davies einen Weg entdeckt, um auch beim Menschen das tun zu können, was die Genetiker schon seit Jahrzehnten bei anderen Organismen – wie Bakterien, Hefepilzen und Fliegen – exerzierten: nämlich eine Genkarte anzufertigen.\nDie Idee zu solch einer genetischen Karte war bereits 1915 aufgekommen, als man bei Fliegen die Vererbung von sichtbaren Mutationen – etwa der Augenfarbe oder der Flügelform – verfolgte und versuchte, den Ort der dazugehörigen Gene oder Genvarianten auf den Chromosomen zu bestimmen. Mit Hilfe der Riflips war es nun – wie 1980 gezeigt wurde – möglich, die Idee der genetischen Karte in die Tat umzusetzen. Ein Polymorphismus lässt auf eine veränderte Information – eine Sequenzvariation – in der DNA schließen, die durch ein Restriktionsenzym bestimmt wird, das an diesem Stück DNA seine Arbeit (das Zerschneiden) verrichten konnte oder nicht.\nAuf diese Weise ließen sich zunächst – in mühevoller Kleinarbeit, die lange Zeit gut für Doktorarbeiten geeignet war – Sequenzmarkierungen spezifischen Orten auf einem Chromosom zuordnen. Und im Anschluss daran ließen sich Gene, die zum Beispiel an der Entstehung von Krankheiten (wie etwa Krebs) beteiligt waren, auf dem Chromosom lokalisieren. Dies gelang, nachdem man deren Vererbungsmuster – ebenfalls in mühevoller Detailarbeit, die mit detektivischem Scharfsinn durchzuführen war – mit denen der Wegzeichen abgeglichen und zusammengefügt hatte.\nDie Möglichkeit, die menschlichen Chromosomen zu kartieren, wurde als „neue Genetik“ begrüßt, und sie führte rasch zu ersten Erfolgen. 1983 gelang es, das Gen, das in einigen Variationen die tödlich verlaufende Krankheit Huntington Chorea hervorbringen kann, auf dem kurzen Arm von Chromosom 7 zu lokalisieren. 1987 wurde eine erste Genkarte des Menschen publiziert, die rund 400 Markierungen enthielt; im folgenden Jahrzehnt gab es dann bereits 10 000 Wegzeichen („Marker“). Inzwischen waren auch andere Markierungsmethoden als die durch die „Riflips“ gefunden worden. Im Sog dieser Kartierungen wandelte die Medizinische Genetik ihr Gesicht, da sie bald mehr als 1000 für Krankheiten verantwortliche Gene wohldefinierten Orten („Loci“) auf den Chromosomen zuordnen konnte.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Es sollte betont werden, dass die ersten DNA-Analysen keineswegs im Sinn hatten, komplette Genome zu entziffern. Diese Aufgabe erschien allen Beteiligten angesichts der Millionen und Milliarden Kettenglieder (Basenpaare) der Genome viel zu groß. Und zwar auch deshalb, weil die in den siebziger Jahren verfügbaren Computer – wir befinden uns in den Anfangsjahren von Apple und Microsoft – mit ihren noch langsamen Operationen, den viel zu geringen Speicherkapazitäten und eher bescheidenen Rechenleistungen den Biologen nicht viel Mut machten, sich auf Datenmengen von vielen Millionen Bytes einzulassen.\nDie ersten Wissenschaftler, die sich als Genentzifferer oder Gensequenzierer betätigten, waren mehr an einzelnen Genen als an kompletten Genomen interessiert, und sie konzentrierten sich besonders auf Gene, die mit der Entstehung von Krebs in Verbindung gebracht werden konnten. Die Idee von sogenannten Onkogenen wurde damals etabliert, nachdem erkannt worden war, dass die Produkte dieser Gene in der Lage sind, eine normal wachsende Zelle in eine Tumorzelle zu verwandeln, die sich unkontrolliert vermehrt und dem gesunden Gewebe dabei die Lebensgrundlage entzieht. Nach und nach stellte sich im Rahmen dieser Arbeiten immer mehr die Gewissheit ein, dass Krebs auch – vielleicht sogar meist – eine genetische Krankheit ist. Und Mitte der achtziger Jahre schlug Renato Dulbecco, ein amerikanischer Nobelpreisträger italienischer Abstammung, vor, ernsthafte Konsequenzen aus dieser Erkenntnis zu ziehen.\nDulbeccos einfache Logik war unwiderstehlich: Wenn man Krebs besiegen will, muss man ihn verstehen, und wenn Krebs genetisch bedingt ist, kann man ihn nur verstehen, wenn man die Gene kennt. Also sollte man alles daran setzen, die Gene kennenzulernen, und das heißt nicht mehr und nicht weniger, als sich an die Aufgabe heranzuwagen, das menschliche Genom zu sequenzieren, und zwar komplett – alle drei Milliarden Basenpaare, die das genetische Material einer menschlichen Körperzelle ausmachen, wenn man nur den einfachen (haploiden) Chromosomensatz rechnet – oder eben sechs Milliarden, wenn man den üblicherweise vorhandenen zweifachen (diploiden) Satz rechnet.\nDulbecco konnten diesen verblüffenden und zunächst eher belächelten Vorschlag auch deshalb machen, weil den Biologen damals ein weiterer methodischer Fortschritt gelungen war, der ein solches Genomprojekt tatsächlich praktikabel erscheinen ließ. Auch dieser Fortschritt basierte auf der Gentechnik, genauer auf ihren Werkzeugen, den Restriktionsenzymen. Mit ihrer Hilfe lässt sich – wie erwähnt – das genetische Material einer Zelle (zum Beispiel einer menschlichen) zerstückeln, fragmentieren. Die dabei entstehenden Restriktionsfragmente lassen sich durch traditionelle Methoden der Biochemie trennen und sortieren, wobei die Ergebnisse in Form schöner Streifenmuster (Banden) präsentiert werden können.\nDieser Tatbestand war nicht weiter aufregend, bis einige Molekularbiologen unter der Führung von David Botstein und Ron Davies im Jahre 1980 feststellten, dass die dabei produzierten Schnittmuster erstens von Individuum zu Individuum verschieden sind und zweitens weitervererbt werden. Das Genom eines jeden einzelnen Menschen besaß andere Stärken und Schwächen gegenüber den Restriktionsenzymen als das eines anderen. Wo bei dem einen der DNA-Strang zerschnitten war, blieb er bei einem anderen ungetrennt und ungekehrt. Die einer Person zugehörende Vielgestaltigkeit der Restriktionsfragmente bekam nun den leicht nachvollziehbaren Namen Polymorphismus, und wenn man die beiden Ausdrücke zusammenzieht, entsteht ein Wortungetüm, das es in sich hat und das man erst üben muss, bevor es einem leicht über die Lippen geht. Die Rede ist von dem Restriktionsfragmentlängenpolymorphismus, den die Biologen rasch RFLP abkürzten und „Riflip“ aussprachen. Mit diesem Phänomen hatten Botstein und Davies einen Weg entdeckt, um auch beim Menschen das tun zu können, was die Genetiker schon seit Jahrzehnten bei anderen Organismen – wie Bakterien, Hefepilzen und Fliegen – exerzierten: nämlich eine Genkarte anzufertigen.\nDie Idee zu solch einer genetischen Karte war bereits 1915 aufgekommen, als man bei Fliegen die Vererbung von sichtbaren Mutationen – etwa der Augenfarbe oder der Flügelform – verfolgte und versuchte, den Ort der dazugehörigen Gene oder Genvarianten auf den Chromosomen zu bestimmen. Mit Hilfe der Riflips war es nun – wie 1980 gezeigt wurde – möglich, die Idee der genetischen Karte in die Tat umzusetzen. Ein Polymorphismus lässt auf eine veränderte Information – eine Sequenzvariation – in der DNA schließen, die durch ein Restriktionsenzym bestimmt wird, das an diesem Stück DNA seine Arbeit (das Zerschneiden) verrichten konnte oder nicht.\nAuf diese Weise ließen sich zunächst – in mühevoller Kleinarbeit, die lange Zeit gut für Doktorarbeiten geeignet war – Sequenzmarkierungen spezifischen Orten auf einem Chromosom zuordnen. Und im Anschluss daran ließen sich Gene, die zum Beispiel an der Entstehung von Krankheiten (wie etwa Krebs) beteiligt waren, auf dem Chromosom lokalisieren. Dies gelang, nachdem man deren Vererbungsmuster – ebenfalls in mühevoller Detailarbeit, die mit detektivischem Scharfsinn durchzuführen war – mit denen der Wegzeichen abgeglichen und zusammengefügt hatte.\nDie Möglichkeit, die menschlichen Chromosomen zu kartieren, wurde als „neue Genetik“ begrüßt, und sie führte rasch zu ersten Erfolgen. 1983 gelang es, das Gen, das in einigen Variationen die tödlich verlaufende Krankheit Huntington Chorea hervorbringen kann, auf dem kurzen Arm von Chromosom 7 zu lokalisieren. 1987 wurde eine erste Genkarte des Menschen publiziert, die rund 400 Markierungen enthielt; im folgenden Jahrzehnt gab es dann bereits 10 000 Wegzeichen („Marker“). Inzwischen waren auch andere Markierungsmethoden als die durch die „Riflips“ gefunden worden. Im Sog dieser Kartierungen wandelte die Medizinische Genetik ihr Gesicht, da sie bald mehr als 1000 für Krankheiten verantwortliche Gene wohldefinierten Orten („Loci“) auf den Chromosomen zuordnen konnte.",
      "id": [
        "145844"
      ],
      "item": "Die neue Genetik und das Genomprojekt"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145845]\n[%Restriktionsfragmente]\nRestriktionsfragmente lassen sich sortieren, wenn sie sich auf einem Substrat, einem Gel, bewegen können. Bei Anlegung einer positiven Spannung an einem Ende der Anordnung setzen sich die negativ geladenen Basen der RNA-Fragmente in Richtung Spannungspol in Bewegung, und sie sind dabei umso schneller, je kürzer sie sind.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Restriktionsfragmente lassen sich sortieren, wenn sie sich auf einem Substrat, einem Gel, bewegen können. Bei Anlegung einer positiven Spannung an einem Ende der Anordnung setzen sich die negativ geladenen Basen der RNA-Fragmente in Richtung Spannungspol in Bewegung, und sie sind dabei umso schneller, je kürzer sie sind.",
      "id": [
        "145845"
      ],
      "item": "Restriktionsfragmente"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145846]\n[%Chromosomen]\nChromosomen sind Körper im Zellkern, die außer der DNA deren „Verpackung“ enthalten, die hauptsächlich aus Proteinen besteht. Jeder Mensch hat 46 Chromosomen, von denen bei Frauen zwei sogenannte X-Chromosomen sind; Männer besitzen stattdessen ein X- und ein Y-Chromosom. Seit 1910 ist nachgewiesen, dass Chromosomen Träger des Erbguts sind.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Chromosomen sind Körper im Zellkern, die außer der DNA deren „Verpackung“ enthalten, die hauptsächlich aus Proteinen besteht. Jeder Mensch hat 46 Chromosomen, von denen bei Frauen zwei sogenannte X-Chromosomen sind; Männer besitzen stattdessen ein X- und ein Y-Chromosom. Seit 1910 ist nachgewiesen, dass Chromosomen Träger des Erbguts sind.",
      "id": [
        "145846"
      ],
      "item": "Chromosomen"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145847]\n[%Der Wettlauf um das humane Genom]\nNatürlich traf der Plan Renato Delbuccos nach der kompletten Sequenzierung des humanen Genoms schon allein deshalb auf Skepsis, weil es um mehr als drei Milliarden Basenpaare ging, die es in die richtige Reihenfolge zu bringen galt. Die damaligen Methoden erlaubten bestenfalls das Ermitteln von 300 Basenpaaren an einem Stück, was bedeutete, dass Heerscharen von Assistenten (oder noch billiger: Doktoranden) benötigt würden, die jahrelang repetitive und damit stumpfsinnige Arbeiten verrichten. Außerdem ließ sich abschätzen, dass es rund einen Dollar kosten würde, die Position einer Base ausfindig zu machen, und wenn deshalb tatsächlich drei Milliarden Dollar für das Projekt ausgegeben wurden, mussten andere Bereiche der Forschung zurücktreten und ihre Ziele mit stark geschrumpften Budgets verfolgen.\nDas Geld ist eine Sache, die Größenordnung der Aufgabe eine andere. Jedes der drei Milliarden Basenpaare trägt jeweils eine Information. Das entspricht etwa der Menge der Buchstaben von 1000 Büchern mit 1000 Seiten oder eben einer ganzen Bibliothek, in der die gesammelten Werke nicht nur von Thomas Mann, sondern auch von Goethe, Shakespeare und vielen anderen Größen der Weltliteratur Platz finden würden.\nWie lange würde man brauchen, um diese Menge zu lesen? Als das Humangenom-Projekt konzipiert wurde, hätte man mit den verfügbaren Techniken etwa ein Jahr (und viel Geduld) gebraucht, um gerade einmal so um die 12 000 Basenpaare zu sequenzieren. Ohne die optimistische Annahme, dass große technische Fortschritte hier Abhilfe schaffen würden, wäre an dieser Stelle jede weitere Planung sinnlos gewesen. Doch Wissenschaftler haben Vertrauen in die eigenen Fähigkeiten, und bald kamen Maschinen auf den Markt, die für 12 000 Basenpaare nur noch 20 Minuten benötigten, und der sicher schon wieder veraltete „letzte Stand“ der technischen Dinge besagt, dass es möglich ist, 12 000 „Buchstaben“ des Genoms pro Minute lesbar zu machen.\nDamit konnte zwar zunächst natürlich niemand rechnen, aber unabhängig davon galt die schiere Quantität, die zur Sequenzierung des menschlichen Genoms zu bewältigen war, niemals als gutes Argument gegen das Projekt, vor allem deshalb nicht, weil inzwischen die Computerkapazitäten dank besserer Chips mit besserer Software ungeheuer groß geworden waren und weiter zunahmen. Es gab aber ein besseres Argument, und das stammte aus der Biologie selbst, die nämlich längst bemerkt hatte, dass nicht alles Gold war, was da genetisch glänzte. Anders ausgedrückt: Man vermutete schon länger, dass nur rund zehn Prozent des menschlichen Erbmaterials das darstellte, was man Gene nennt. Zwischen diesen informativen und relevanten Segmenten lagen wahrscheinlich viele Stellen, die anscheinend ohne Bedeutung waren und abfällig als „junk“ tituliert wurden. Warum – so fragten viele Molekularbiologen – soll man all diese DNA sequenzieren, wenn man nur die wichtigen zehn Prozent braucht.\nDoch jedes Ziel, das erreichbar ist, lockt uns an, selbst wenn mehr Management als Wissenschaft benötigt wird, um es zu erreichen. Nach und nach sammelten sich die Wissenschaftler, die sich zutrauten, das menschliche Genom zu einem vernünftigen Preis zu entziffern. Sie wollten deshalb das Projekt in Angriff nehmen und begannen damit vor allem in den USA und in Frankreich. Allen voran entwarf der charismatische Walter Gilbert schon früh einen Weg, auf dem es gelingen sollte, eine komplette Sequenz zu erreichen.\nDabei kam es unter anderem darauf an, die langen DNA-Moleküle, aus denen das Genom besteht, erstens geeignet zu fragmentieren, zweitens die Bruchstücke in ausreichender Menge herzustellen und drittens deren Basenfolge zu bestimmen. Um ausreichend Material für die Sequenzierung herzustellen, wurden die produzierten DNA-Fragmente mit Hilfe der Gentechnik kloniert. Die Sequenzierung des Genoms erfolgte dann Klon nach Klon – oder Clone-by-clone, wie es im Amerikanischen heißt. \nDer ursprüngliche Plan, sich direkt am Menschen und seinem Genom zu versuchen, wurde bald zugunsten der Strategie aufgegeben, erst die kleinen Genome von Organismen zu sequenzieren, mit denen man viel experimentelle Erfahrung hatte – also von Bakterien, Hefepilzen, Fliegen und Würmern. Alle diese Bemühungen wurden ab 1990 unter dem Dach des Humangenom-Projektes zusammengefasst und organisiert, das den ersten Versuch der biologischen Wissenschaft darstellte, eine großräumige Infrastruktur aufzubauen, um die Mechanismen und Gesetze des Lebens zu erkunden.\nDas erste große Projekt, das auf diese Weise angegangen wurde, bestand in der Sequenzierung der 12 Millionen Basen, die das Genom der Hefe ausmachen. Zwischen 1992 und 1996 brachten zwölf kooperierende Laboratorien die Sequenzen individueller Chromosomen hervor, bis die vollständige Sequenz der Hefe vorlag. Durch den dabei erzielten Erfolg ermutigt, wagte man sich 1998 an die 97 Millionen Basen des kleinen und niedlichen Wurms mit dem schönen Namen __Caenorhabditis elegans__, was allein deshalb bemerkenswert ist, weil damit erstmals das Genom eines vielzelligen Organismus sequenziert wurde.\nDie für die Finanzierung der öffentlichen Forschung zuständigen Behörden zögerten einige Zeit, ihre Ressourcen einem Projekt anzuvertrauen, von dem niemand genau sagen konnte, wo es enden würde. Erst nachdem in den USA das Department of Energy unter seinem Direktor Charles DeLisi ab 1987 bereit war, Geld in Genomprojekte zu stecken, reagierten auch die National Institutes of Health (NIH), also die Behörde, die mit Abstand weltweit das meiste Geld für biomedizinische Forschungen zur Verfügung stellt. Im September 1988 wurde unter ihrer Führung ein Büro für Genomforschung eingerichtet, das bald in ein „National Center for Human Genome Research“ (NCHGR) umgewandelt und einige Jahre von James D. Watson, dem Mitentdecker der Doppelhelix, geleitet wurde.\nZu dieser Zeit war das Sequenzieren immer noch langsam und teuer, und nur größte Optimisten konnten es schon wagen, von einem möglichen Abschluss des Projektes zu sprechen. Noch sehr viele wissenschaftliche und technische Fortschritte wurden benötigt, um überzeugend voranzukommen, und die kamen zunächst aus Frankreich, genauer aus den Pariser Laboratorien von Daniel Cohen und Jean Weissenbach, die mit neuen Strategien nach dem Vorbild der Riflips (Restriktionsfragmentlängenpolymorphismus) immer genauere Genkarten anfertigen und damit das Genom immer besser zugänglich machten.\nWährend die Erforschung des Humangenoms dank der Entwicklung zunehmend leistungsfähiger Computer und dank Geldspenden der Computerhersteller und anderer kapitalkräftiger Firmen Schritt für Schritt vorankam, tauchte zu Beginn der neunziger Jahre eine schillernde Figur in der Genomforschung auf, die ihr eine neue Geschwindigkeit und eine kommerzielle Dimension gab. Gemeint ist der amerikanische Biochemiker Craig Venter, der 1992 das erste Unternehmen gründete, dessen Produkt Genomdaten in Form von Gensequenzen waren. Es heißt The Institute for Genomic Research, ist in Rockville im US-Bundesstaat Maryland beheimatet und in aller Welt unter seiner bewusst raubtierhaft klingenden Abkürzung bekannt: TIGR. Der auf dem Tiger Reitende, also Venter, ist überzeugt davon, dass sich die genetischen Informationen verkaufen lassen – zum Beispiel an die medizinischen Forschungsinstitute, die sich für Krebs interessieren, oder an die Pharmaindustrie, die nach Angriffspunkten für neue Medikamente sucht. Folglich sinnt er nach Wegen, die von seinen Mitarbeitern sequenzierten Gene bzw. Genabschnitte patentieren zu lassen. Ihm geht es nicht um Vollständigkeit bis ins letzte vielleicht nutzlose Detail. Ihm geht es um anwendungsfähige Ergebnisse, und sie will er so schnell wie möglich bekommen und anbieten können. Zunächst ersinnt Venter ein unter dem Stichwort „expressed sequence tags“ (EST) bekanntes Verfahren, mit dem es möglich wird, die aktiven und somit interessanten Gene in einem Genom auszusondern und von dem verbleibenden „Abfall“ der übrigen Sequenzen zu trennen. Und etwas später findet er einen Weg – die Schrotschussmethode oder Shotgun-Sequenzierung – auf dem sich auf zwar unelegante, dafür aber schnelle Weise viel mehr DNA-Sequenzen ermitteln lassen als mit den herkömmlichen Methoden, die in den öffentlich finanzierten Genomprojekten eingesetzt werden.\nVenters Verfahren setzt mehr auf Computerkapazitäten als auf raffinierte Überlegungen. Er nutzt jede Möglichkeit der Automatisierung und wirbelt dabei die Welt der Genomforschung wild durcheinander – zuerst und nachhaltig mit der Sequenz von Haemophilus influenza. Diese und andere Projekte bringen ihm am Ende des 20. Jahrhunderts die große Aufmerksamkeit des Publikums – vertreten durch Magazine und Fernsehstationen – ein, was oft den Eindruck entstehen lässt, als hätte Venter das Humangenom mit seinen Streichen fast im Alleingang entschlüsselt. Dem ist entschieden zu widersprechen, denn bei allen Vorzügen von Venters Schrotschuss-Methode hatte das Verfahren auch große Schwächen.\nVenter sollte noch die Hilfe der anderen Seite brauchen, also der Vertreter der öffentlichen Wissenschaft, deren Führung inzwischen Francis Collins übernommen hatte. Er hatte Watson 1992 als Direktor des NCHGR abgelöst, und ihm war der Gedanke an unveröffentlichte Daten, die als Ware gehandelt werden, ein Gräuel. Für ihn ging es nicht in erster Linie um Geld, sondern um die Wissenschaft. Collins und seine Kollegen aus den Universitäten beschlossen deshalb 1996 die sogenannten Bermuda-Prinzipien, die alle Teilnehmer an Genomprojekten verpflichteten, ihre Daten innerhalb von 24 Stunden einer allgemein zugänglichen Datenbank zur Verfügung zu stellen. Zahlreiche finanzstarke Stiftungen – allen voran der britische Wellcome Trust –, förderten diesen Gedanken ideell und materiell.\nVenter nimmt den Fehdehandschuh auf, indem er eine weitere Firma gründet, die er nach dem lateinischen Wort für Geschwindigkeit Celera nennt. Und er ist wirklich schnell! Im Mai 1998 schockiert er seine Kontrahenten, als er verkündet, er werde mit seinen Verfahren das menschliche Genom innerhalb von drei Jahren komplett sequenzieren. Er ist damit der erste Wissenschaftler, der einen Termin für die Erreichung dieses Ziels nennt, und wenn auch niemand behaupten sollte, dass Venter sein kühnes Versprechen eingelöst hat, so muss doch zugegeben werden, dass ohne seine ungewöhnliche Umtriebigkeit alles schleppender und gemütlicher zugegangen wäre.\nBei Celera stehen rund 300 automatische Informationslesemaschinen (Sequenziergeräte) – alle vom Typ ABI PRISM DNA Analyzer, der einige Hunderttausend Dollar kostet. Diese Geräte laufen mit voller Kapazität Tag und Nacht, was unter anderem die jährliche Rechnung des Elektrizitätswerkes auf die schwindelerregende Höhe von 1 Million Dollar bringt. Die kontinuierlich produzierten Sequenzdaten – in der Größenordnung von Terabytes – werden von speziell angefertigten Computern montiert, wobei die Berechnungen für die erste „assembly“ 500 Millionen Billionen Sequenzvergleiche nötig macht. Die abschließenden Berechnungen benötigten 64 Gigabytes an Speicherkapazität. Spätestes 1999 reagierten die aus öffentlichen Mitteln finanzierten Wissenschaftler auf diese Herausforderung. In einer groß angelegten weltweiten Kooperation mit Schwerpunkten im britischen und im amerikanischen Cambridge und wesentlichen Zuarbeiten aus Japan und Deutschland holten sie Chromosom für Chromosom und Sequenz für Sequenz Venters Vorsprung auf. Und im Juni 2000 entschlossen sich das öffentliche Genomprojekt und die private Celera-Initiative, ihre Daten zusammenzulegen, um gemeinsam von einer ersten umfassenden Analyse des menschlichen Genoms berichten zu können. Das humane Genom wurde dann zum ersten Mal im Februar 2001 publiziert. Allerdings waren auch danach noch viele Feinarbeiten und Sequenzangleichungen nötig. Im Frühjahr 2003 wurde das offizielle Ende des Humangenom-Projektes bekannt gegeben, pünktlich zum fünfzigsten Jahrestag der Entdeckung der Doppelhelix. Es war schon eine erstaunliche Geschichte, die sich in diesem halben Jahrhundert abgespielt hatte, von der Entdeckung einer phantastischen Struktur bis zur vollständigen Erkundung des wahrscheinlich kompliziertesten Moleküls, das die Natur „erfunden“ hat.\nÜbrigens – jetzt haben wir das humane Genom und wissen gar nicht, wessen DNA-Sequenz denn da vorliegt. Wessen Basenpaare kennen wir nun in der aufgereihten Form?\nDie Antwort lautet, dass es nicht einen einzelnen DNA-Spender, sondern eine Vielzahl von ihnen gibt. Sie haben sich aufgrund von Anzeigen in lokalen Zeitungen bei wissenschaftlichen Laboratorien gemeldet, wo ihnen Zellproben abgenommen wurden. Diese wurden sodann aufbewahrt und weitergegeben, sodass keiner mehr wissen konnte, mit wessen DNA er gearbeitet hat. Von Anfang an wurde darauf geachtet, Individuen zu nehmen, die sich verschiedenen Volksgruppen zugehörig fühlten. Im Humangenom stecken unter anderen die Sequenzen von afrikanischen, asiatischen, amerikanischen und europäischen Menschen. Natürlich unterscheiden sich die einzelnen Spender untereinander. Aber aus ihnen allen lässt sich die eine gemeinsame Sequenz – einen Konsensus-Sequenz – ableiten, die wir jetzt „menschlich“ nennen. So zeigt die Wissenschaft am Genom, dass es viel gibt, was uns eint, und nur wenig, was uns trennt.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Natürlich traf der Plan Renato Delbuccos nach der kompletten Sequenzierung des humanen Genoms schon allein deshalb auf Skepsis, weil es um mehr als drei Milliarden Basenpaare ging, die es in die richtige Reihenfolge zu bringen galt. Die damaligen Methoden erlaubten bestenfalls das Ermitteln von 300 Basenpaaren an einem Stück, was bedeutete, dass Heerscharen von Assistenten (oder noch billiger: Doktoranden) benötigt würden, die jahrelang repetitive und damit stumpfsinnige Arbeiten verrichten. Außerdem ließ sich abschätzen, dass es rund einen Dollar kosten würde, die Position einer Base ausfindig zu machen, und wenn deshalb tatsächlich drei Milliarden Dollar für das Projekt ausgegeben wurden, mussten andere Bereiche der Forschung zurücktreten und ihre Ziele mit stark geschrumpften Budgets verfolgen.\nDas Geld ist eine Sache, die Größenordnung der Aufgabe eine andere. Jedes der drei Milliarden Basenpaare trägt jeweils eine Information. Das entspricht etwa der Menge der Buchstaben von 1000 Büchern mit 1000 Seiten oder eben einer ganzen Bibliothek, in der die gesammelten Werke nicht nur von Thomas Mann, sondern auch von Goethe, Shakespeare und vielen anderen Größen der Weltliteratur Platz finden würden.\nWie lange würde man brauchen, um diese Menge zu lesen? Als das Humangenom-Projekt konzipiert wurde, hätte man mit den verfügbaren Techniken etwa ein Jahr (und viel Geduld) gebraucht, um gerade einmal so um die 12 000 Basenpaare zu sequenzieren. Ohne die optimistische Annahme, dass große technische Fortschritte hier Abhilfe schaffen würden, wäre an dieser Stelle jede weitere Planung sinnlos gewesen. Doch Wissenschaftler haben Vertrauen in die eigenen Fähigkeiten, und bald kamen Maschinen auf den Markt, die für 12 000 Basenpaare nur noch 20 Minuten benötigten, und der sicher schon wieder veraltete „letzte Stand“ der technischen Dinge besagt, dass es möglich ist, 12 000 „Buchstaben“ des Genoms pro Minute lesbar zu machen.\nDamit konnte zwar zunächst natürlich niemand rechnen, aber unabhängig davon galt die schiere Quantität, die zur Sequenzierung des menschlichen Genoms zu bewältigen war, niemals als gutes Argument gegen das Projekt, vor allem deshalb nicht, weil inzwischen die Computerkapazitäten dank besserer Chips mit besserer Software ungeheuer groß geworden waren und weiter zunahmen. Es gab aber ein besseres Argument, und das stammte aus der Biologie selbst, die nämlich längst bemerkt hatte, dass nicht alles Gold war, was da genetisch glänzte. Anders ausgedrückt: Man vermutete schon länger, dass nur rund zehn Prozent des menschlichen Erbmaterials das darstellte, was man Gene nennt. Zwischen diesen informativen und relevanten Segmenten lagen wahrscheinlich viele Stellen, die anscheinend ohne Bedeutung waren und abfällig als „junk“ tituliert wurden. Warum – so fragten viele Molekularbiologen – soll man all diese DNA sequenzieren, wenn man nur die wichtigen zehn Prozent braucht.\nDoch jedes Ziel, das erreichbar ist, lockt uns an, selbst wenn mehr Management als Wissenschaft benötigt wird, um es zu erreichen. Nach und nach sammelten sich die Wissenschaftler, die sich zutrauten, das menschliche Genom zu einem vernünftigen Preis zu entziffern. Sie wollten deshalb das Projekt in Angriff nehmen und begannen damit vor allem in den USA und in Frankreich. Allen voran entwarf der charismatische Walter Gilbert schon früh einen Weg, auf dem es gelingen sollte, eine komplette Sequenz zu erreichen.\nDabei kam es unter anderem darauf an, die langen DNA-Moleküle, aus denen das Genom besteht, erstens geeignet zu fragmentieren, zweitens die Bruchstücke in ausreichender Menge herzustellen und drittens deren Basenfolge zu bestimmen. Um ausreichend Material für die Sequenzierung herzustellen, wurden die produzierten DNA-Fragmente mit Hilfe der Gentechnik kloniert. Die Sequenzierung des Genoms erfolgte dann Klon nach Klon – oder Clone-by-clone, wie es im Amerikanischen heißt. \nDer ursprüngliche Plan, sich direkt am Menschen und seinem Genom zu versuchen, wurde bald zugunsten der Strategie aufgegeben, erst die kleinen Genome von Organismen zu sequenzieren, mit denen man viel experimentelle Erfahrung hatte – also von Bakterien, Hefepilzen, Fliegen und Würmern. Alle diese Bemühungen wurden ab 1990 unter dem Dach des Humangenom-Projektes zusammengefasst und organisiert, das den ersten Versuch der biologischen Wissenschaft darstellte, eine großräumige Infrastruktur aufzubauen, um die Mechanismen und Gesetze des Lebens zu erkunden.\nDas erste große Projekt, das auf diese Weise angegangen wurde, bestand in der Sequenzierung der 12 Millionen Basen, die das Genom der Hefe ausmachen. Zwischen 1992 und 1996 brachten zwölf kooperierende Laboratorien die Sequenzen individueller Chromosomen hervor, bis die vollständige Sequenz der Hefe vorlag. Durch den dabei erzielten Erfolg ermutigt, wagte man sich 1998 an die 97 Millionen Basen des kleinen und niedlichen Wurms mit dem schönen Namen __Caenorhabditis elegans__, was allein deshalb bemerkenswert ist, weil damit erstmals das Genom eines vielzelligen Organismus sequenziert wurde.\nDie für die Finanzierung der öffentlichen Forschung zuständigen Behörden zögerten einige Zeit, ihre Ressourcen einem Projekt anzuvertrauen, von dem niemand genau sagen konnte, wo es enden würde. Erst nachdem in den USA das Department of Energy unter seinem Direktor Charles DeLisi ab 1987 bereit war, Geld in Genomprojekte zu stecken, reagierten auch die National Institutes of Health (NIH), also die Behörde, die mit Abstand weltweit das meiste Geld für biomedizinische Forschungen zur Verfügung stellt. Im September 1988 wurde unter ihrer Führung ein Büro für Genomforschung eingerichtet, das bald in ein „National Center for Human Genome Research“ (NCHGR) umgewandelt und einige Jahre von James D. Watson, dem Mitentdecker der Doppelhelix, geleitet wurde.\nZu dieser Zeit war das Sequenzieren immer noch langsam und teuer, und nur größte Optimisten konnten es schon wagen, von einem möglichen Abschluss des Projektes zu sprechen. Noch sehr viele wissenschaftliche und technische Fortschritte wurden benötigt, um überzeugend voranzukommen, und die kamen zunächst aus Frankreich, genauer aus den Pariser Laboratorien von Daniel Cohen und Jean Weissenbach, die mit neuen Strategien nach dem Vorbild der Riflips (Restriktionsfragmentlängenpolymorphismus) immer genauere Genkarten anfertigen und damit das Genom immer besser zugänglich machten.\nWährend die Erforschung des Humangenoms dank der Entwicklung zunehmend leistungsfähiger Computer und dank Geldspenden der Computerhersteller und anderer kapitalkräftiger Firmen Schritt für Schritt vorankam, tauchte zu Beginn der neunziger Jahre eine schillernde Figur in der Genomforschung auf, die ihr eine neue Geschwindigkeit und eine kommerzielle Dimension gab. Gemeint ist der amerikanische Biochemiker Craig Venter, der 1992 das erste Unternehmen gründete, dessen Produkt Genomdaten in Form von Gensequenzen waren. Es heißt The Institute for Genomic Research, ist in Rockville im US-Bundesstaat Maryland beheimatet und in aller Welt unter seiner bewusst raubtierhaft klingenden Abkürzung bekannt: TIGR. Der auf dem Tiger Reitende, also Venter, ist überzeugt davon, dass sich die genetischen Informationen verkaufen lassen – zum Beispiel an die medizinischen Forschungsinstitute, die sich für Krebs interessieren, oder an die Pharmaindustrie, die nach Angriffspunkten für neue Medikamente sucht. Folglich sinnt er nach Wegen, die von seinen Mitarbeitern sequenzierten Gene bzw. Genabschnitte patentieren zu lassen. Ihm geht es nicht um Vollständigkeit bis ins letzte vielleicht nutzlose Detail. Ihm geht es um anwendungsfähige Ergebnisse, und sie will er so schnell wie möglich bekommen und anbieten können. Zunächst ersinnt Venter ein unter dem Stichwort „expressed sequence tags“ (EST) bekanntes Verfahren, mit dem es möglich wird, die aktiven und somit interessanten Gene in einem Genom auszusondern und von dem verbleibenden „Abfall“ der übrigen Sequenzen zu trennen. Und etwas später findet er einen Weg – die Schrotschussmethode oder Shotgun-Sequenzierung – auf dem sich auf zwar unelegante, dafür aber schnelle Weise viel mehr DNA-Sequenzen ermitteln lassen als mit den herkömmlichen Methoden, die in den öffentlich finanzierten Genomprojekten eingesetzt werden.\nVenters Verfahren setzt mehr auf Computerkapazitäten als auf raffinierte Überlegungen. Er nutzt jede Möglichkeit der Automatisierung und wirbelt dabei die Welt der Genomforschung wild durcheinander – zuerst und nachhaltig mit der Sequenz von Haemophilus influenza. Diese und andere Projekte bringen ihm am Ende des 20. Jahrhunderts die große Aufmerksamkeit des Publikums – vertreten durch Magazine und Fernsehstationen – ein, was oft den Eindruck entstehen lässt, als hätte Venter das Humangenom mit seinen Streichen fast im Alleingang entschlüsselt. Dem ist entschieden zu widersprechen, denn bei allen Vorzügen von Venters Schrotschuss-Methode hatte das Verfahren auch große Schwächen.\nVenter sollte noch die Hilfe der anderen Seite brauchen, also der Vertreter der öffentlichen Wissenschaft, deren Führung inzwischen Francis Collins übernommen hatte. Er hatte Watson 1992 als Direktor des NCHGR abgelöst, und ihm war der Gedanke an unveröffentlichte Daten, die als Ware gehandelt werden, ein Gräuel. Für ihn ging es nicht in erster Linie um Geld, sondern um die Wissenschaft. Collins und seine Kollegen aus den Universitäten beschlossen deshalb 1996 die sogenannten Bermuda-Prinzipien, die alle Teilnehmer an Genomprojekten verpflichteten, ihre Daten innerhalb von 24 Stunden einer allgemein zugänglichen Datenbank zur Verfügung zu stellen. Zahlreiche finanzstarke Stiftungen – allen voran der britische Wellcome Trust –, förderten diesen Gedanken ideell und materiell.\nVenter nimmt den Fehdehandschuh auf, indem er eine weitere Firma gründet, die er nach dem lateinischen Wort für Geschwindigkeit Celera nennt. Und er ist wirklich schnell! Im Mai 1998 schockiert er seine Kontrahenten, als er verkündet, er werde mit seinen Verfahren das menschliche Genom innerhalb von drei Jahren komplett sequenzieren. Er ist damit der erste Wissenschaftler, der einen Termin für die Erreichung dieses Ziels nennt, und wenn auch niemand behaupten sollte, dass Venter sein kühnes Versprechen eingelöst hat, so muss doch zugegeben werden, dass ohne seine ungewöhnliche Umtriebigkeit alles schleppender und gemütlicher zugegangen wäre.\nBei Celera stehen rund 300 automatische Informationslesemaschinen (Sequenziergeräte) – alle vom Typ ABI PRISM DNA Analyzer, der einige Hunderttausend Dollar kostet. Diese Geräte laufen mit voller Kapazität Tag und Nacht, was unter anderem die jährliche Rechnung des Elektrizitätswerkes auf die schwindelerregende Höhe von 1 Million Dollar bringt. Die kontinuierlich produzierten Sequenzdaten – in der Größenordnung von Terabytes – werden von speziell angefertigten Computern montiert, wobei die Berechnungen für die erste „assembly“ 500 Millionen Billionen Sequenzvergleiche nötig macht. Die abschließenden Berechnungen benötigten 64 Gigabytes an Speicherkapazität. Spätestes 1999 reagierten die aus öffentlichen Mitteln finanzierten Wissenschaftler auf diese Herausforderung. In einer groß angelegten weltweiten Kooperation mit Schwerpunkten im britischen und im amerikanischen Cambridge und wesentlichen Zuarbeiten aus Japan und Deutschland holten sie Chromosom für Chromosom und Sequenz für Sequenz Venters Vorsprung auf. Und im Juni 2000 entschlossen sich das öffentliche Genomprojekt und die private Celera-Initiative, ihre Daten zusammenzulegen, um gemeinsam von einer ersten umfassenden Analyse des menschlichen Genoms berichten zu können. Das humane Genom wurde dann zum ersten Mal im Februar 2001 publiziert. Allerdings waren auch danach noch viele Feinarbeiten und Sequenzangleichungen nötig. Im Frühjahr 2003 wurde das offizielle Ende des Humangenom-Projektes bekannt gegeben, pünktlich zum fünfzigsten Jahrestag der Entdeckung der Doppelhelix. Es war schon eine erstaunliche Geschichte, die sich in diesem halben Jahrhundert abgespielt hatte, von der Entdeckung einer phantastischen Struktur bis zur vollständigen Erkundung des wahrscheinlich kompliziertesten Moleküls, das die Natur „erfunden“ hat.\nÜbrigens – jetzt haben wir das humane Genom und wissen gar nicht, wessen DNA-Sequenz denn da vorliegt. Wessen Basenpaare kennen wir nun in der aufgereihten Form?\nDie Antwort lautet, dass es nicht einen einzelnen DNA-Spender, sondern eine Vielzahl von ihnen gibt. Sie haben sich aufgrund von Anzeigen in lokalen Zeitungen bei wissenschaftlichen Laboratorien gemeldet, wo ihnen Zellproben abgenommen wurden. Diese wurden sodann aufbewahrt und weitergegeben, sodass keiner mehr wissen konnte, mit wessen DNA er gearbeitet hat. Von Anfang an wurde darauf geachtet, Individuen zu nehmen, die sich verschiedenen Volksgruppen zugehörig fühlten. Im Humangenom stecken unter anderen die Sequenzen von afrikanischen, asiatischen, amerikanischen und europäischen Menschen. Natürlich unterscheiden sich die einzelnen Spender untereinander. Aber aus ihnen allen lässt sich die eine gemeinsame Sequenz – einen Konsensus-Sequenz – ableiten, die wir jetzt „menschlich“ nennen. So zeigt die Wissenschaft am Genom, dass es viel gibt, was uns eint, und nur wenig, was uns trennt.",
      "id": [
        "145847"
      ],
      "item": "Der Wettlauf um das humane Genom"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145848]\n[%Humangenom-Projekt]\nDie frühe Schätzung, dass das Humangenom-Pjekt drei Milliarden Dollar kosten würde, hat sich als erstaunlich zutreffend erwiesen, was eigentlich ein Grund für die Forscher sein sollte, stolz zu sein.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Die frühe Schätzung, dass das Humangenom-Pjekt drei Milliarden Dollar kosten würde, hat sich als erstaunlich zutreffend erwiesen, was eigentlich ein Grund für die Forscher sein sollte, stolz zu sein.",
      "id": [
        "145848"
      ],
      "item": "Humangenom-Projekt"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145849]\n[%Die Strategie Gilberts]\nDie Strategie Walter Gilberts war es, aus der Kartographie des menschlichen Genoms besonders interessante DNA-Abschnitte herauszulösen und zu analysieren und nach und nach diese bereits analysierten Abschnitte zu einem Gesamtbild des menschlichen Genoms zusammenzufügen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Die Strategie Walter Gilberts war es, aus der Kartographie des menschlichen Genoms besonders interessante DNA-Abschnitte herauszulösen und zu analysieren und nach und nach diese bereits analysierten Abschnitte zu einem Gesamtbild des menschlichen Genoms zusammenzufügen.",
      "id": [
        "145849"
      ],
      "item": "Die Strategie Gilberts"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145850]\n[%Shotgun-Sequenzierung]\nBei der Shotgun-Sequenzierung wird die DNA zunächst kloniert und grob zerteilt. Zusammenhängende DNA-Abschnitte, die sich an ihren Enden überlappen – die Experten sprechen von Contigs –, kann man durch erneute Zerlegung der kleineren Fragmente generieren, die jetzt zur Sequenzierung vorbereitet werden. Die Hauptaufgabe besteht zuletzt darin, die genetischen Informationen (DNA-Sequenzen) der Schrotschussfragmente in die richtige Reihenfolge zu bringen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Bei der Shotgun-Sequenzierung wird die DNA zunächst kloniert und grob zerteilt. Zusammenhängende DNA-Abschnitte, die sich an ihren Enden überlappen – die Experten sprechen von Contigs –, kann man durch erneute Zerlegung der kleineren Fragmente generieren, die jetzt zur Sequenzierung vorbereitet werden. Die Hauptaufgabe besteht zuletzt darin, die genetischen Informationen (DNA-Sequenzen) der Schrotschussfragmente in die richtige Reihenfolge zu bringen.",
      "id": [
        "145850"
      ],
      "item": "Shotgun-Sequenzierung"
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145851]\n[##Geheimnis des Lebens; In Erwartung einer neuen Wissenschaft]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 2,
      "progress": true,
      "toc": true,
      "id": [
        "145851"
      ],
      "title": "Geheimnis des Lebens; In Erwartung einer neuen Wissenschaft"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145852]\n[%Einleitung]\nAn der modernen Bioforschung, die sich mit Genomen befasst und die dazugehörige DNA erst sequenziert, dann ediert und annotiert und zuletzt mit anderen Folgen von genetischen Buchstaben vergleicht, fällt auf, dass ihre Vertreter sich nicht lange damit aufhalten, die erreichten Daten vorzustellen. Vielmehr lassen sie fast noch im gleichen Atemzug den Blick über den plötzlich beengenden Zaun der Sequenzenentschlüsselung schweifen, und verweisen auf das, was im Anschluss an diese Erfolge der Wissenschaft möglich wird. Vorträge über das humane Genom heißen immer gleich „The Human Genome and Beyond“, und bei diesen Gelegenheiten wird neben dem Genom immer noch etwas anderes beschworen. Da ist zunächst das Proteom. Damit ist eine Art Katalog all der Proteine gemeint, die in einer Zelle agieren und ein dynamisches Netzwerk bilden, das dann von der dazugehörigen Wissenschaft namens Proteomik erforscht wird, wie es in Analogie zu Genomik heißt. Und selbst wenn man noch nichts von dem Proteom in der Hand hat, so weiß man doch schon heute, dass danach noch nicht Schluss sein wird. Vielmehr wird die Aufmerksamkeit sich einem Transkriptom oder Metabolom zuwenden, das heißt der Liste aller DNA-Elemente, deren Informationen chemisch übertragen werden. Dies sind die Stoffwechselprodukte einer Zelle, die einzeln auch als Metabolite bekannt sind, was sich unschwer vom wissenschaftlichen Wort für den Stoffwechsel selbst, vom Metabolismus, ableitet. \nAuch wenn der inflationäre Gebrauch der Endsilbe „om“ vermutlich nicht von Dauer sein wird, so bleibt doch anzumerken, dass viele Genetiker und Mediziner mit solchen Worthülsen zu erkennen geben, dass sie sich in Erwartung einer neuen Art von Biowissenschaft befinden. Diese neue Entwicklungsstufe der Biowissenschaft soll die Folge der informationellen Durchmusterung der Genome des Lebendigen sein, die in unseren Tagen stattfindet. Historisch ist die Hoffnung auf etwas grundlegend Neues gerechtfertigt, denn im Verlauf der Geschichte ist es oft so gewesen, dass eine zunehmende Menge an Daten – zum Beispiel über die Zusammensetzung der Erdschichten oder über die Verteilung und Entfernung der Sterne – die Atmosphäre schufen, in der große Persönlichkeiten wie Charles Darwin oder Niels Bohr den gesammeltem Informationen einen Sinn unterlegen und ein neues Bild der Welt entwerfen konnten. Warum soll dies nicht wieder möglich sein?\nBohr ist 1885 in Kopenhagen geboren worden, und seinen großen Auftritt in der Physik kann man auf 1912 datieren. Die Ideen zu dem, was er damals vortrug, sind ihm in den Jahren zuvor gekommen – vielleicht schon 1910. Wir können uns also jemanden vorstellen, der 1985 geboren worden ist und sich in den nächsten Jahren so unkonventionell und originell mit den Daten der genetischen Objekte (Genome) befasst, wie es Bohr zu seiner Zeit mit den Daten der physikalischen Objekte von atomarer Größenordnung getan hat. Wenn dies der Fall ist, wird er oder sie uns in einigen Jahren erklären, was da zu sehen ist, wenn wir auf die Sequenzen eines evolutionären Genoms schauen, so wie Bohr uns erklärte, was da wirklich zu sehen ist, wenn wir auf die Spektren der Atome schauen. Was mag der erwartete Bohr der Biologie zur Zeit denken? Solange wir dies nicht wissen, bleibt uns nur, selbst den Versuch zu unternehmen, uns wenigstens ein erstes und vielleicht noch grobes Bild von den Genomen zu machen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "An der modernen Bioforschung, die sich mit Genomen befasst und die dazugehörige DNA erst sequenziert, dann ediert und annotiert und zuletzt mit anderen Folgen von genetischen Buchstaben vergleicht, fällt auf, dass ihre Vertreter sich nicht lange damit aufhalten, die erreichten Daten vorzustellen. Vielmehr lassen sie fast noch im gleichen Atemzug den Blick über den plötzlich beengenden Zaun der Sequenzenentschlüsselung schweifen, und verweisen auf das, was im Anschluss an diese Erfolge der Wissenschaft möglich wird. Vorträge über das humane Genom heißen immer gleich „The Human Genome and Beyond“, und bei diesen Gelegenheiten wird neben dem Genom immer noch etwas anderes beschworen. Da ist zunächst das Proteom. Damit ist eine Art Katalog all der Proteine gemeint, die in einer Zelle agieren und ein dynamisches Netzwerk bilden, das dann von der dazugehörigen Wissenschaft namens Proteomik erforscht wird, wie es in Analogie zu Genomik heißt. Und selbst wenn man noch nichts von dem Proteom in der Hand hat, so weiß man doch schon heute, dass danach noch nicht Schluss sein wird. Vielmehr wird die Aufmerksamkeit sich einem Transkriptom oder Metabolom zuwenden, das heißt der Liste aller DNA-Elemente, deren Informationen chemisch übertragen werden. Dies sind die Stoffwechselprodukte einer Zelle, die einzeln auch als Metabolite bekannt sind, was sich unschwer vom wissenschaftlichen Wort für den Stoffwechsel selbst, vom Metabolismus, ableitet. \nAuch wenn der inflationäre Gebrauch der Endsilbe „om“ vermutlich nicht von Dauer sein wird, so bleibt doch anzumerken, dass viele Genetiker und Mediziner mit solchen Worthülsen zu erkennen geben, dass sie sich in Erwartung einer neuen Art von Biowissenschaft befinden. Diese neue Entwicklungsstufe der Biowissenschaft soll die Folge der informationellen Durchmusterung der Genome des Lebendigen sein, die in unseren Tagen stattfindet. Historisch ist die Hoffnung auf etwas grundlegend Neues gerechtfertigt, denn im Verlauf der Geschichte ist es oft so gewesen, dass eine zunehmende Menge an Daten – zum Beispiel über die Zusammensetzung der Erdschichten oder über die Verteilung und Entfernung der Sterne – die Atmosphäre schufen, in der große Persönlichkeiten wie Charles Darwin oder Niels Bohr den gesammeltem Informationen einen Sinn unterlegen und ein neues Bild der Welt entwerfen konnten. Warum soll dies nicht wieder möglich sein?\nBohr ist 1885 in Kopenhagen geboren worden, und seinen großen Auftritt in der Physik kann man auf 1912 datieren. Die Ideen zu dem, was er damals vortrug, sind ihm in den Jahren zuvor gekommen – vielleicht schon 1910. Wir können uns also jemanden vorstellen, der 1985 geboren worden ist und sich in den nächsten Jahren so unkonventionell und originell mit den Daten der genetischen Objekte (Genome) befasst, wie es Bohr zu seiner Zeit mit den Daten der physikalischen Objekte von atomarer Größenordnung getan hat. Wenn dies der Fall ist, wird er oder sie uns in einigen Jahren erklären, was da zu sehen ist, wenn wir auf die Sequenzen eines evolutionären Genoms schauen, so wie Bohr uns erklärte, was da wirklich zu sehen ist, wenn wir auf die Spektren der Atome schauen. Was mag der erwartete Bohr der Biologie zur Zeit denken? Solange wir dies nicht wissen, bleibt uns nur, selbst den Versuch zu unternehmen, uns wenigstens ein erstes und vielleicht noch grobes Bild von den Genomen zu machen.",
      "id": [
        "145852"
      ],
      "item": "Einleitung"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145853]\n[%Einsichten in das Leben]\nVon Bertolt Brecht stammt die Bemerkung, dass die Lage, in der sich die Menschen in der ersten Hälfte des 20. Jahrhunderts befanden, dadurch so kompliziert wurde, dass die einfache Wiedergabe der Realität immer weniger über diese Realität aussagte. Was die Materie wirklich war, zeigte der Blick auf ihre Oberfläche nicht. Und was die Menschen wirklich dachten und wollten, zeigte sich nicht, wenn man ihr Aussehen in einem Schnappschuss festhielt. Die eigentliche Realität – so meinte der Autor – sei „in die Funktionale gerutscht“.\nDiese Worte zitiert Walter Benjamin in seiner __Kleinen Geschichte der Photographie__, denn die Wiedergabe der Realität, die Brecht meinte, ist ihre Reproduktion in Form fotographischer Aufnahmen. Wenn auf ihnen Menschen zu sehen sind, erkennen wir sie einzeln sofort wieder, ohne allerdings damit auch nur das Geringste über die Beziehungen auszumachen, deren Verflechtungen erst eine Gesellschaft – und damit die eigentlich wichtige Realität – hervorbringen.\nVielleicht lohnt es sich, an diese Einsicht zu denken, wenn man auf die Genomsequenzen schaut, die ja wie eine fotographische Wiedergabe der molekularen Wirklichkeit erscheinen und ebensowenig wie die angesprochenen Aufnahmen einer Kamera Auskunft über das Funktionieren der Gene geben. Sequenzen liefern ein Bild (ein Foto) von den Genen, ohne uns unmittelbar ein Bild (eine Vorstellung) von ihnen zu geben. Die Sequenzen zeigen keineswegs das Leben selbst, und die Aufgabe der Wissenschaft besteht deshalb darin, in den gesammelten Daten einen Sinn ausfindig zu machen.\nEine uralte Frage an die Biologie lautet, wie Leben entstanden sein kann, und eine mögliche Lösung dieses Problems liegt darin, dass man versucht, sich die einfachste Form auszudenken, in der Leben existieren kann. Die Genomsequenzen können leicht dazu verleiten, an diesem Spiel teilzunehmen, und so hat es einige Bemühungen gegeben, ein minimales Genom zu konstruieren, womit das Genom gemeint ist, das über die kleinste Zahl von Genen verfügt, die ein – dann sicher einzelliges – Wesen benötigt, um sich vermehren und an seiner Umwelt teilhaben zu können. Auf verschlungenen gedanklichen Wegen ist man dabei auf die Zahl 256 gekommen, die einige Leute weniger erfreut und mehr besorgt. Denn, wenn es Leben in dieser sehr kleinen Form geben kann, dann lässt es sich im Reagenzglas herstellen – eine Leistung, die vom Publikum sicher mit gemischten Gefühlen aufgenommen würde.\nWas an den Bemühungen um ein Minimalgenom auffällt und vielleicht sogar gefällt, ist die Zahl 256 selbst, die sich als 28 schreiben und sofort die Anhänger des Pythagoras aufhorchen lässt, denen als Zahlenmystiker bereits aufgefallen ist, dass das Leben mit vier Basen auskommt, die ihrerseits aus vier Atomen bestehen (Wasserstoff, Sauerstoff, Kohlenstoff, Stickstoff). „Alles ist Zahl“, lautet das Motto der Pythagoräer und ihrer modernen Nachfolger, und sie werden auch im Bereich der Genome fündig werden. Sie können sich darüber freuen, dass die Idee der Quanteninformation sogar eine Begründung für die heilige Vierzahl – die Tetraktys – in den Genen liefern kann. Was die Zahl oder die Zahlen angeht, so lautet eine der sicher noch für lange Zeit gestellten Fragen, wie viel Gene denn nun in einem Genom stecken oder angelegt sind. In der Literatur schwirren oft seltsame Angaben herum: der Fliege __Drosophila melanogaster__ werden nur rund 14 000 Gene zugewiesen, während in dem kleinen Wurm __Caenorhabdidtis elegans__ gut 18 000 Gene stecken sollen. Beim Menschen schwirren Zahlen umher, die etwas niedriger als 30 000 Gene liegen, die aber immer weniger ernst genommen werden.\nFrüher war bekanntlich alles besser, und diese Binsenweisheit verschont auch die Genetik nicht, für die man sagen kann, dass ihre alten Vertreter noch gewusst haben, was ein Gen ist. In der modernen Genomforschung ist dieser würdige Begriff allerdings unter die Räder des Fortschritts geraten. Das Gen, das früher ein elegantes Konzept zum Verstehen des Lebendigen war, wird heute als DNA-Sequenz erfasst, die gelesen („exprimiert“) werden kann. Es ist eine Art Text zwischen dem Startsignal und einem Stopp-Codon.\nVielleicht wäre es besser, man würde den Ausdruck „Gen“ unbenutzt lassen, wenn man Genomsequenzen analysiert und stattdessen von „genetischen Orten“ reden, also von Abschnitten der DNA. Hier könnten sich die Kenntnisse der alten Genetik mit den Ergebnissen der neuen Genomforschung treffen. Die alte Genetik begann ihre Arbeit mit sichtbaren (phänotypischen) Mutationen, und sie versuchte, deren Position auf den Chromosomen zu finden (Kartieren). Die neue Genetik beginnt ihre Arbeit mit (genotypischen) Zeichenfolgen des Genoms, von denen sie nicht weiß, wie sie sich auf das Erscheinungsbild des Organismus auswirken. Sie kann sie nur zählen, und dabei fallen oft seltsame Konstruktionen der Natur auf. So zeigt die Genomanalyse, dass Organismen auf höheren Stufen der evolutionären Leiter oft eng verwandte Sequenzen (Gene) besitzen, die ähnlichen, aber trotzdem leicht verschiedenen Aufgaben dienen. So gibt es in Wirbeltiergenomen drei unterschiedliche genetische Orte, mit deren Informationen der Bau eines Proteins durchgeführt wird, das in den Lehrbüchern Aldolase heißt. Die Aldolase gehört zum Grundinventar lebender Zellen. Sie spielt eine Rolle in der sogenannten Zellatmung, bei der Zucker aus der Nahrung (Glukose) zur Energiegewinnung umgeformt wird. Einen Schritt dieses Stoffwechsels katalysiert die Aldolase, die höhere Organismen in drei Varianten herstellen, vermutlich um auf jeden Fall einen Weg verfügbar zu haben, auf dem die nötige Energie zu gewinnen ist.\nIn der Fliege __Drosophila__ finden die Genomforscher zwar nur einen genetischen Ort für die Aldolase; ihre biochemischen Kollegen können ihnen aber sagen, dass die Zellen der Fliege die Mosaikstruktur des Aldolase-Gens unterschiedlich verarbeiten und dieses dadurch so nutzen können, dass auch ihnen drei Varianten des Proteins zur Verfügung stehen. Offenbar gehört dies zum genetischen Stil von __Drosophila__, denn auch bei den Proteinen mit Namen Myosin, aus denen unter anderem die Muskeln aufgebaut werden, bringt die Fliege alle benötigten Varianten durch eine Kombination der Bestandteile des entsprechenden Gens zustande. Im Gegensatz dazu hat der Wurm __C. elegans__ vier verschiedene Myosin-Gene in seinem Genom angelegt.\nOffenbar müssen die unterschiedlichen genetischen Strategien zur Herstellung einer vergleichbaren Vielfalt von Proteinen gut verstanden sein, bevor man sich ernsthaft an die Aufgabe macht, Ergebnisse aus der Genomanalyse sinnvoll zu deuten. Die Wissenschaft, die einmal eine Genomik werden will, steckt nach wie vor in den Kinderschuhen, und es wird noch einige Zeit dauern, bevor wir manches begreifen. Zum Beispiel die Beobachtung, dass Fliegen zur Herstellung einer bestimmten Klasse von Proteinen, die mit dem genetischen Material selbst in Wechselwirkung treten – sie heißen aufgrund ihrer Form und wegen des zusätzlichen Einbaus eines Metallatoms Zink-Finger-Proteine –, mehr als doppelt so viele Gene angelegt haben wie der Wurm. In __Drosophila__ zählen die Genetiker nämlich 352 Zink-Finger-Gene, während __C. elegans__ mit 132 zufrieden ist. Was in dieser Reihe das humane Genom angeht, so belassen es die Wissenschaftler nicht beim Staunen darüber, dass wir mehr als doppelt so viele Zink-Finger-Gene wie die Fliege haben, sondern fangen auch mit Versuchen an, dies genauer zu verstehen.\nZink-Finger-Proteine gehören ihrer Funktion nach zu der Gruppe von Transkriptionsfaktoren, was heißt, dass sie an dem Schritt beteiligt sind, bei dem eine DNA-Sequenz auf die Protein-Hardware der Zellen übertragen wird. Sie führen die Transkription dabei zwar nicht selbst durch, sie regulieren oder steuern sie aber. Zink-Finger-Proteine können Gene an- und abschalten, und es besteht schon lange der Verdacht, dass die entscheidenden Sequenzen für die Evolution des Lebens nicht in den proteinkodierenden Abschnitten, sondern in den dazugehörigen regulierenden Passagen zu finden sind. Die Offenlegung der Genomsequenzen erlaubt nun, diese Hypothese einem präzisen Test zu unterwerfen, der sicher in den nächsten Jahren durchgeführt werden wird.\nDie erste Analyse der Genome ergibt eine umfassende Liste der Proteine und ihrer Teile, die im Leben benutzt werden – so kann man nachlesen, dass menschliche Zellen dreißig Gene für Wachstumsfaktoren haben, die das Bindegewebe hervorbringen (Fibroblasten), und 765 Gene, die beim Zustandekommen von Antikörpern eine Rolle spielen. (Fliegen und Würmer haben im ersten Fall neun bzw. sechs und im zweiten Fall 140 bzw. 64 Gene.) Aber die entscheidende Information zum Verständnis der Lebensdynamik bleibt ein Geheimnis – so offen sie auch vor uns liegt. Gemeint ist die Sequenzinformation, die festlegt, wann und wo, für wie lange und unter welchen Umständen ein Gen ein- oder ausgeschaltet ist.\nDiese Steuerinformation kann den Sequenzdaten nicht entnommen werden, was allein deshalb zu bedauern ist, weil sie essentiell an der Evolution beteiligt sein muss. Die Entwicklung von Leben geschieht sicher weniger dadurch, dass es sich mehr Proteine zulegt, als dadurch, dass es mit den vorhandenen Proteinen raffinierter umgeht und ihre Verwendung moduliert.\nDie ersten Preise für Genomforscher werden vermutlich an die vergeben werden, die angeben können, welche Variationen von Proteinen für die Entwicklung des Lebens eine entscheidende Rolle spielen. Sie können aber auch an die Biologen fallen, die einen ersten Überblick über das Spektrum der genetischen Variationen liefern, die den menschlichen Genpool ausmachen.\nEs könnte sich hier um ein gar nicht so verzwicktes Problem handeln, da die Menschheit eine relativ junge Spezies ist. Die derzeit vorhandene Population von rund 6 Milliarden Menschen – so die genetisch gut begründete Ansicht der Anthropologen – stammt von einigen Tausend Vorfahren ab, die vor rund 200 000 Jahren in Afrika gelebt und sich von dort über alle Welt ausgebreitet haben. Von solch relativ kleinen Gruppen lässt sich annehmen, dass sie nur eine begrenzte Vielfalt in den Genen produzieren und bewahren kann. Man rechnet mit ein paar allgemein verbreiteten Varianten in den kodierenden Sequenzen eines jeden Gens in dem Genom, und die nur wenigen Tausend Generationen bis heute haben an diesem Spektrum sicher nicht sehr viel ändern können. Aus alldem kann man die Hoffnung schöpfen, dass es bald möglich wird, die gemeinsamen Varianten (Allele) aller menschlichen Gene zu katalogisieren.\nNeben der allgemeinen Evolution (Phylogenese) interessiert die Genomforscher noch die individuelle Entwicklung (Ontogenese) eines Lebewesens, also sein Weg von einer befruchteten Eizelle zu einem ausgereiften Erwachsenen. Aus der nahezu formlosen Eizelle wird in zahlreichen Entwicklungsstufen ein hochgradig gestalteter Organismus, und es ist keine Frage, dass das Genom mit seinen Informationen zu diesem Vorgang beiträgt. Aber wie macht es dies?\nWer diese Frage beantworten will, wird bald merken, dass neue Begriffe für das nötig werden, was Gene bzw. DNA-Sequenzen dabei bewirken. Gene informieren, Gene kodieren, Gene regulieren – aber reicht dieses Vokabular, um damit die regelmäßige Folge von Differenzierungen und Gestaltbildungen erklären zu können, die auf dem Weg von der Eizelle über den Embryo und den Fötus zum Organismus zu beobachten sind?",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Von Bertolt Brecht stammt die Bemerkung, dass die Lage, in der sich die Menschen in der ersten Hälfte des 20. Jahrhunderts befanden, dadurch so kompliziert wurde, dass die einfache Wiedergabe der Realität immer weniger über diese Realität aussagte. Was die Materie wirklich war, zeigte der Blick auf ihre Oberfläche nicht. Und was die Menschen wirklich dachten und wollten, zeigte sich nicht, wenn man ihr Aussehen in einem Schnappschuss festhielt. Die eigentliche Realität – so meinte der Autor – sei „in die Funktionale gerutscht“.\nDiese Worte zitiert Walter Benjamin in seiner __Kleinen Geschichte der Photographie__, denn die Wiedergabe der Realität, die Brecht meinte, ist ihre Reproduktion in Form fotographischer Aufnahmen. Wenn auf ihnen Menschen zu sehen sind, erkennen wir sie einzeln sofort wieder, ohne allerdings damit auch nur das Geringste über die Beziehungen auszumachen, deren Verflechtungen erst eine Gesellschaft – und damit die eigentlich wichtige Realität – hervorbringen.\nVielleicht lohnt es sich, an diese Einsicht zu denken, wenn man auf die Genomsequenzen schaut, die ja wie eine fotographische Wiedergabe der molekularen Wirklichkeit erscheinen und ebensowenig wie die angesprochenen Aufnahmen einer Kamera Auskunft über das Funktionieren der Gene geben. Sequenzen liefern ein Bild (ein Foto) von den Genen, ohne uns unmittelbar ein Bild (eine Vorstellung) von ihnen zu geben. Die Sequenzen zeigen keineswegs das Leben selbst, und die Aufgabe der Wissenschaft besteht deshalb darin, in den gesammelten Daten einen Sinn ausfindig zu machen.\nEine uralte Frage an die Biologie lautet, wie Leben entstanden sein kann, und eine mögliche Lösung dieses Problems liegt darin, dass man versucht, sich die einfachste Form auszudenken, in der Leben existieren kann. Die Genomsequenzen können leicht dazu verleiten, an diesem Spiel teilzunehmen, und so hat es einige Bemühungen gegeben, ein minimales Genom zu konstruieren, womit das Genom gemeint ist, das über die kleinste Zahl von Genen verfügt, die ein – dann sicher einzelliges – Wesen benötigt, um sich vermehren und an seiner Umwelt teilhaben zu können. Auf verschlungenen gedanklichen Wegen ist man dabei auf die Zahl 256 gekommen, die einige Leute weniger erfreut und mehr besorgt. Denn, wenn es Leben in dieser sehr kleinen Form geben kann, dann lässt es sich im Reagenzglas herstellen – eine Leistung, die vom Publikum sicher mit gemischten Gefühlen aufgenommen würde.\nWas an den Bemühungen um ein Minimalgenom auffällt und vielleicht sogar gefällt, ist die Zahl 256 selbst, die sich als 28 schreiben und sofort die Anhänger des Pythagoras aufhorchen lässt, denen als Zahlenmystiker bereits aufgefallen ist, dass das Leben mit vier Basen auskommt, die ihrerseits aus vier Atomen bestehen (Wasserstoff, Sauerstoff, Kohlenstoff, Stickstoff). „Alles ist Zahl“, lautet das Motto der Pythagoräer und ihrer modernen Nachfolger, und sie werden auch im Bereich der Genome fündig werden. Sie können sich darüber freuen, dass die Idee der Quanteninformation sogar eine Begründung für die heilige Vierzahl – die Tetraktys – in den Genen liefern kann. Was die Zahl oder die Zahlen angeht, so lautet eine der sicher noch für lange Zeit gestellten Fragen, wie viel Gene denn nun in einem Genom stecken oder angelegt sind. In der Literatur schwirren oft seltsame Angaben herum: der Fliege __Drosophila melanogaster__ werden nur rund 14 000 Gene zugewiesen, während in dem kleinen Wurm __Caenorhabdidtis elegans__ gut 18 000 Gene stecken sollen. Beim Menschen schwirren Zahlen umher, die etwas niedriger als 30 000 Gene liegen, die aber immer weniger ernst genommen werden.\nFrüher war bekanntlich alles besser, und diese Binsenweisheit verschont auch die Genetik nicht, für die man sagen kann, dass ihre alten Vertreter noch gewusst haben, was ein Gen ist. In der modernen Genomforschung ist dieser würdige Begriff allerdings unter die Räder des Fortschritts geraten. Das Gen, das früher ein elegantes Konzept zum Verstehen des Lebendigen war, wird heute als DNA-Sequenz erfasst, die gelesen („exprimiert“) werden kann. Es ist eine Art Text zwischen dem Startsignal und einem Stopp-Codon.\nVielleicht wäre es besser, man würde den Ausdruck „Gen“ unbenutzt lassen, wenn man Genomsequenzen analysiert und stattdessen von „genetischen Orten“ reden, also von Abschnitten der DNA. Hier könnten sich die Kenntnisse der alten Genetik mit den Ergebnissen der neuen Genomforschung treffen. Die alte Genetik begann ihre Arbeit mit sichtbaren (phänotypischen) Mutationen, und sie versuchte, deren Position auf den Chromosomen zu finden (Kartieren). Die neue Genetik beginnt ihre Arbeit mit (genotypischen) Zeichenfolgen des Genoms, von denen sie nicht weiß, wie sie sich auf das Erscheinungsbild des Organismus auswirken. Sie kann sie nur zählen, und dabei fallen oft seltsame Konstruktionen der Natur auf. So zeigt die Genomanalyse, dass Organismen auf höheren Stufen der evolutionären Leiter oft eng verwandte Sequenzen (Gene) besitzen, die ähnlichen, aber trotzdem leicht verschiedenen Aufgaben dienen. So gibt es in Wirbeltiergenomen drei unterschiedliche genetische Orte, mit deren Informationen der Bau eines Proteins durchgeführt wird, das in den Lehrbüchern Aldolase heißt. Die Aldolase gehört zum Grundinventar lebender Zellen. Sie spielt eine Rolle in der sogenannten Zellatmung, bei der Zucker aus der Nahrung (Glukose) zur Energiegewinnung umgeformt wird. Einen Schritt dieses Stoffwechsels katalysiert die Aldolase, die höhere Organismen in drei Varianten herstellen, vermutlich um auf jeden Fall einen Weg verfügbar zu haben, auf dem die nötige Energie zu gewinnen ist.\nIn der Fliege __Drosophila__ finden die Genomforscher zwar nur einen genetischen Ort für die Aldolase; ihre biochemischen Kollegen können ihnen aber sagen, dass die Zellen der Fliege die Mosaikstruktur des Aldolase-Gens unterschiedlich verarbeiten und dieses dadurch so nutzen können, dass auch ihnen drei Varianten des Proteins zur Verfügung stehen. Offenbar gehört dies zum genetischen Stil von __Drosophila__, denn auch bei den Proteinen mit Namen Myosin, aus denen unter anderem die Muskeln aufgebaut werden, bringt die Fliege alle benötigten Varianten durch eine Kombination der Bestandteile des entsprechenden Gens zustande. Im Gegensatz dazu hat der Wurm __C. elegans__ vier verschiedene Myosin-Gene in seinem Genom angelegt.\nOffenbar müssen die unterschiedlichen genetischen Strategien zur Herstellung einer vergleichbaren Vielfalt von Proteinen gut verstanden sein, bevor man sich ernsthaft an die Aufgabe macht, Ergebnisse aus der Genomanalyse sinnvoll zu deuten. Die Wissenschaft, die einmal eine Genomik werden will, steckt nach wie vor in den Kinderschuhen, und es wird noch einige Zeit dauern, bevor wir manches begreifen. Zum Beispiel die Beobachtung, dass Fliegen zur Herstellung einer bestimmten Klasse von Proteinen, die mit dem genetischen Material selbst in Wechselwirkung treten – sie heißen aufgrund ihrer Form und wegen des zusätzlichen Einbaus eines Metallatoms Zink-Finger-Proteine –, mehr als doppelt so viele Gene angelegt haben wie der Wurm. In __Drosophila__ zählen die Genetiker nämlich 352 Zink-Finger-Gene, während __C. elegans__ mit 132 zufrieden ist. Was in dieser Reihe das humane Genom angeht, so belassen es die Wissenschaftler nicht beim Staunen darüber, dass wir mehr als doppelt so viele Zink-Finger-Gene wie die Fliege haben, sondern fangen auch mit Versuchen an, dies genauer zu verstehen.\nZink-Finger-Proteine gehören ihrer Funktion nach zu der Gruppe von Transkriptionsfaktoren, was heißt, dass sie an dem Schritt beteiligt sind, bei dem eine DNA-Sequenz auf die Protein-Hardware der Zellen übertragen wird. Sie führen die Transkription dabei zwar nicht selbst durch, sie regulieren oder steuern sie aber. Zink-Finger-Proteine können Gene an- und abschalten, und es besteht schon lange der Verdacht, dass die entscheidenden Sequenzen für die Evolution des Lebens nicht in den proteinkodierenden Abschnitten, sondern in den dazugehörigen regulierenden Passagen zu finden sind. Die Offenlegung der Genomsequenzen erlaubt nun, diese Hypothese einem präzisen Test zu unterwerfen, der sicher in den nächsten Jahren durchgeführt werden wird.\nDie erste Analyse der Genome ergibt eine umfassende Liste der Proteine und ihrer Teile, die im Leben benutzt werden – so kann man nachlesen, dass menschliche Zellen dreißig Gene für Wachstumsfaktoren haben, die das Bindegewebe hervorbringen (Fibroblasten), und 765 Gene, die beim Zustandekommen von Antikörpern eine Rolle spielen. (Fliegen und Würmer haben im ersten Fall neun bzw. sechs und im zweiten Fall 140 bzw. 64 Gene.) Aber die entscheidende Information zum Verständnis der Lebensdynamik bleibt ein Geheimnis – so offen sie auch vor uns liegt. Gemeint ist die Sequenzinformation, die festlegt, wann und wo, für wie lange und unter welchen Umständen ein Gen ein- oder ausgeschaltet ist.\nDiese Steuerinformation kann den Sequenzdaten nicht entnommen werden, was allein deshalb zu bedauern ist, weil sie essentiell an der Evolution beteiligt sein muss. Die Entwicklung von Leben geschieht sicher weniger dadurch, dass es sich mehr Proteine zulegt, als dadurch, dass es mit den vorhandenen Proteinen raffinierter umgeht und ihre Verwendung moduliert.\nDie ersten Preise für Genomforscher werden vermutlich an die vergeben werden, die angeben können, welche Variationen von Proteinen für die Entwicklung des Lebens eine entscheidende Rolle spielen. Sie können aber auch an die Biologen fallen, die einen ersten Überblick über das Spektrum der genetischen Variationen liefern, die den menschlichen Genpool ausmachen.\nEs könnte sich hier um ein gar nicht so verzwicktes Problem handeln, da die Menschheit eine relativ junge Spezies ist. Die derzeit vorhandene Population von rund 6 Milliarden Menschen – so die genetisch gut begründete Ansicht der Anthropologen – stammt von einigen Tausend Vorfahren ab, die vor rund 200 000 Jahren in Afrika gelebt und sich von dort über alle Welt ausgebreitet haben. Von solch relativ kleinen Gruppen lässt sich annehmen, dass sie nur eine begrenzte Vielfalt in den Genen produzieren und bewahren kann. Man rechnet mit ein paar allgemein verbreiteten Varianten in den kodierenden Sequenzen eines jeden Gens in dem Genom, und die nur wenigen Tausend Generationen bis heute haben an diesem Spektrum sicher nicht sehr viel ändern können. Aus alldem kann man die Hoffnung schöpfen, dass es bald möglich wird, die gemeinsamen Varianten (Allele) aller menschlichen Gene zu katalogisieren.\nNeben der allgemeinen Evolution (Phylogenese) interessiert die Genomforscher noch die individuelle Entwicklung (Ontogenese) eines Lebewesens, also sein Weg von einer befruchteten Eizelle zu einem ausgereiften Erwachsenen. Aus der nahezu formlosen Eizelle wird in zahlreichen Entwicklungsstufen ein hochgradig gestalteter Organismus, und es ist keine Frage, dass das Genom mit seinen Informationen zu diesem Vorgang beiträgt. Aber wie macht es dies?\nWer diese Frage beantworten will, wird bald merken, dass neue Begriffe für das nötig werden, was Gene bzw. DNA-Sequenzen dabei bewirken. Gene informieren, Gene kodieren, Gene regulieren – aber reicht dieses Vokabular, um damit die regelmäßige Folge von Differenzierungen und Gestaltbildungen erklären zu können, die auf dem Weg von der Eizelle über den Embryo und den Fötus zum Organismus zu beobachten sind?",
      "id": [
        "145853"
      ],
      "item": "Einsichten in das Leben"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145854]\n[%Stopp-Condon]\nEin Stopp-Codon ist ein Basentriplett des genetischen Codes. Solche Tripletts sind die Einheiten, in denen die Informationen der DNA an die Proteine der Zelle weitergegeben werden. Ein Stopp-Codon bezeichnet dabei das Ende einer Information.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Ein Stopp-Codon ist ein Basentriplett des genetischen Codes. Solche Tripletts sind die Einheiten, in denen die Informationen der DNA an die Proteine der Zelle weitergegeben werden. Ein Stopp-Codon bezeichnet dabei das Ende einer Information.",
      "id": [
        "145854"
      ],
      "item": "Stopp-Condon"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145855]\n[%Genpool]\nGenpool wird die Menge alles Gensequenzen genannt, die in einer Population gefunden werden. Es gibt also einen Genpool aller Menschen, einen Genpool der Säugetiere, der Wirbeltiere usw.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Genpool wird die Menge alles Gensequenzen genannt, die in einer Population gefunden werden. Es gibt also einen Genpool aller Menschen, einen Genpool der Säugetiere, der Wirbeltiere usw.",
      "id": [
        "145855"
      ],
      "item": "Genpool"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145856]\n[%Die DNA ist keine Software]\nBei der Frage nach der biologischen Entwicklung eines Lebewesens greifen die meisten Biologen auf eine Vorstellung aus den sechziger Jahren zurück Danach stellt sich die biologische Entwicklung als Folge eines genetischen Programms ein. Das Genom wird in diesem Bild als die Software gesehen, deren Hardware dann unser Körper mit all seinen molekularen Bauteilen ist. Die Nähe der Genomforschung zur Computerindustrie hat diesen Gedanken fest verankert und so populär werden lassen, dass die lebende Legende namens Bill Gates davon sprechen konnte, ein Gen bzw. ein Genom sei das raffinierteste Programm, das ihm je unter die Augen gekommen sei – „by far the most sophisticated program around“.\nZeigt der Blick auf das Genom tatsächlich die Software des Lebens? Es gibt Gründe, dies zu bezweifeln.\nDie Software eines Computers wie das Windowsprogramm, mit dem auch der Laptop funktioniert, auf dem gerade dieser Text geschrieben wird, entsteht in einer Vielzahl von Schritten. Zuerst legen die Programmierer in einer gegebenen Programmiersprache (bei Microsoft Windows trägt sie den Namen C++) einen Code fest; dann überführt ein sogenannter Compiler dieses Programm in eine anwendungsfähige Datei, so dass ein Computernutzer nichts selbst programmieren muss, sondern bequem mit seiner Arbeit beginnen kann.\nHalten wir also fest: Es gibt zwei verschiedene Dinge, einen Primärcode und eine komplexe Funktionseinheit, die durch einen bestimmten Code hervorgebracht wird, und zwar auf höchst verwickelte Weise mit einer eigenwilligen Sprache. Wenn wir lebendige Organismen betrachten, treffen wir in ihnen und ihren Zellen auf eine Art Compiler in Form all der Regeln, denen die biochemischen Prozesse unterliegen, die etwa zur Embryogenese gehören. \nWenn es nun ein Windows-Genom-Projekt in Analogie zum Humangenom-Projekt gäbe, bestünde das erste Ziel darin, den C++-Primärcode von Microsoft bis zum letzen Bit zu entschlüsseln. Die nächste Frage aber wäre, was man wüsste, wenn dies gelungen wäre. Die Antwort darauf hängt wiederum davon ab, ob bekannt ist, wie der Compiler funktioniert. Wenn dies nicht der Fall ist, hätten die Forscher vor allem Kauderwelsch vor Augen, in dem sie bestenfalls grobe Strukturen erkennen und zum Beispiel ermitteln könnten, wie sich alte Windows- Versionen von neuen unterscheiden. Sie könnten vielleicht auch sagen, welche Teile des Codes dafür sorgen, dass Informationen auf dem Bildschirm angezeigt werden, aber mehr ist nicht möglich.\nDen Compiler, also die vielen Ausführungsregeln des Lebens in einer Zelle, aber kennen wir kaum ansatzweise, und es ist nicht davon auszugehen, dass er so funktioniert wie der Compiler bei Windows.\nVielleicht kämen die Forscher weiter, wenn sie sich entschließen könnten, auf den Programmbegriff, auf die 1:1-Analogie mit dem Computer, zu verzichten.\nDenn was soll es heißen, wenn man sagt, eine Eizelle spult ein Programm ab, wenn sie Instruktionen zum Bau eines Organismus gibt? In dieser Konzeption muss es jemanden oder etwas geben, der oder das diese eintreffenden Anweisungen interpretiert und umsetzt. Dieses Etwas wiederum muss so unabhängig von den programmatischen Instruktionen sein wie ein Mechaniker von den Bauplänen des Autos, das er zusammensetzt. Damit stellt sich eine offenkundige Frage, die bislang ohne Antwort bleibt: Wer oder was ist der Mechaniker des Lebens, und wie ist er zustandegekommen? \nNatürlich kann man als Lösung vorschlagen, er sei von Anfang an da gewesen. Doch damit würde man nur das uralte vitalistische Kaninchen aus dem Zylinder ziehen und das Leben durch etwas Geheimnisvolles erklären, das selbst unerklärt bleibt, am besten das Leben selbst. Im Kontext der heutigen Wissenschaft ist eine solche „Erklärung“ nicht möglich. Hier muss auch der Mechaniker auf biochemische Weise entstanden sein, und auch um ihn zu erklären, können wir nur auf die Gene zurückgreifen. Sie müssen ihn gemacht haben, und damit muten wir den Genen – im Denkschema des Programms – etwas Unmögliches zu, nämlich etwas gemacht zu haben, bevor es sie selbst gegeben hat. Wir verwickeln uns in ein zirkuläres Argument, da (in der Computersprache) eine Software nicht auf einer Hardware laufen kann, für die sie noch keine Bauanleitung geliefert hat und die dennoch erst mit ihrer Hilfe hergestellt werden kann.\nEs ergibt keinen Sinn, das Leben als Computer zu betrachten und dessen Zweiteilung in Hardware und Software in die Biologie zu übertragen, etwa dadurch, dass man die Gene als Software und die Proteine als Hardware bezeichnet. Beim Computer existiert das Programm unabhängig von der Hardware. Man kann ein Gerät bekanntlich ohne Software kaufen. Und dieses Gerät ist darüber hinaus auf keinen Fall von einem der Programme hergestellt worden, die später auf ihm laufen.\nKein Leben – vor allem kein menschliches Leben – wird so nach Plan angefertigt, wie es bei einem Industrieprodukt geschieht, etwa einem Auto, einer Waschmaschine oder einem Computer. Will man die Entwicklung des Lebens verstehen, stiftet es nur Verwirrung, wenn man den Plan und seine Ausführung trennt. Beide gehören nämlich eng zusammen, wie die jüngsten Einsichten der Entwicklungsbiologen zeigen. Die Gene und ihre Auswirkungen gehören sogar so eng zusammen, dass man geneigt sein könnte, an Stelle des Maschinenbildes ein viel schöneres zu benutzen. Dieses Bild zeigt kein Programm, das abläuft, wenn Lebewesen, nicht zuletzt Menschen, entstehen. Es zeigt vielmehr so etwas wie einen Schöpfungsvorgang, wobei weniger die Kreativität eines Gottes, sondern die eines Künstlers gemeint ist. Vielleicht entstehen wir (und andere Lebensformen) so, wie die Werke eines Malers entstehen. Beim Malen fängt der Prozess des Hervorbringens mit einer Vorstellung im Kopf des Künstlers an, und seine Fortführung hängt von den Ergebnissen ab, die im Laufe der Bildentstehung auf der Leinwand sichtbar werden. Bei der Embryonalentwicklung fängt der Prozess mit Vorgaben im Kern der Zelle an, seine Fortführung hängt aber von den Bildungen ab, die im Laufe der Zeit entstehen, auf die auch die Umwelt wirkt und die wiederum auf die Umwelt zurückwirken.\nWer die Entstehung eines Bildes beschreibt und dabei den Macher vom Gemachten trennt, geht an der Sache vorbei. Dies gilt auch für die Entwicklung. Bei ihrer Beschreibung sollte man nicht versuchen, das Bildende von dem Gebildeten zu trennen, weil die Gene und ihre Produkte in kontinuierlicher Wechselwirkung stehen. Es ist übrigens dieses Zusammenspiel, das empfindlich gestört wird, wenn es an das Klonieren geht. Ein Menschenklon zum Beispiel muss dann ohne all die Kreativität auskommen, die das Leben im Verlauf der Evolution erwerben musste, um zu lernen, sich selbst hervorzubringen. Mit dem Klonieren fallen wir hinter die Geschichte des Lebens zurück, weshalb nur dringend von allen Experimenten in diese Richtung abgeraten werden kann.\nWenn wir einem Genom schon so etwas wie Kreativität zubilligen, dann können wir es auch mit einem Gehirn vergleichen, was konkret bedeutet, dass wir das, was Gehirne tun, mit dem vergleichen, was Genome tun. Gehirne erlauben es Menschen, eine Gesellschaft zu bilden, und Genome erlauben es Zellen, einen Organismus zu bilden. Das Genom als ein hochsensibles Zellorgan zu bezeichnen, das ungewohnte und unerwartete Ereignisse registriert und darauf reagiert, ist kein aus der Luft gegriffener Einfall des Autors, sondern ein Vorschlag, den die amerikanische Genetikern Barbara McClintock gemacht hat, als sie 1983 nach der Überreichung des Nobelpreises für Medizin die Gelegenheit hatte, ihre Sicht der Dinge einem breiten und aufmerksamen Publikum vorzustellen. Mit solchen Überlegungen ist das Genom „in die Funktionale gerutscht“, wie Brecht hätte sagen können. Allerdings ist bislang noch nicht genau zu erkennen, wo es dabei angekommen ist.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Bei der Frage nach der biologischen Entwicklung eines Lebewesens greifen die meisten Biologen auf eine Vorstellung aus den sechziger Jahren zurück Danach stellt sich die biologische Entwicklung als Folge eines genetischen Programms ein. Das Genom wird in diesem Bild als die Software gesehen, deren Hardware dann unser Körper mit all seinen molekularen Bauteilen ist. Die Nähe der Genomforschung zur Computerindustrie hat diesen Gedanken fest verankert und so populär werden lassen, dass die lebende Legende namens Bill Gates davon sprechen konnte, ein Gen bzw. ein Genom sei das raffinierteste Programm, das ihm je unter die Augen gekommen sei – „by far the most sophisticated program around“.\nZeigt der Blick auf das Genom tatsächlich die Software des Lebens? Es gibt Gründe, dies zu bezweifeln.\nDie Software eines Computers wie das Windowsprogramm, mit dem auch der Laptop funktioniert, auf dem gerade dieser Text geschrieben wird, entsteht in einer Vielzahl von Schritten. Zuerst legen die Programmierer in einer gegebenen Programmiersprache (bei Microsoft Windows trägt sie den Namen C++) einen Code fest; dann überführt ein sogenannter Compiler dieses Programm in eine anwendungsfähige Datei, so dass ein Computernutzer nichts selbst programmieren muss, sondern bequem mit seiner Arbeit beginnen kann.\nHalten wir also fest: Es gibt zwei verschiedene Dinge, einen Primärcode und eine komplexe Funktionseinheit, die durch einen bestimmten Code hervorgebracht wird, und zwar auf höchst verwickelte Weise mit einer eigenwilligen Sprache. Wenn wir lebendige Organismen betrachten, treffen wir in ihnen und ihren Zellen auf eine Art Compiler in Form all der Regeln, denen die biochemischen Prozesse unterliegen, die etwa zur Embryogenese gehören. \nWenn es nun ein Windows-Genom-Projekt in Analogie zum Humangenom-Projekt gäbe, bestünde das erste Ziel darin, den C++-Primärcode von Microsoft bis zum letzen Bit zu entschlüsseln. Die nächste Frage aber wäre, was man wüsste, wenn dies gelungen wäre. Die Antwort darauf hängt wiederum davon ab, ob bekannt ist, wie der Compiler funktioniert. Wenn dies nicht der Fall ist, hätten die Forscher vor allem Kauderwelsch vor Augen, in dem sie bestenfalls grobe Strukturen erkennen und zum Beispiel ermitteln könnten, wie sich alte Windows- Versionen von neuen unterscheiden. Sie könnten vielleicht auch sagen, welche Teile des Codes dafür sorgen, dass Informationen auf dem Bildschirm angezeigt werden, aber mehr ist nicht möglich.\nDen Compiler, also die vielen Ausführungsregeln des Lebens in einer Zelle, aber kennen wir kaum ansatzweise, und es ist nicht davon auszugehen, dass er so funktioniert wie der Compiler bei Windows.\nVielleicht kämen die Forscher weiter, wenn sie sich entschließen könnten, auf den Programmbegriff, auf die 1:1-Analogie mit dem Computer, zu verzichten.\nDenn was soll es heißen, wenn man sagt, eine Eizelle spult ein Programm ab, wenn sie Instruktionen zum Bau eines Organismus gibt? In dieser Konzeption muss es jemanden oder etwas geben, der oder das diese eintreffenden Anweisungen interpretiert und umsetzt. Dieses Etwas wiederum muss so unabhängig von den programmatischen Instruktionen sein wie ein Mechaniker von den Bauplänen des Autos, das er zusammensetzt. Damit stellt sich eine offenkundige Frage, die bislang ohne Antwort bleibt: Wer oder was ist der Mechaniker des Lebens, und wie ist er zustandegekommen? \nNatürlich kann man als Lösung vorschlagen, er sei von Anfang an da gewesen. Doch damit würde man nur das uralte vitalistische Kaninchen aus dem Zylinder ziehen und das Leben durch etwas Geheimnisvolles erklären, das selbst unerklärt bleibt, am besten das Leben selbst. Im Kontext der heutigen Wissenschaft ist eine solche „Erklärung“ nicht möglich. Hier muss auch der Mechaniker auf biochemische Weise entstanden sein, und auch um ihn zu erklären, können wir nur auf die Gene zurückgreifen. Sie müssen ihn gemacht haben, und damit muten wir den Genen – im Denkschema des Programms – etwas Unmögliches zu, nämlich etwas gemacht zu haben, bevor es sie selbst gegeben hat. Wir verwickeln uns in ein zirkuläres Argument, da (in der Computersprache) eine Software nicht auf einer Hardware laufen kann, für die sie noch keine Bauanleitung geliefert hat und die dennoch erst mit ihrer Hilfe hergestellt werden kann.\nEs ergibt keinen Sinn, das Leben als Computer zu betrachten und dessen Zweiteilung in Hardware und Software in die Biologie zu übertragen, etwa dadurch, dass man die Gene als Software und die Proteine als Hardware bezeichnet. Beim Computer existiert das Programm unabhängig von der Hardware. Man kann ein Gerät bekanntlich ohne Software kaufen. Und dieses Gerät ist darüber hinaus auf keinen Fall von einem der Programme hergestellt worden, die später auf ihm laufen.\nKein Leben – vor allem kein menschliches Leben – wird so nach Plan angefertigt, wie es bei einem Industrieprodukt geschieht, etwa einem Auto, einer Waschmaschine oder einem Computer. Will man die Entwicklung des Lebens verstehen, stiftet es nur Verwirrung, wenn man den Plan und seine Ausführung trennt. Beide gehören nämlich eng zusammen, wie die jüngsten Einsichten der Entwicklungsbiologen zeigen. Die Gene und ihre Auswirkungen gehören sogar so eng zusammen, dass man geneigt sein könnte, an Stelle des Maschinenbildes ein viel schöneres zu benutzen. Dieses Bild zeigt kein Programm, das abläuft, wenn Lebewesen, nicht zuletzt Menschen, entstehen. Es zeigt vielmehr so etwas wie einen Schöpfungsvorgang, wobei weniger die Kreativität eines Gottes, sondern die eines Künstlers gemeint ist. Vielleicht entstehen wir (und andere Lebensformen) so, wie die Werke eines Malers entstehen. Beim Malen fängt der Prozess des Hervorbringens mit einer Vorstellung im Kopf des Künstlers an, und seine Fortführung hängt von den Ergebnissen ab, die im Laufe der Bildentstehung auf der Leinwand sichtbar werden. Bei der Embryonalentwicklung fängt der Prozess mit Vorgaben im Kern der Zelle an, seine Fortführung hängt aber von den Bildungen ab, die im Laufe der Zeit entstehen, auf die auch die Umwelt wirkt und die wiederum auf die Umwelt zurückwirken.\nWer die Entstehung eines Bildes beschreibt und dabei den Macher vom Gemachten trennt, geht an der Sache vorbei. Dies gilt auch für die Entwicklung. Bei ihrer Beschreibung sollte man nicht versuchen, das Bildende von dem Gebildeten zu trennen, weil die Gene und ihre Produkte in kontinuierlicher Wechselwirkung stehen. Es ist übrigens dieses Zusammenspiel, das empfindlich gestört wird, wenn es an das Klonieren geht. Ein Menschenklon zum Beispiel muss dann ohne all die Kreativität auskommen, die das Leben im Verlauf der Evolution erwerben musste, um zu lernen, sich selbst hervorzubringen. Mit dem Klonieren fallen wir hinter die Geschichte des Lebens zurück, weshalb nur dringend von allen Experimenten in diese Richtung abgeraten werden kann.\nWenn wir einem Genom schon so etwas wie Kreativität zubilligen, dann können wir es auch mit einem Gehirn vergleichen, was konkret bedeutet, dass wir das, was Gehirne tun, mit dem vergleichen, was Genome tun. Gehirne erlauben es Menschen, eine Gesellschaft zu bilden, und Genome erlauben es Zellen, einen Organismus zu bilden. Das Genom als ein hochsensibles Zellorgan zu bezeichnen, das ungewohnte und unerwartete Ereignisse registriert und darauf reagiert, ist kein aus der Luft gegriffener Einfall des Autors, sondern ein Vorschlag, den die amerikanische Genetikern Barbara McClintock gemacht hat, als sie 1983 nach der Überreichung des Nobelpreises für Medizin die Gelegenheit hatte, ihre Sicht der Dinge einem breiten und aufmerksamen Publikum vorzustellen. Mit solchen Überlegungen ist das Genom „in die Funktionale gerutscht“, wie Brecht hätte sagen können. Allerdings ist bislang noch nicht genau zu erkennen, wo es dabei angekommen ist.",
      "id": [
        "145856"
      ],
      "item": "Die DNA ist keine Software"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145857]\n[%Barbara McClintock]\nDie amerikanerische Genetikerin Barbara McClintock (1902-1992) war eine der wenigen Frauen, die sich in der Mitte des 20. Jahrhunderts in der männlich dominierten naturwissenschaftlichen Spitzenforschung durchsetzen konnten.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Die amerikanerische Genetikerin Barbara McClintock (1902-1992) war eine der wenigen Frauen, die sich in der Mitte des 20. Jahrhunderts in der männlich dominierten naturwissenschaftlichen Spitzenforschung durchsetzen konnten.",
      "id": [
        "145857"
      ],
      "item": "Barbara McClintock"
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145858]\n[##Aussichten für das Leben; Gentests und medizinische Praxis]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 2,
      "progress": true,
      "toc": true,
      "id": [
        "145858"
      ],
      "title": "Aussichten für das Leben; Gentests und medizinische Praxis"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145859]\n[%Einführung]\nAls James Watson für kurze Zeit Direktor des Genomprojektes war und dem Unternehmen „National Center for Human Genome Research“ (NCHGR) den entscheidenden Anstoß verpasste, sorgte er scheinbar nebenbei auch dafür, dass man in Wissenschaft und Öffentlichkeit damit begann, sich auf die ethischen, juristischen und sozialen Folgen des neuen Wissens einzurichten. Auf einer Pressekonferenz erklärte Watson im Jahre 1990 – ohne vorherige Absprache mit Kollegen oder Vorgesetzten –, dass es mit zum Humanen Genomprojekt gehöre, die „ethical, legal, and social issues“ zu erörtern, und er hielt es für angemessen, für ein solches „ELSI“-Programm zwischen drei und fünf Prozent des Budgets aufzuwenden, das für Kartieren und Sequenzieren bereit gestellt wurde. Watsons Reputation in der Öffentlichkeit ließ nach dieser Verlautbarung jede kritische Stimme verstummen, und so bemühen sich seit diesen Tagen eine Vielzahl von Ethikern, Juristen und Soziologen darum, der Öffentlichkeit die Konsequenzen der Sequenzen vorzustellen.\nEs gibt viele Fragen, die in dem Zusammenhang auftauchen. Wie geht man fair mit genetischen Informationen um? Muss oder kann man diejenigen, die genetisch benachteiligt sind – was immer das bedeutet – finanziell oder auf andere Weise entschädigen? Wie sorgt man dafür, dass genetische Informationen privat bleiben? Wer darf meine Gendaten kennen? Meine Familie? Mein Arzt? Mein Arbeitgeber?\nNatürlich hängen die Antworten auf diese Fragen nicht zuletzt von dem Gehalt der Informationen ab, die bei genetischen Analysen – sogenannten Gentests – gewonnen werden können. Wenn Gentests einmal halten, was ihre Anbieter versprechen, dann werden sie in Zukunft einem Individuum erlauben, seine genetisch bedingte Anfälligkeit für Krankheiten zu ermitteln, einschließlich der Störungen eines gesunden und normalen Lebens, die erst im Alter auftreten. Gentests sind prädiktiv, wie man sagt, um sie von anderen Untersuchungen zu unterscheiden. Prädiktive Tests unternimmt man, während man noch gesund ist, und sie sagen einem (im Idealfall), wann man krank wird. Niemand soll sagen, dass es leicht ist, mit solch einer Information zu leben, mit der erneut ein Paradies der Unwissenheit verlassen wird. Dieser Schritt fällt vor allem dann nicht leicht, wenn es keine Möglichkeit gibt, der Krankheitsanfälligkeit mit Hilfe einer Therapie zu begegnen.\nNun kann man vorschlagen, prädiktive Gentests zu verbieten – was zum Beispiel auch die absehbaren Probleme mit privaten Lebensversicherungen beseitigen würde –, aber bereits im Jahr 2000 gab es mehr als 700 Gentests auf dem Markt, (Auf der amerikanischen Website Details www.genetests.org erhält man einen Einblick in das heutige Gentest-Business) und solche Zahlen entwickeln ihre kommerzielle Eigendynamik, auf die eine Gesellschaft vorbereitet sein sollte. Man kann noch so oft vom Recht auf Nichtwissen reden. Menschen streben nun einmal nach dem Gegenteil. Und wenn zum Beispiel die Gentests, mit denen die Länge von sogenannten Triplett-Repeats bestimmt werden kann, immer billiger und zuverlässiger werden, wird es mehr Menschen geben, die ihn durchführen, wenn ihre Familiensituation dies nahelegt. Es geht bei dem Test zum Beispiel um die Folge CAG in der Erbsubstanz, wobei alle Erkenntnis der Wissenschaft darauf hinausläuft, dass jemand, der diese Dreierkombination (Triplett) weniger als 39mal hintereinander an einer bekannten Stelle in seinem Genom hat, nicht von der Krankheit Huntington Chorea betroffen wird („normal“), während jemand, der mehr als 39 Wiederholungen von CAG aufweist, erkranken wird („abnormal“). Das Interesse an Nachweisen von Triplett-Repeats ist allein deshalb sehr gestiegen, weil sich herausgestellt hat, dass sie die molekulare Grundlage einer Reihe von menschlichen Krankheiten bilden, die das Nervensystem betreffen.\nGenetische Tests erlauben es unter anderem, die Disposition von Personen für bestimmte Krankheiten schon Jahrzehnte vor dem etwaigen Ausbruch der Krankheit zu erkennen, und es stellt sich die Frage, welche gesetzlichen Regelungen für genetische Diagnostik denkbar und nötig sind. Auch sie hängen von den Möglichkeiten ab, die sich für die Medizin aus der Kenntnis des humanen Genoms ergeben. Nach der Präsentation der Daten des humanen Genoms im Februar 2001 wurde zum Beispiel prognostiziert, dass bis zum Jahre 2030 eine umfassende genetische Gesundheitsversorgung die Norm sein und die durchschnittliche Lebenserwartung (in Nordamerika und Westeuropa) bei 90 Jahren liegen würde.\nIm Januar 2002 hat der damalige Leiter des Max-Delbrück-Centrums für Molekulare Medizin in Berlin-Buch (MDC), Detlev Ganten, eine große Zukunft für eine Biomedizin entworfen, die sich am Genom orientiert. Ganten rechnet unter anderem damit, dass die genetische Sequenzierung bis zum Jahre 2030 so zuverlässig und preisgünstig durchgeführt werden kann, dass sie als Massentechnologie zum Einsatz kommt und vielleicht sogar zu den Routineuntersuchungen gehört, die jeder über sich ergehen lassen muss, der in ein Krankenhaus eingeliefert wird. Gendiagnosen können schon heute sinnvolle Informationen für den Anästhesisten liefern, der durch DNA-Sequenzen auf Unverträglichkeiten hingewiesen wird. Ganten erwartet, dass bis zum Jahre 2040 das Gesundheitswesen „genombasiert“ sein und dadurch den Menschen individualisierte Präventionsmaßnahmen anbieten können wird.\nDiese Vorhersagen beruhen vor allem auf der Annahme, dass mit Hilfe der Genomsequenzen eine neue Wissenschaft begründet werden kann, die dem individuellen Genom eines Menschen Rechnung trägt, wenn er sein Medikament bekommt. Diese Wissenschaft heißt Pharmakogenetik oder Pharmakogenomik. \nEs geht – wie das Wort sagt – um die Erkundung der Verbindungen, die zwischen Arzneimitteln (Pharmaka) und den genetischen Vorgaben der individuellen Menschen bestehen, die sie einnehmen. Die Gene steuern in den Organen und ihren Zellen den Aufbau der Moleküle (Proteine), mit denen die Medikamente in Wechselwirkung treten und über die sie ihren Einfluss ausbreiten. Dabei entstehen oft nicht nur die erwünschten Wirkungen, sondern auch unerwünschte Nebenwirkungen, und schon seit langer Zeit versucht die Wissenschaft intensiv zu verstehen, wie das Verhältnis von Arznei und individueller Zellarchitektur für den Patienten optimiert werden kann.\nEs lohnt sich, dies genauer auszuführen. Menschen sind durch ihre Genome individuell verschieden, wie man heute sagen kann, wie aber zu Beginn des 20. Jahrhunderts nicht bekannt war. Damals war es für wissenschaftlich argumentierende Ärzte ein unlösbares Rätsel, wie das zustande kommt, was man organische Individualität einer Person oder eines Patienten nennen könnte. Natürlich sehen wir von außen alle verschieden aus, und auch die inneren Organe lassen sich mit geübtem Auge unterscheiden, wenn sie sichtbar gemacht werden. Aber wo finden sich diese Unterschiede, wenn man weiter in das Innere geht und die Moleküle in den Zellen betrachtet? Wo steckt die Individualität der Strukturen, mit deren Hilfe die Medikamente ihre Wirkung entfalten? \nDass es so etwas wie die „chemische Individualität“ einer einzelnen Person geben muss, hat zuerst der britische Arzt Archibald Garrod erkannt, der diesen Begriff kurz nach 1900 geprägt hat. Garrod erkannte sogar, dass die „chemische Individualität“ eines Menschen zu seinen vererbbaren Eigenschaften gehört und den Mendelschen Gesetzen folgt. Er forderte im ersten Jahrzehnt des 20. Jahrhunderts die Wissenschaft auf, alle Anstrengungen darauf zu richten, die Grundlage dieser genetischen Besonderheit zu finden, um sie für die Bemühungen der Medizin und für die Gesundheit der Menschen nutzbar zu machen.\nGarrods Wunsch kann heute – rund einhundert Jahre später – im Rahmen des Humangenom-Projekts erfüllt werden, dessen Sequenzen genau angeben können, worin die chemische Individualität eines Menschen besteht, die nicht nur seine Reaktionen auf Medikamente bestimmt. Das Schlüsselwort heißt dabei Vielgestaltigkeit – Polymorphismus –, womit die Entdeckung beschrieben wird, dass Gene von Menschen in unterschiedlichen Formen vorliegen, was konkret bedeutet, dass in der Erbsubstanz verschiedener Menschen Abschnitte aus DNA auftauchen, die zwar die gleiche Funktion haben, aber aus individuell abweichenden Basensequenzen bestehen.\nIm Anschluss an diese Entdeckung formiert sich außer der schon genannten Pharmakogenomik noch eine weitere neue Wissenschaft, die Toxikogenomik. Sie will die traditionelle Toxikologie erweitern und Fragen der Art beantworten, wie es kommt, dass die einen Menschen durch Umwelteinflüsse Schaden nehmen, während die anderen unbetroffen bleiben.\nDie Wissenschaftler suchen die Antwort in den polymorphen („vielgestaltigen“) Genen, wobei sich das Interesse der Pharmakogenetiker immer mehr auf Variationen in __einer einzelnen Position__ im genetischen Material (Genom) richtet. Der englische Ausdruck dafür klingt zunächst etwas lang und umständlich – single nucleotide polymorphism („Einzel-Nukleotid-Polymorphismus“). Die drei Wörter lassen sich aber durch ihre Anfangsbuchstaben einfach als SNP abkürzen, und diese Kombination kann sogar als einsilbiges „Snip“ ausgesprochen werden. Es lohnt sich, sich diesen Begriff zu merken, weil derzeit alle pharmakogenetischen Überlegungen von ihm ausgehen.\nHinweise auf Existenz und Auswirkung von relevanten individuellen Genvarianten haben biogenetisch orientierte Ärzte schon vor Jahren gefunden, als man Krankheiten untersuchte, die durch Veränderungen in einzelnen Genen zustande kommen (die sogenannten monogenetischen Krankheiten). Vor allem bei der Muskoviszidose, die durch typische Ansammlungen von zähem Schleim in der Lunge charakterisiert ist, konnte gezeigt werden, dass es individuelle Polymorphismen auf genetischer Grundlage gab, die zu unterschiedlichen Ausprägungen der Krankheit führten. Inzwischen weiß man, dass Mannigfaltigkeiten dieser Art auch bei Gesundheitsstörungen eine Rolle spielen, die nicht durch einen, sondern durch zahlreiche Faktoren (multifaktoriell) bedingt sind, wie dies zum Beispiel bei Morbus Alzheimer der Fall ist. Mit Beobachtungen dieser Art tauchte irgendwann unter den Genetikern der Gedanke auf, dass es sich lohnt, das Erbgut (Genom) eines Menschen durch seine SNPs zu beschreiben. Vielleicht lassen sich für jeden Menschen charakteristische Snip-Muster aufstellen, aus denen dann Schlüsse auf die individuelle Wirksamkeit gegebener Medikamente oder auf die entsprechende Anfälligkeit für bekannte Krankheiten möglich werden. Für die Umsetzung dieser Idee sprach die Erwartung, dass sich solche Snip-Muster im Gefolge des Humangenom-Projektes ergeben würden.\nWissenschaftler bezeichnen nicht jede individuell veränderte Position im Genom als Snip. Sie tun dies erst dann, wenn ein SNP in einer gegebenen Gruppe von Menschen (Population) mit einer Häufigkeit von mehr als 1% auftritt. Wenn diese Grenze nicht erreicht wird, handelt es sich bei den punktuellen Änderungen um zufällige Mutationen, die spontan aufgetreten sind und nicht an die Nachkommen weitergegeben werden. Anders als solche Punktmutationen sind die Snips stabil. Sie werden von einer Generation zur nächsten vererbt, und sie bleiben über viele solcher Folgen unverändert, wie die Genomforschung in den letzten Jahren zeigen konnte.\nIm Humangenom findet man ein SNP etwa alle 500 bis 1000 Basenpaare. Bei drei Milliarden Basenpaaren insgesamt bedeutet dies, dass jeder Mensch drei bis sechs Millionen Snips in seinem Erbgut mit sich trägt. Die Zeichen der Individualität sind dabei nicht gleichmäßig auf alle Chromosomen verteilt, vielmehr scheinen sie ab und zu gehäuft aufzutreten, was in der vornehmen Sprache der Wissenschaft mit dem englischen Wort für Haufen – also Cluster – ausgedrückt wird.\nWichtig ist noch der genaue Ort, an dem ein SNP lokalisiert ist. Die meisten SNPs bzw. die dazugehörenden Cluster befinden sich in nicht-kodierenden DNA-Stücken, was im Normalfall bedeutet, dass sie keine nachweisbaren Veränderungen im Organismus hervorrufen. Wissenschaftler interessieren sich aus naheliegenden Gründen vor allem für Snips in kodierenden und regulierenden Genregionen, denn diese können sich sehr wohl auswirken und ihren Träger belasten. So wurde in dem Kontrollabschnitt eines Gens mit Namen TNF-Alpha ein Snip gefunden, der ein erhöhtes Risiko nach sich zieht, an Malaria zu erkranken. Und weiter konnten inzwischen 560 Snips in rund 100 Genen identifiziert werden, die sich bei der koronarer Herzkrankheit, Diabetes mellitus Typ 2 und Schizophrenie auswirken.\nDiese und andere Snips werden nicht nur in Hinblick auf die möglichen Beiträge zur Auslösung von Krankheiten geprüft. Es wird auch untersucht, wie ihre Struktur die Proteine des Körpers beeinflusst, die für den Abbau eines Medikaments in den Zellen zuständig sind. Man spricht dabei oft von „Entgiftungsenzymen“ und weiß, dass sie mit zur Verträglichkeit von Arzneimitteln beitragen. Es gibt im menschlichen Körper mehr als einhundert verschiedene Proteine, die Medikamente abbauen und in kleine Molekülbruchstücke zerlegen. Für alle diese Proteine gibt es Gene, und in jedem von ihnen ist ein SNP zu finden. Auf diese Weise wird leicht erklärbar, warum die Verträglichkeit einer Arznei eine höchst individuelle Eigenschaft ist, die im Rahmen der Pharmakogenetik erfasst und genutzt werden kann.\nViele Wissenschaftler schätzen heute, dass die SNPs für den größten Teil (90%) der genetischen Inhomogenität unter den Menschen verantwortlich sind und insofern auch maßgeblich zu den sichtbaren (phänotypischen) Unterschieden zwischen ihnen beitragen. Konkret können die individuellen Varianten in der Basensequenz erklären, warum spezifische Medikamente bei einigen Patienten helfen, während sie bei anderen wirkungslos bleiben oder gar unerwünschte Nebenerscheinungen mit sich bringen.\nMan hofft in Zukunft immer stärker SNPs als sichere Indikatoren für die Verträglichkeit und die Wirksamkeit von Medikamenten nutzen zu können. Darüber hinaus glaubt man, mit ihrer Hilfe Gene mit Krankheitswert besser und schneller erkennen zu können.\nEs ist klar, dass großes Interesse an Karten besteht, auf denen die SNPs der Menschen oder eines Menschen verzeichnet sind. Pharmaunternehmen und Universitäten haben unter finanzieller Mithilfe der britischen Stiftung __The Wellcome__ Trust ein SNP-Konsortium gegründet, das bereits Anfang September 2000 verkünden konnte, 800’000 SNPs identifiziert zu haben. Die entsprechenden Informationen und Karten sind öffentlich zugänglich.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Als James Watson für kurze Zeit Direktor des Genomprojektes war und dem Unternehmen „National Center for Human Genome Research“ (NCHGR) den entscheidenden Anstoß verpasste, sorgte er scheinbar nebenbei auch dafür, dass man in Wissenschaft und Öffentlichkeit damit begann, sich auf die ethischen, juristischen und sozialen Folgen des neuen Wissens einzurichten. Auf einer Pressekonferenz erklärte Watson im Jahre 1990 – ohne vorherige Absprache mit Kollegen oder Vorgesetzten –, dass es mit zum Humanen Genomprojekt gehöre, die „ethical, legal, and social issues“ zu erörtern, und er hielt es für angemessen, für ein solches „ELSI“-Programm zwischen drei und fünf Prozent des Budgets aufzuwenden, das für Kartieren und Sequenzieren bereit gestellt wurde. Watsons Reputation in der Öffentlichkeit ließ nach dieser Verlautbarung jede kritische Stimme verstummen, und so bemühen sich seit diesen Tagen eine Vielzahl von Ethikern, Juristen und Soziologen darum, der Öffentlichkeit die Konsequenzen der Sequenzen vorzustellen.\nEs gibt viele Fragen, die in dem Zusammenhang auftauchen. Wie geht man fair mit genetischen Informationen um? Muss oder kann man diejenigen, die genetisch benachteiligt sind – was immer das bedeutet – finanziell oder auf andere Weise entschädigen? Wie sorgt man dafür, dass genetische Informationen privat bleiben? Wer darf meine Gendaten kennen? Meine Familie? Mein Arzt? Mein Arbeitgeber?\nNatürlich hängen die Antworten auf diese Fragen nicht zuletzt von dem Gehalt der Informationen ab, die bei genetischen Analysen – sogenannten Gentests – gewonnen werden können. Wenn Gentests einmal halten, was ihre Anbieter versprechen, dann werden sie in Zukunft einem Individuum erlauben, seine genetisch bedingte Anfälligkeit für Krankheiten zu ermitteln, einschließlich der Störungen eines gesunden und normalen Lebens, die erst im Alter auftreten. Gentests sind prädiktiv, wie man sagt, um sie von anderen Untersuchungen zu unterscheiden. Prädiktive Tests unternimmt man, während man noch gesund ist, und sie sagen einem (im Idealfall), wann man krank wird. Niemand soll sagen, dass es leicht ist, mit solch einer Information zu leben, mit der erneut ein Paradies der Unwissenheit verlassen wird. Dieser Schritt fällt vor allem dann nicht leicht, wenn es keine Möglichkeit gibt, der Krankheitsanfälligkeit mit Hilfe einer Therapie zu begegnen.\nNun kann man vorschlagen, prädiktive Gentests zu verbieten – was zum Beispiel auch die absehbaren Probleme mit privaten Lebensversicherungen beseitigen würde –, aber bereits im Jahr 2000 gab es mehr als 700 Gentests auf dem Markt, (Auf der amerikanischen Website Details www.genetests.org erhält man einen Einblick in das heutige Gentest-Business) und solche Zahlen entwickeln ihre kommerzielle Eigendynamik, auf die eine Gesellschaft vorbereitet sein sollte. Man kann noch so oft vom Recht auf Nichtwissen reden. Menschen streben nun einmal nach dem Gegenteil. Und wenn zum Beispiel die Gentests, mit denen die Länge von sogenannten Triplett-Repeats bestimmt werden kann, immer billiger und zuverlässiger werden, wird es mehr Menschen geben, die ihn durchführen, wenn ihre Familiensituation dies nahelegt. Es geht bei dem Test zum Beispiel um die Folge CAG in der Erbsubstanz, wobei alle Erkenntnis der Wissenschaft darauf hinausläuft, dass jemand, der diese Dreierkombination (Triplett) weniger als 39mal hintereinander an einer bekannten Stelle in seinem Genom hat, nicht von der Krankheit Huntington Chorea betroffen wird („normal“), während jemand, der mehr als 39 Wiederholungen von CAG aufweist, erkranken wird („abnormal“). Das Interesse an Nachweisen von Triplett-Repeats ist allein deshalb sehr gestiegen, weil sich herausgestellt hat, dass sie die molekulare Grundlage einer Reihe von menschlichen Krankheiten bilden, die das Nervensystem betreffen.\nGenetische Tests erlauben es unter anderem, die Disposition von Personen für bestimmte Krankheiten schon Jahrzehnte vor dem etwaigen Ausbruch der Krankheit zu erkennen, und es stellt sich die Frage, welche gesetzlichen Regelungen für genetische Diagnostik denkbar und nötig sind. Auch sie hängen von den Möglichkeiten ab, die sich für die Medizin aus der Kenntnis des humanen Genoms ergeben. Nach der Präsentation der Daten des humanen Genoms im Februar 2001 wurde zum Beispiel prognostiziert, dass bis zum Jahre 2030 eine umfassende genetische Gesundheitsversorgung die Norm sein und die durchschnittliche Lebenserwartung (in Nordamerika und Westeuropa) bei 90 Jahren liegen würde.\nIm Januar 2002 hat der damalige Leiter des Max-Delbrück-Centrums für Molekulare Medizin in Berlin-Buch (MDC), Detlev Ganten, eine große Zukunft für eine Biomedizin entworfen, die sich am Genom orientiert. Ganten rechnet unter anderem damit, dass die genetische Sequenzierung bis zum Jahre 2030 so zuverlässig und preisgünstig durchgeführt werden kann, dass sie als Massentechnologie zum Einsatz kommt und vielleicht sogar zu den Routineuntersuchungen gehört, die jeder über sich ergehen lassen muss, der in ein Krankenhaus eingeliefert wird. Gendiagnosen können schon heute sinnvolle Informationen für den Anästhesisten liefern, der durch DNA-Sequenzen auf Unverträglichkeiten hingewiesen wird. Ganten erwartet, dass bis zum Jahre 2040 das Gesundheitswesen „genombasiert“ sein und dadurch den Menschen individualisierte Präventionsmaßnahmen anbieten können wird.\nDiese Vorhersagen beruhen vor allem auf der Annahme, dass mit Hilfe der Genomsequenzen eine neue Wissenschaft begründet werden kann, die dem individuellen Genom eines Menschen Rechnung trägt, wenn er sein Medikament bekommt. Diese Wissenschaft heißt Pharmakogenetik oder Pharmakogenomik. \nEs geht – wie das Wort sagt – um die Erkundung der Verbindungen, die zwischen Arzneimitteln (Pharmaka) und den genetischen Vorgaben der individuellen Menschen bestehen, die sie einnehmen. Die Gene steuern in den Organen und ihren Zellen den Aufbau der Moleküle (Proteine), mit denen die Medikamente in Wechselwirkung treten und über die sie ihren Einfluss ausbreiten. Dabei entstehen oft nicht nur die erwünschten Wirkungen, sondern auch unerwünschte Nebenwirkungen, und schon seit langer Zeit versucht die Wissenschaft intensiv zu verstehen, wie das Verhältnis von Arznei und individueller Zellarchitektur für den Patienten optimiert werden kann.\nEs lohnt sich, dies genauer auszuführen. Menschen sind durch ihre Genome individuell verschieden, wie man heute sagen kann, wie aber zu Beginn des 20. Jahrhunderts nicht bekannt war. Damals war es für wissenschaftlich argumentierende Ärzte ein unlösbares Rätsel, wie das zustande kommt, was man organische Individualität einer Person oder eines Patienten nennen könnte. Natürlich sehen wir von außen alle verschieden aus, und auch die inneren Organe lassen sich mit geübtem Auge unterscheiden, wenn sie sichtbar gemacht werden. Aber wo finden sich diese Unterschiede, wenn man weiter in das Innere geht und die Moleküle in den Zellen betrachtet? Wo steckt die Individualität der Strukturen, mit deren Hilfe die Medikamente ihre Wirkung entfalten? \nDass es so etwas wie die „chemische Individualität“ einer einzelnen Person geben muss, hat zuerst der britische Arzt Archibald Garrod erkannt, der diesen Begriff kurz nach 1900 geprägt hat. Garrod erkannte sogar, dass die „chemische Individualität“ eines Menschen zu seinen vererbbaren Eigenschaften gehört und den Mendelschen Gesetzen folgt. Er forderte im ersten Jahrzehnt des 20. Jahrhunderts die Wissenschaft auf, alle Anstrengungen darauf zu richten, die Grundlage dieser genetischen Besonderheit zu finden, um sie für die Bemühungen der Medizin und für die Gesundheit der Menschen nutzbar zu machen.\nGarrods Wunsch kann heute – rund einhundert Jahre später – im Rahmen des Humangenom-Projekts erfüllt werden, dessen Sequenzen genau angeben können, worin die chemische Individualität eines Menschen besteht, die nicht nur seine Reaktionen auf Medikamente bestimmt. Das Schlüsselwort heißt dabei Vielgestaltigkeit – Polymorphismus –, womit die Entdeckung beschrieben wird, dass Gene von Menschen in unterschiedlichen Formen vorliegen, was konkret bedeutet, dass in der Erbsubstanz verschiedener Menschen Abschnitte aus DNA auftauchen, die zwar die gleiche Funktion haben, aber aus individuell abweichenden Basensequenzen bestehen.\nIm Anschluss an diese Entdeckung formiert sich außer der schon genannten Pharmakogenomik noch eine weitere neue Wissenschaft, die Toxikogenomik. Sie will die traditionelle Toxikologie erweitern und Fragen der Art beantworten, wie es kommt, dass die einen Menschen durch Umwelteinflüsse Schaden nehmen, während die anderen unbetroffen bleiben.\nDie Wissenschaftler suchen die Antwort in den polymorphen („vielgestaltigen“) Genen, wobei sich das Interesse der Pharmakogenetiker immer mehr auf Variationen in __einer einzelnen Position__ im genetischen Material (Genom) richtet. Der englische Ausdruck dafür klingt zunächst etwas lang und umständlich – single nucleotide polymorphism („Einzel-Nukleotid-Polymorphismus“). Die drei Wörter lassen sich aber durch ihre Anfangsbuchstaben einfach als SNP abkürzen, und diese Kombination kann sogar als einsilbiges „Snip“ ausgesprochen werden. Es lohnt sich, sich diesen Begriff zu merken, weil derzeit alle pharmakogenetischen Überlegungen von ihm ausgehen.\nHinweise auf Existenz und Auswirkung von relevanten individuellen Genvarianten haben biogenetisch orientierte Ärzte schon vor Jahren gefunden, als man Krankheiten untersuchte, die durch Veränderungen in einzelnen Genen zustande kommen (die sogenannten monogenetischen Krankheiten). Vor allem bei der Muskoviszidose, die durch typische Ansammlungen von zähem Schleim in der Lunge charakterisiert ist, konnte gezeigt werden, dass es individuelle Polymorphismen auf genetischer Grundlage gab, die zu unterschiedlichen Ausprägungen der Krankheit führten. Inzwischen weiß man, dass Mannigfaltigkeiten dieser Art auch bei Gesundheitsstörungen eine Rolle spielen, die nicht durch einen, sondern durch zahlreiche Faktoren (multifaktoriell) bedingt sind, wie dies zum Beispiel bei Morbus Alzheimer der Fall ist. Mit Beobachtungen dieser Art tauchte irgendwann unter den Genetikern der Gedanke auf, dass es sich lohnt, das Erbgut (Genom) eines Menschen durch seine SNPs zu beschreiben. Vielleicht lassen sich für jeden Menschen charakteristische Snip-Muster aufstellen, aus denen dann Schlüsse auf die individuelle Wirksamkeit gegebener Medikamente oder auf die entsprechende Anfälligkeit für bekannte Krankheiten möglich werden. Für die Umsetzung dieser Idee sprach die Erwartung, dass sich solche Snip-Muster im Gefolge des Humangenom-Projektes ergeben würden.\nWissenschaftler bezeichnen nicht jede individuell veränderte Position im Genom als Snip. Sie tun dies erst dann, wenn ein SNP in einer gegebenen Gruppe von Menschen (Population) mit einer Häufigkeit von mehr als 1% auftritt. Wenn diese Grenze nicht erreicht wird, handelt es sich bei den punktuellen Änderungen um zufällige Mutationen, die spontan aufgetreten sind und nicht an die Nachkommen weitergegeben werden. Anders als solche Punktmutationen sind die Snips stabil. Sie werden von einer Generation zur nächsten vererbt, und sie bleiben über viele solcher Folgen unverändert, wie die Genomforschung in den letzten Jahren zeigen konnte.\nIm Humangenom findet man ein SNP etwa alle 500 bis 1000 Basenpaare. Bei drei Milliarden Basenpaaren insgesamt bedeutet dies, dass jeder Mensch drei bis sechs Millionen Snips in seinem Erbgut mit sich trägt. Die Zeichen der Individualität sind dabei nicht gleichmäßig auf alle Chromosomen verteilt, vielmehr scheinen sie ab und zu gehäuft aufzutreten, was in der vornehmen Sprache der Wissenschaft mit dem englischen Wort für Haufen – also Cluster – ausgedrückt wird.\nWichtig ist noch der genaue Ort, an dem ein SNP lokalisiert ist. Die meisten SNPs bzw. die dazugehörenden Cluster befinden sich in nicht-kodierenden DNA-Stücken, was im Normalfall bedeutet, dass sie keine nachweisbaren Veränderungen im Organismus hervorrufen. Wissenschaftler interessieren sich aus naheliegenden Gründen vor allem für Snips in kodierenden und regulierenden Genregionen, denn diese können sich sehr wohl auswirken und ihren Träger belasten. So wurde in dem Kontrollabschnitt eines Gens mit Namen TNF-Alpha ein Snip gefunden, der ein erhöhtes Risiko nach sich zieht, an Malaria zu erkranken. Und weiter konnten inzwischen 560 Snips in rund 100 Genen identifiziert werden, die sich bei der koronarer Herzkrankheit, Diabetes mellitus Typ 2 und Schizophrenie auswirken.\nDiese und andere Snips werden nicht nur in Hinblick auf die möglichen Beiträge zur Auslösung von Krankheiten geprüft. Es wird auch untersucht, wie ihre Struktur die Proteine des Körpers beeinflusst, die für den Abbau eines Medikaments in den Zellen zuständig sind. Man spricht dabei oft von „Entgiftungsenzymen“ und weiß, dass sie mit zur Verträglichkeit von Arzneimitteln beitragen. Es gibt im menschlichen Körper mehr als einhundert verschiedene Proteine, die Medikamente abbauen und in kleine Molekülbruchstücke zerlegen. Für alle diese Proteine gibt es Gene, und in jedem von ihnen ist ein SNP zu finden. Auf diese Weise wird leicht erklärbar, warum die Verträglichkeit einer Arznei eine höchst individuelle Eigenschaft ist, die im Rahmen der Pharmakogenetik erfasst und genutzt werden kann.\nViele Wissenschaftler schätzen heute, dass die SNPs für den größten Teil (90%) der genetischen Inhomogenität unter den Menschen verantwortlich sind und insofern auch maßgeblich zu den sichtbaren (phänotypischen) Unterschieden zwischen ihnen beitragen. Konkret können die individuellen Varianten in der Basensequenz erklären, warum spezifische Medikamente bei einigen Patienten helfen, während sie bei anderen wirkungslos bleiben oder gar unerwünschte Nebenerscheinungen mit sich bringen.\nMan hofft in Zukunft immer stärker SNPs als sichere Indikatoren für die Verträglichkeit und die Wirksamkeit von Medikamenten nutzen zu können. Darüber hinaus glaubt man, mit ihrer Hilfe Gene mit Krankheitswert besser und schneller erkennen zu können.\nEs ist klar, dass großes Interesse an Karten besteht, auf denen die SNPs der Menschen oder eines Menschen verzeichnet sind. Pharmaunternehmen und Universitäten haben unter finanzieller Mithilfe der britischen Stiftung __The Wellcome__ Trust ein SNP-Konsortium gegründet, das bereits Anfang September 2000 verkünden konnte, 800’000 SNPs identifiziert zu haben. Die entsprechenden Informationen und Karten sind öffentlich zugänglich.",
      "id": [
        "145859"
      ],
      "item": "Einführung"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145860]\n[%Triplett-Repeats]\nAls Triplett-Repeats (Wiederholungen der Tripletts von DNA-Basen) werden Erbkrankheiten bezeichnet, die auf unkontrollierte Wiederholungen von Tripletts in Genomabschnitten zurückgehen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Als Triplett-Repeats (Wiederholungen der Tripletts von DNA-Basen) werden Erbkrankheiten bezeichnet, die auf unkontrollierte Wiederholungen von Tripletts in Genomabschnitten zurückgehen.",
      "id": [
        "145860"
      ],
      "item": "Triplett-Repeats"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145861]\n[%Genchips]\nEine individuelle SNP-Analyse scheint für einen einzelnen Patienten viel zu teuer zu werden. Abhilfe an dieser Stelle erwartet man in Zukunft von den Genchips, die technisch DNA-Microarrays heißen und Informationen über die Aktivität einzelner Gene liefern.\nDie Technik ist in den 1990er Jahren entstanden und geht auf Arbeiten des amerikanischen Biologen Stephen Fodor zurück, der zur Entwicklung und Vermarktung von Genchips eine Firma namens Affymetrix gegründet hat. Microarrays werden mit kurzen DNA-Stücken ausgestattet, die das Spiegelbild eines Abschnittes aus dem Genom mit einem definierten SNP-Muster ausmachen. Wenn der untersuchte Patient dieses Muster besitzt, dockt seine DNA-Probe am Chip an. Dank zugesetzter spezieller Farbstoffe beginnt die entsprechende Position zu leuchten, ein Muster wird erkennbar und ein entsprechender Detektor erfasst das Ergebnis. Eine dazugehörende Datenbank – so könnte ein absehbares Szenarium der biomedizinischen Zukunft aussehen – verrät anschließend, welche Medikamente in welchen Kombinationen und Dosierungen der eben analysierten genetischen Ausstattung des Patienten entgegenkommen; sie prognostiziert die Erfolgsaussichten der Therapie und die Wahrscheinlichkeit unerwünschter Wirkungen.\nDer erste Genchip ist 1994 auf den Markt gekommen. Mit seiner Hilfe konnte festgestellt werden, ob das untersuchte genetische Material Abschnitte beherbergt, die zu dem als HIV bekannt gewordenen Virus gehören, der die Immunschwäche AIDS verursacht. In Deutschland hat sich im Jahre 2008 ein Nationales Genomforschungsnetz (NGFN) vorgenommen, 25 000 Patienten mit Genchips zu untersuchen, die mehr als eine halbe Million Snips (und andere Genvarianten) nachweisen können. Es soll um die genetisch feststellbaren Ursachen unter anderem von Übergewicht, Alzheimer-Krankheit und Schizophrenie gehen, und die Betreiber erwarten, damit „eine neue Welt der Genetik“ eröffnen zu können, deren Betreten neben molekularbiologischen Werkzeugen auch viel Informationstechnologie benötigt.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Eine individuelle SNP-Analyse scheint für einen einzelnen Patienten viel zu teuer zu werden. Abhilfe an dieser Stelle erwartet man in Zukunft von den Genchips, die technisch DNA-Microarrays heißen und Informationen über die Aktivität einzelner Gene liefern.\nDie Technik ist in den 1990er Jahren entstanden und geht auf Arbeiten des amerikanischen Biologen Stephen Fodor zurück, der zur Entwicklung und Vermarktung von Genchips eine Firma namens Affymetrix gegründet hat. Microarrays werden mit kurzen DNA-Stücken ausgestattet, die das Spiegelbild eines Abschnittes aus dem Genom mit einem definierten SNP-Muster ausmachen. Wenn der untersuchte Patient dieses Muster besitzt, dockt seine DNA-Probe am Chip an. Dank zugesetzter spezieller Farbstoffe beginnt die entsprechende Position zu leuchten, ein Muster wird erkennbar und ein entsprechender Detektor erfasst das Ergebnis. Eine dazugehörende Datenbank – so könnte ein absehbares Szenarium der biomedizinischen Zukunft aussehen – verrät anschließend, welche Medikamente in welchen Kombinationen und Dosierungen der eben analysierten genetischen Ausstattung des Patienten entgegenkommen; sie prognostiziert die Erfolgsaussichten der Therapie und die Wahrscheinlichkeit unerwünschter Wirkungen.\nDer erste Genchip ist 1994 auf den Markt gekommen. Mit seiner Hilfe konnte festgestellt werden, ob das untersuchte genetische Material Abschnitte beherbergt, die zu dem als HIV bekannt gewordenen Virus gehören, der die Immunschwäche AIDS verursacht. In Deutschland hat sich im Jahre 2008 ein Nationales Genomforschungsnetz (NGFN) vorgenommen, 25 000 Patienten mit Genchips zu untersuchen, die mehr als eine halbe Million Snips (und andere Genvarianten) nachweisen können. Es soll um die genetisch feststellbaren Ursachen unter anderem von Übergewicht, Alzheimer-Krankheit und Schizophrenie gehen, und die Betreiber erwarten, damit „eine neue Welt der Genetik“ eröffnen zu können, deren Betreten neben molekularbiologischen Werkzeugen auch viel Informationstechnologie benötigt.",
      "id": [
        "145861"
      ],
      "item": "Genchips"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145862]\n[%Microarrays]\nMicroarrays sind Analysevorrichtungen, die es erlauben, biochemisch codierte Informationen in computertaugliche Datenfolgen zu verwandeln.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Microarrays sind Analysevorrichtungen, die es erlauben, biochemisch codierte Informationen in computertaugliche Datenfolgen zu verwandeln.",
      "id": [
        "145862"
      ],
      "item": "Microarrays"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145863]\n[%Die Information der Gene]\nIm Rahmen der Pharmakogenetik (Wissenschaft, die dem individuellen Genom eines Menschen Rechnung trägt, wenn er seine Arznei bekommt) wird es besser möglich sein, Krebs zu definieren. Zurzeit teilen Ärzte die vielen unterschiedlichen Tumore nach den Organen oder Geweben ein, in denen sie auftreten. Sie sprechen etwa vom Brustkrebs oder von Hautkrebs und drücken dabei den Eindruck aus, dass hier völlig verschiedene Entwicklungen stattgefunden haben. In Zukunft kann etwas anderes versucht werden, nämlich für jeden Tumor ein SNP-Profil (single nucleotide polymorphism bzw. „Einzel-Nukleotid-Polymorphismus“) zu erstellen, und zwar in der Absicht oder in der Hoffnung, dabei angeben zu können, auf welche der vorhandenen Medikamente die entarteten Zellen reagieren und auf welche nicht. Entsprechend würde man nicht mehr von Brust- oder Hautkrebs, sondern von sensitiven oder insensitiven Tumoren sprechen, und unter dieser Vorgabe könnten sie gezielter behandelt werden.\nSo wichtig das Verständnis von Krankheiten mit Hilfe von Genominformationen ist und noch werden kann, es gilt daran zu denken, dass Gene nicht in erster Linie mit Krankheit zu tun haben. Genome sollen und können Menschen einzigartig machen, sie können aber bestimmt auch alle Menschen zusammen einzigartig machen und sie zum Beispiel sicher von den nächsten Verwandten, etwa den Schimpansen unterscheiden. Unser Erbgut und das der Menschenaffen ist zwar auf den ersten Blick ähnlicher als die dazugehörigen äußeren Gestalten, aber es werden sich Variationen finden lassen, die erklären, wodurch Menschen im Verlauf der Evolution den etwas anderen Weg gefunden haben, der uns zum Beispiel mit der Fähigkeit ausgestattet hat, Genome offenzulegen und die Fragen zu stellen, die hier gefragt werden.\nDas Genom ist wie die Literatur. Literatur ist nicht alles, aber alles kann man in ihr finden. Auch das Genom ist nicht alles, aber irgendwie muss alles in ihm zu finden sein.\nÜbrigens, da gerade von Literatur die Rede war: Viele Genetiker denken, dass mit der genetischen Information ein Text vorliegt, der von der Zelle gelesen wird. Wenn wir diesen Gedanken ernst nehmen, können wir neben der direkten (expliziten) Aussage der Gene noch eine indirekte (implizite) Auskunft erwarten. Die explizite Information eines Satzes „Ich fahre nach Hause“ hängt nur von den Buchstaben ab. Die implizite Information erschließt sich erst aus dem Kontext. Der Satz „Ich fahre nach Hause“ verrät nicht, welches Verkehrsmittel benutzt wird. Er bedeutet etwas anderes, ob er in einem Auto oder in der Straßenbahn geäußert wird. Entsprechend gilt: Die explizite Information eines Gensequenz hängt nur von ihr selbst ab; die implizite Information erschließt sich erst aus dem Kontext.\nNun ist klar, dass das Genom im Computer nur die explizite Information enthält. Den impliziten Teil, der sicher spannender und wahrscheinlich größer ist, liefert die Zelle, die der Computer nicht kennt – und wir auch nicht. Mit anderen Worten – die besten Informationen entgehen uns noch.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Im Rahmen der Pharmakogenetik (Wissenschaft, die dem individuellen Genom eines Menschen Rechnung trägt, wenn er seine Arznei bekommt) wird es besser möglich sein, Krebs zu definieren. Zurzeit teilen Ärzte die vielen unterschiedlichen Tumore nach den Organen oder Geweben ein, in denen sie auftreten. Sie sprechen etwa vom Brustkrebs oder von Hautkrebs und drücken dabei den Eindruck aus, dass hier völlig verschiedene Entwicklungen stattgefunden haben. In Zukunft kann etwas anderes versucht werden, nämlich für jeden Tumor ein SNP-Profil (single nucleotide polymorphism bzw. „Einzel-Nukleotid-Polymorphismus“) zu erstellen, und zwar in der Absicht oder in der Hoffnung, dabei angeben zu können, auf welche der vorhandenen Medikamente die entarteten Zellen reagieren und auf welche nicht. Entsprechend würde man nicht mehr von Brust- oder Hautkrebs, sondern von sensitiven oder insensitiven Tumoren sprechen, und unter dieser Vorgabe könnten sie gezielter behandelt werden.\nSo wichtig das Verständnis von Krankheiten mit Hilfe von Genominformationen ist und noch werden kann, es gilt daran zu denken, dass Gene nicht in erster Linie mit Krankheit zu tun haben. Genome sollen und können Menschen einzigartig machen, sie können aber bestimmt auch alle Menschen zusammen einzigartig machen und sie zum Beispiel sicher von den nächsten Verwandten, etwa den Schimpansen unterscheiden. Unser Erbgut und das der Menschenaffen ist zwar auf den ersten Blick ähnlicher als die dazugehörigen äußeren Gestalten, aber es werden sich Variationen finden lassen, die erklären, wodurch Menschen im Verlauf der Evolution den etwas anderen Weg gefunden haben, der uns zum Beispiel mit der Fähigkeit ausgestattet hat, Genome offenzulegen und die Fragen zu stellen, die hier gefragt werden.\nDas Genom ist wie die Literatur. Literatur ist nicht alles, aber alles kann man in ihr finden. Auch das Genom ist nicht alles, aber irgendwie muss alles in ihm zu finden sein.\nÜbrigens, da gerade von Literatur die Rede war: Viele Genetiker denken, dass mit der genetischen Information ein Text vorliegt, der von der Zelle gelesen wird. Wenn wir diesen Gedanken ernst nehmen, können wir neben der direkten (expliziten) Aussage der Gene noch eine indirekte (implizite) Auskunft erwarten. Die explizite Information eines Satzes „Ich fahre nach Hause“ hängt nur von den Buchstaben ab. Die implizite Information erschließt sich erst aus dem Kontext. Der Satz „Ich fahre nach Hause“ verrät nicht, welches Verkehrsmittel benutzt wird. Er bedeutet etwas anderes, ob er in einem Auto oder in der Straßenbahn geäußert wird. Entsprechend gilt: Die explizite Information eines Gensequenz hängt nur von ihr selbst ab; die implizite Information erschließt sich erst aus dem Kontext.\nNun ist klar, dass das Genom im Computer nur die explizite Information enthält. Den impliziten Teil, der sicher spannender und wahrscheinlich größer ist, liefert die Zelle, die der Computer nicht kennt – und wir auch nicht. Mit anderen Worten – die besten Informationen entgehen uns noch.",
      "id": [
        "145863"
      ],
      "item": "Die Information der Gene"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145864]\n[%Die persönlichen Informationen]\n„Was passiert, wenn jeder von uns sein Genom kennt?“ So lautet die Frage, um die es in Zukunft gehen könnte, und es empfiehlt sich, darauf vorbreitet zu sein. Es gibt zum Beispiel schon länger ein umfassendes „Personal Genome Project“, mit dem sein Initiator, George Church von der Harvard Universität, genetische Daten mit Informationen über das Erscheinungsbild (Phänotyp) und die Umwelt integrieren möchte. Er sucht seit 2007 eintausend Freiwillige, die ihm dabei helfen und ihre persönlichen Genomsequenzen als Daten zur Verfügung stellen und öffentlich nutzen lassen.\nDas Personal Genome Project zielt letzten Endes darauf ab, insgesamt 100'000 Freiwillige zu finden, die ihr Genom sequenzieren lassen und ins Web stellen bzw. die Daten im Internet zur Verfügung stellen. Was zunächst Verwunderung hervorgerufen hat, bekommt inzwischen Schwung. Tatsächlich haben zehn Prominente sich bereit erklärt, ihre Gene auszustellen. Zu ihnen gehören der Neurobiologe und Bestsellerautor Steven Pinker sowie Sergey Brin, ein Mitbegründer von Google. Im Anschluss an diese Prominenten sind die ersten 5000 Anmeldungen bei George Church eingegangen. Gefragt, ob sie ihre genetischen Informationen nicht für sich behalten wollten, haben einige der Freiwilligen geantwortet, wenn sie etwas geheim hielten, dann ihre finanzielle Situation.\nDie Firmen, die eine Erstellung des persönlichen Genoms anbieten, betonen natürlich, dass der Kunde allein wichtige Informationen über sich selbst und seine Möglichkeiten bekommt, seine Gesundheit zu fördern. Allerdings kosten diese Informationen bislang noch zwischen 300’000 und einer Million US-Dollar. Trotz dieses Preises rechnet die Branche aber damit, dass in den kommenden Monaten rund ein Dutzend individueller Gensequenzen mit drei Milliarden Buchstaben ermittelt und publiziert werden. Der charismatische Genforscher Craig Venter und der DNA-Pionier James Watson waren die ersten Personen, deren eigene persönliche DNA-Sequenzen ermittelt und veröffentlicht wurden, ohne dass dabei etwas Spektakuläres zu vermelden gewesen wäre.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "„Was passiert, wenn jeder von uns sein Genom kennt?“ So lautet die Frage, um die es in Zukunft gehen könnte, und es empfiehlt sich, darauf vorbreitet zu sein. Es gibt zum Beispiel schon länger ein umfassendes „Personal Genome Project“, mit dem sein Initiator, George Church von der Harvard Universität, genetische Daten mit Informationen über das Erscheinungsbild (Phänotyp) und die Umwelt integrieren möchte. Er sucht seit 2007 eintausend Freiwillige, die ihm dabei helfen und ihre persönlichen Genomsequenzen als Daten zur Verfügung stellen und öffentlich nutzen lassen.\nDas Personal Genome Project zielt letzten Endes darauf ab, insgesamt 100'000 Freiwillige zu finden, die ihr Genom sequenzieren lassen und ins Web stellen bzw. die Daten im Internet zur Verfügung stellen. Was zunächst Verwunderung hervorgerufen hat, bekommt inzwischen Schwung. Tatsächlich haben zehn Prominente sich bereit erklärt, ihre Gene auszustellen. Zu ihnen gehören der Neurobiologe und Bestsellerautor Steven Pinker sowie Sergey Brin, ein Mitbegründer von Google. Im Anschluss an diese Prominenten sind die ersten 5000 Anmeldungen bei George Church eingegangen. Gefragt, ob sie ihre genetischen Informationen nicht für sich behalten wollten, haben einige der Freiwilligen geantwortet, wenn sie etwas geheim hielten, dann ihre finanzielle Situation.\nDie Firmen, die eine Erstellung des persönlichen Genoms anbieten, betonen natürlich, dass der Kunde allein wichtige Informationen über sich selbst und seine Möglichkeiten bekommt, seine Gesundheit zu fördern. Allerdings kosten diese Informationen bislang noch zwischen 300’000 und einer Million US-Dollar. Trotz dieses Preises rechnet die Branche aber damit, dass in den kommenden Monaten rund ein Dutzend individueller Gensequenzen mit drei Milliarden Buchstaben ermittelt und publiziert werden. Der charismatische Genforscher Craig Venter und der DNA-Pionier James Watson waren die ersten Personen, deren eigene persönliche DNA-Sequenzen ermittelt und veröffentlicht wurden, ohne dass dabei etwas Spektakuläres zu vermelden gewesen wäre.",
      "id": [
        "145864"
      ],
      "item": "Die persönlichen Informationen"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145865]\n[%Personal Genome Project]\nMehr zu dem umstrittenen Personal Genome Project findet man im Internet unter www.1000genomes.org und www.personalgenomes.org.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Mehr zu dem umstrittenen Personal Genome Project findet man im Internet unter www.1000genomes.org und www.personalgenomes.org.",
      "id": [
        "145865"
      ],
      "item": "Personal Genome Project"
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145866]\n[%4]\n[#Trillionen Transistoren]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "4",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 1,
      "progress": true,
      "toc": true,
      "id": [
        "145866"
      ],
      "title": "Trillionen Transistoren"
    }
  },
  {
    "bitmark": "[.chapter]\n[@id:145867]\n[##Komplizierte Alltagsgegenstände; Computer und ihre Sprachen]",
    "bit": {
      "type": "chapter",
      "format": "bitmark--",
      "body": "",
      "item": "",
      "hint": "",
      "isExample": false,
      "example": "",
      "level": 2,
      "progress": true,
      "toc": true,
      "id": [
        "145867"
      ],
      "title": "Komplizierte Alltagsgegenstände; Computer und ihre Sprachen"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145868]\n[%Einführung]\nDer Transistor ist 1947 von drei Forschern entwickelt worden, die bei den Bell-Laboratorien beschäftigt waren. Dieses Institut arbeitete im Auftrag der größten amerikanischen Telefongesellschaft AT&T. Der Telefongesellschaft ging es um Methoden, das transatlantische Rauschen bei interkontinentalen Telefongesprächen zu unterdrücken, das noch bis in die 1970er Jahre hinein erkennen ließ, dass man ein Ferngespräch mit Übersee führte. Zu diesem Zweck wurden elektronische Schaltelemente benötigt, die den in dicken Kabeln über weite Strecken zu übertragenden Signalen in regelmäßigen Abständen neue Stärke in alter Form verleihen konnten. Nachdem sich die zu Beginn sehr anfälligen und viel zu viel Energie verbrauchenden Röhren aus verschiedenen Gründen für diese Aufgabe als unbrauchbar erwiesen hatten, suchten die Forscher nach einem besseren elektronischen Verstärker. Und im Verlauf dieser Arbeiten konstruierten sie den ersten Transistor.\nDas aus drei Schichten – einem Emitter, einem Kollektor und einer dazwischen liegenden Base – bestehende Gebilde war anfänglich noch ziemlich klobig. Es erfüllte aber sofort seinen Zweck und half zum Beispiel, Sprach- und Musiksignale zu verstärken. Bald konnte man die kristalline Anordnung der Halbleiterelemente kompakter herstellen, und damit begann der eigentliche und wahrhaft triumphale Siegeszug des Transistors in der Informationstechnologie, kurz IT genannt.\nIm 21. Jahrhundert sind Jahr um Jahr viele Trillionen Transistoren hergestellt und auf winzigen Prozessoren in informationsverarbeitende Computer eingebaut worden. Schon die Anfertigung einer einzigen Trillion Transistoren im Jahr bedeutet die Produktion von rund einer Million Transistoren in der Sekunde! Eine unbegreifliche Menge! Im Folgenden soll beleuchtet werden, wie und wann es zu dieser Dynamik mit der nachfolgenden industriellen Revolution der Informationen gekommen ist.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der Transistor ist 1947 von drei Forschern entwickelt worden, die bei den Bell-Laboratorien beschäftigt waren. Dieses Institut arbeitete im Auftrag der größten amerikanischen Telefongesellschaft AT&T. Der Telefongesellschaft ging es um Methoden, das transatlantische Rauschen bei interkontinentalen Telefongesprächen zu unterdrücken, das noch bis in die 1970er Jahre hinein erkennen ließ, dass man ein Ferngespräch mit Übersee führte. Zu diesem Zweck wurden elektronische Schaltelemente benötigt, die den in dicken Kabeln über weite Strecken zu übertragenden Signalen in regelmäßigen Abständen neue Stärke in alter Form verleihen konnten. Nachdem sich die zu Beginn sehr anfälligen und viel zu viel Energie verbrauchenden Röhren aus verschiedenen Gründen für diese Aufgabe als unbrauchbar erwiesen hatten, suchten die Forscher nach einem besseren elektronischen Verstärker. Und im Verlauf dieser Arbeiten konstruierten sie den ersten Transistor.\nDas aus drei Schichten – einem Emitter, einem Kollektor und einer dazwischen liegenden Base – bestehende Gebilde war anfänglich noch ziemlich klobig. Es erfüllte aber sofort seinen Zweck und half zum Beispiel, Sprach- und Musiksignale zu verstärken. Bald konnte man die kristalline Anordnung der Halbleiterelemente kompakter herstellen, und damit begann der eigentliche und wahrhaft triumphale Siegeszug des Transistors in der Informationstechnologie, kurz IT genannt.\nIm 21. Jahrhundert sind Jahr um Jahr viele Trillionen Transistoren hergestellt und auf winzigen Prozessoren in informationsverarbeitende Computer eingebaut worden. Schon die Anfertigung einer einzigen Trillion Transistoren im Jahr bedeutet die Produktion von rund einer Million Transistoren in der Sekunde! Eine unbegreifliche Menge! Im Folgenden soll beleuchtet werden, wie und wann es zu dieser Dynamik mit der nachfolgenden industriellen Revolution der Informationen gekommen ist.",
      "id": [
        "145868"
      ],
      "item": "Einführung"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145869]\n[%IT]\nIT, meist englisch ausgesprochen, bedeutet Informationstechnik oder – wie es oft in Nachbildung des englischen Wortes für Technik, technology, heißt – Informationstechnologie. IT bezeichnet die Computertechnik und darüber hinaus die mit dieser zunehmend zusammenwachsende Kommunikationstechnik. IT hat den etwas enger auf Computertechnik abhebenden Begriff EDV (elektronische Datenverarbeitung) weitgehend abgelöst.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "IT, meist englisch ausgesprochen, bedeutet Informationstechnik oder – wie es oft in Nachbildung des englischen Wortes für Technik, technology, heißt – Informationstechnologie. IT bezeichnet die Computertechnik und darüber hinaus die mit dieser zunehmend zusammenwachsende Kommunikationstechnik. IT hat den etwas enger auf Computertechnik abhebenden Begriff EDV (elektronische Datenverarbeitung) weitgehend abgelöst.",
      "id": [
        "145869"
      ],
      "item": "IT"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145870]\n[%Silicon Valley]\nVon den drei Wissenschaftlern, die 1956 als Erfinder des Transistors mit dem Nobelpreis für Physik ausgezeichnet worden sind, hat nur einer gesehen, dass in diesem elektronischen Bauelement ein ungeheures Wirtschafts- und Wachstumspotential steckte, und er hat konsequent nach dieser Einsicht gehandelt. Gemeint ist Bill Shockley, der die Bell Labs in den 1950er Jahren verließ, um eine eigene Firma zur Herstellung von Halbleitern zu gründen – „Shockley Semiconductor“.\nEs ging ihm und den Mitarbeitern des Unternehmens unter anderem darum, die Herstellung und Dotierung von Halbleitern wie Germanium und Silizium zu verbessern, was zum Beispiel bedeutete, sich um die Reinheit der Kristalle, die Genauigkeit der Abmessungen und die Zuverlässigkeit der Schichtung zu kümmern und zugleich das ganze Konstrukt immer kleiner werden zu lassen. Diese Miniaturisierung und die anderen Verbesserungen lagen nicht nur im Interesse des Militärs, das viele Aufträge vergab. Sie lagen zum Beispiel auch im Interesse der Hersteller (und Nutzer) von Hörgeräten, die ab 1951 mit Transistoren ausgestattet werden konnten und damit nicht nur erfreulich klein und also leicht tragbar wurden, sondern auch zuverlässiger funktionierten.\nShockley und sein Team machten sich mit Macht daran, Transistoren mit geringerem Gewicht und höherer Betriebssicherheit zu entwerfen – technisch gesprochen wechselten sie das grundlegende Design des elektronischen Verstärkerelements aus: Sie ersetzten den Punkt- durch einen Flächentransistor. Um 1953 präsentierten sie ein serienreifes Modell, wobei sie als grundlegenden Halbleiter auf das Element Silizium zurückgreifen konnten, das reichlich zur Verfügung stand. Ihm verdanken wir den 1971 aufgekommenen Namen „Silicon Valley“ für das Gebiet im Süden der Bucht von San Francisco, in dem in den kommenden Jahrzehnten die Computerindustrie aufblühte.\nAm Anfang von Silicon Valley gab es nur ein Stück Land, das die Gründerfamilie der Stanford-Universität in Kalifornien ihrer akademischen Heimstätte mit der Maßgabe vermacht hatte, es nicht zu verkaufen, sondern – wie auch immer – zu nutzen. So kam es, dass in den 1930er Jahren der Dekan der Stanford-Universität junge Studenten wie William Hewlett und David Packard ermutigte, auf dem Gelände eigene Unternehmen – etwa eine Elektronikfirma – zu gründen. Die Universität stattete die Mutigen sogar mit dem passenden Startkapital aus. Nach dem Zweiten Weltkrieg wurden die Pläne zur Besiedlung des Tals durch die Einrichtung eines Stanford Industrial Parks erweitert, und Mitte der 1950er Jahre traf dann das Shockley-Team mit seinen Halbleitern und den ehrgeizigen Plänen ein. Das dazu gegründete Unternehmen funktionierte zwar nur bis 1957, aber die acht Personen, die sich in diesem Jahr von dem als Unternehmensleiter anscheinend unerträglichen Shockley trennten und ihr eigenes Unternehmen mit Namen „Fairchild Semiconductor“ gründeten – sie wurden unter Insidern zunächst „Die acht Verräter“ genannt –, blieben vor Ort und machten ihn bald riesengroß und weltbekannt. Wer ihre Namen zum ersten Mal liest, wird nicht unbedingt auf viele Bekannte stoßen – Julius Blank, Victor Grinich, Jean Hoerni, Eugene Kleiner, Jay Last, Gordon Moore, Robert Noyce und Sheldon Roberts –, aber dieses Schattendasein stellt oftmals das Schicksal von Wissenschaftlern oder Ingenieuren dar, die eine verwöhnte Gesellschaft ihre Schuldigkeit tun und dann ohne weitere Beachtung gehen lässt.\nWer die oben angeführte Liste liest, wird vielleicht bei dem Namen Moore stutzen und sich an das erinnern, was in den Zeitungen oft als Mooresches Gesetz bezeichnet wird, obwohl es das weder ist noch sein kann. Es geht dabei vielmehr um die in der Mitte der 1960er Jahre von Moore in einem Vortrag aufgestellte (außerordentlich kühne und höchst optimistische) Prognose, dass die Zahl der Transistoren, die man auf einem Silizium-Chip unterbringen kann – und damit die Speicher- und Rechenkapazität eines Computers, der damit ausgestattet ist –, sich etwa alle achtzehn Monate verdoppeln wird. Kurz gesagt, Computer werden sehr rasch sehr viel besser, und das dauernd. Als Moore diese Prognose riskierte, wurde viel gelächelt. Aber heute dürfen wir voller Verblüffung feststellen, dass sich sein „Gesetz“ mindestens vierzig Jahre lang ziemlich genau bewährt hat, was eine fast unvorstellbare Bilanz bedeutet, wenn man ihre Konsequenzen konkret ausrechnet. Wenn sich nämlich eine Menge (oder etwas anderes) vierzig Jahre lang alle achtzehn Monate verdoppelt, dann ergibt dies eine (exponentielle) Steigerung um den Faktor hundert Millionen, und genau diese gigantische Zahl lässt sich tatsächlich für die Rechenleistung der Computer nachweisen – mit der praktischen Folge, dass heute in jedem einzelnen Mikrowellenherd oder in jeder noch so preiswerten Digitalkamera mehr Rechenkapazität vorhanden ist, als der ganzen Welt in den 1950er Jahren zur Verfügung stand, als Shockley ins Silicon Valley kam.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Von den drei Wissenschaftlern, die 1956 als Erfinder des Transistors mit dem Nobelpreis für Physik ausgezeichnet worden sind, hat nur einer gesehen, dass in diesem elektronischen Bauelement ein ungeheures Wirtschafts- und Wachstumspotential steckte, und er hat konsequent nach dieser Einsicht gehandelt. Gemeint ist Bill Shockley, der die Bell Labs in den 1950er Jahren verließ, um eine eigene Firma zur Herstellung von Halbleitern zu gründen – „Shockley Semiconductor“.\nEs ging ihm und den Mitarbeitern des Unternehmens unter anderem darum, die Herstellung und Dotierung von Halbleitern wie Germanium und Silizium zu verbessern, was zum Beispiel bedeutete, sich um die Reinheit der Kristalle, die Genauigkeit der Abmessungen und die Zuverlässigkeit der Schichtung zu kümmern und zugleich das ganze Konstrukt immer kleiner werden zu lassen. Diese Miniaturisierung und die anderen Verbesserungen lagen nicht nur im Interesse des Militärs, das viele Aufträge vergab. Sie lagen zum Beispiel auch im Interesse der Hersteller (und Nutzer) von Hörgeräten, die ab 1951 mit Transistoren ausgestattet werden konnten und damit nicht nur erfreulich klein und also leicht tragbar wurden, sondern auch zuverlässiger funktionierten.\nShockley und sein Team machten sich mit Macht daran, Transistoren mit geringerem Gewicht und höherer Betriebssicherheit zu entwerfen – technisch gesprochen wechselten sie das grundlegende Design des elektronischen Verstärkerelements aus: Sie ersetzten den Punkt- durch einen Flächentransistor. Um 1953 präsentierten sie ein serienreifes Modell, wobei sie als grundlegenden Halbleiter auf das Element Silizium zurückgreifen konnten, das reichlich zur Verfügung stand. Ihm verdanken wir den 1971 aufgekommenen Namen „Silicon Valley“ für das Gebiet im Süden der Bucht von San Francisco, in dem in den kommenden Jahrzehnten die Computerindustrie aufblühte.\nAm Anfang von Silicon Valley gab es nur ein Stück Land, das die Gründerfamilie der Stanford-Universität in Kalifornien ihrer akademischen Heimstätte mit der Maßgabe vermacht hatte, es nicht zu verkaufen, sondern – wie auch immer – zu nutzen. So kam es, dass in den 1930er Jahren der Dekan der Stanford-Universität junge Studenten wie William Hewlett und David Packard ermutigte, auf dem Gelände eigene Unternehmen – etwa eine Elektronikfirma – zu gründen. Die Universität stattete die Mutigen sogar mit dem passenden Startkapital aus. Nach dem Zweiten Weltkrieg wurden die Pläne zur Besiedlung des Tals durch die Einrichtung eines Stanford Industrial Parks erweitert, und Mitte der 1950er Jahre traf dann das Shockley-Team mit seinen Halbleitern und den ehrgeizigen Plänen ein. Das dazu gegründete Unternehmen funktionierte zwar nur bis 1957, aber die acht Personen, die sich in diesem Jahr von dem als Unternehmensleiter anscheinend unerträglichen Shockley trennten und ihr eigenes Unternehmen mit Namen „Fairchild Semiconductor“ gründeten – sie wurden unter Insidern zunächst „Die acht Verräter“ genannt –, blieben vor Ort und machten ihn bald riesengroß und weltbekannt. Wer ihre Namen zum ersten Mal liest, wird nicht unbedingt auf viele Bekannte stoßen – Julius Blank, Victor Grinich, Jean Hoerni, Eugene Kleiner, Jay Last, Gordon Moore, Robert Noyce und Sheldon Roberts –, aber dieses Schattendasein stellt oftmals das Schicksal von Wissenschaftlern oder Ingenieuren dar, die eine verwöhnte Gesellschaft ihre Schuldigkeit tun und dann ohne weitere Beachtung gehen lässt.\nWer die oben angeführte Liste liest, wird vielleicht bei dem Namen Moore stutzen und sich an das erinnern, was in den Zeitungen oft als Mooresches Gesetz bezeichnet wird, obwohl es das weder ist noch sein kann. Es geht dabei vielmehr um die in der Mitte der 1960er Jahre von Moore in einem Vortrag aufgestellte (außerordentlich kühne und höchst optimistische) Prognose, dass die Zahl der Transistoren, die man auf einem Silizium-Chip unterbringen kann – und damit die Speicher- und Rechenkapazität eines Computers, der damit ausgestattet ist –, sich etwa alle achtzehn Monate verdoppeln wird. Kurz gesagt, Computer werden sehr rasch sehr viel besser, und das dauernd. Als Moore diese Prognose riskierte, wurde viel gelächelt. Aber heute dürfen wir voller Verblüffung feststellen, dass sich sein „Gesetz“ mindestens vierzig Jahre lang ziemlich genau bewährt hat, was eine fast unvorstellbare Bilanz bedeutet, wenn man ihre Konsequenzen konkret ausrechnet. Wenn sich nämlich eine Menge (oder etwas anderes) vierzig Jahre lang alle achtzehn Monate verdoppelt, dann ergibt dies eine (exponentielle) Steigerung um den Faktor hundert Millionen, und genau diese gigantische Zahl lässt sich tatsächlich für die Rechenleistung der Computer nachweisen – mit der praktischen Folge, dass heute in jedem einzelnen Mikrowellenherd oder in jeder noch so preiswerten Digitalkamera mehr Rechenkapazität vorhanden ist, als der ganzen Welt in den 1950er Jahren zur Verfügung stand, als Shockley ins Silicon Valley kam.",
      "id": [
        "145870"
      ],
      "item": "Silicon Valley"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145871]\n[%William Shockley]\nWilliam Shockley (1910-1989), der Miterfinder des Transistors ist in seinen späteren Jahren nicht nur durch seine rassistischen Äußerungen, sondern auch durch seinen schlechten Führungsstil als Unternehmer aufgefallen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "William Shockley (1910-1989), der Miterfinder des Transistors ist in seinen späteren Jahren nicht nur durch seine rassistischen Äußerungen, sondern auch durch seinen schlechten Führungsstil als Unternehmer aufgefallen.",
      "id": [
        "145871"
      ],
      "item": "William Shockley"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145872]\n[%Der integrierte Schaltkreis]\nDer Ausdruck Chip, den man inzwischen zwar allgemein kennt, musste erst einmal eingeführt werden. Die Voraussetzungen dafür wurden 1958 geschaffen, als der amerikanische Ingenieur Jack Kilby das erfand, was heute integrierter Schaltkreis oder Mikrochip heißt – kurz Chip. Die ersten elektronischen Schaltungen bestanden noch aus einzelnen Transistoren, die auf einer sogenannten Leiterplatte angebracht und miteinander verbunden (zusammengeschaltet) wurden, um logische oder andere Operationen ausführen zu können. Als die Designer der dazugehörigen „circuits“ dazu übergingen, immer mehr Transistoren zu kombinieren, bekamen diese Leiterplatten den schönen Namen Platine. Platinen bestehen aus einem isolierfähigen Material (einem Kunststoff), das mit schmalen leitfähigen Bahnen – zum Beispiel aus Kupfer – bestückt wird, in denen die Elektronen fließen und die dazugehörige Elektronik schaffen.",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Der Ausdruck Chip, den man inzwischen zwar allgemein kennt, musste erst einmal eingeführt werden. Die Voraussetzungen dafür wurden 1958 geschaffen, als der amerikanische Ingenieur Jack Kilby das erfand, was heute integrierter Schaltkreis oder Mikrochip heißt – kurz Chip. Die ersten elektronischen Schaltungen bestanden noch aus einzelnen Transistoren, die auf einer sogenannten Leiterplatte angebracht und miteinander verbunden (zusammengeschaltet) wurden, um logische oder andere Operationen ausführen zu können. Als die Designer der dazugehörigen „circuits“ dazu übergingen, immer mehr Transistoren zu kombinieren, bekamen diese Leiterplatten den schönen Namen Platine. Platinen bestehen aus einem isolierfähigen Material (einem Kunststoff), das mit schmalen leitfähigen Bahnen – zum Beispiel aus Kupfer – bestückt wird, in denen die Elektronen fließen und die dazugehörige Elektronik schaffen.",
      "id": [
        "145872"
      ],
      "item": "Der integrierte Schaltkreis"
    }
  },
  {
    "bitmark": "[.article:bitmark--]\n[@id:145873]\n[%Boolesche Logik]\nIm Jahre 1847 machte der Engländer George Boole (1815-1864) sich daran, der traditionellen philosophischen Logik eine mathematische Version an die Seite zu stellen. Logik in der Philosophie argumentierte mit Sätzen wie: Alle Menschen sind sterblich. Sokrates ist ein Mensch. Also ist Sokrates sterblich. Boole wollte solche All- und Einzel-Aussagen und ihre logischen Verknüpfungen, wie sie als UND, ODER, ENTWEDER-ODER, NICHT bekannt sind, ganz allgemein darstellen und abstrakte Regeln für sie und die aus ihnen möglichen Schlussfolgerungen aufstellen. Wenn zum Beispiel eine Zahl X größer ist als eine Zahl Y, und wenn dieses Y größer ist als ein Z, dann ist auch X größer als Z. Oder wenn eine Aussage A und ihr Gegenteil (Nicht-A) gelten, dann kann A nur Null (eine leere Menge) sein und also nichts bedeuten. Oder mit einer anderen Aussage B: Wenn „A und (A oder B)“ gilt, dann gilt auf jeden Fall A. Boole stellte nach und nach viele Regeln zusammen – zum Beispiel: „A und B“ ist gleichbedeutend mit „B und A“ oder eine doppelte Verneinung hebt sich auf – und führte Zeichen für die logischen Operationen ein, und dabei entstand das, was Lehrbücher heute als Boolsche Logik oder Boolsche Algebra bezeichnen, weil man mit seinen Regeln in jeder Hinsicht rechnen kann. Es war der bereits erwähnte Claude Shannon, der1940 bemerkte, dass sich mit der Boolschen Logik und dem dazugehörigen Kalkül elektrische Schaltungen beschreiben ließen, die als UND-Gatter oder als ODER-Gatter realisiert werden konnten. Bei einem UND-Gatter hat man zum Beispiel zwei Eingänge (a, b), die in einen Ausgang münden, und die elektronische Schaltung sorgt dafür, dass nur dann etwas dort ankommt, wenn beide Eingänge (a und b) ein Signal bekommen. Bei einem ODER-Gatter reicht es, wenn bei a oder b ein Strom fließt.  Tatsächlich konnte die Boolsche Logik konnte als Transistorlogik (oder einfacher als Diodenlogik) in Maschinen umgesetzt und eingesetzt werden, und da sie – wie die klassische Logik der Philosophen – eindeutig zwischen einer wahren und einer falschen Aussage unterscheidet, da sie also binär gesprochen, mit den Zeichen 0 und 1 operierte, konnte sie benutzt werden, um die Grundlage für alle Computerschaltungen und Programmiersprachen darzustellen, was sie heute tut.Wenn Transistoren mit Boolscher Logik auf Chips kombiniert und integriert werden, ermöglichen sie dem dabei entstehenden Prozessor, genau so logisch zu operieren wie unser Gehirn. Man kann sich auf beide verlassen – oder?",
    "bit": {
      "type": "article",
      "format": "bitmark--",
      "body": "Im Jahre 1847 machte der Engländer George Boole (1815-1864) sich daran, der traditionellen philosophischen Logik eine mathematische Version an die Seite zu stellen. Logik in der Philosophie argumentierte mit Sätzen wie: Alle Menschen sind sterblich. Sokrates ist ein Mensch. Also ist Sokrates sterblich. Boole wollte solche All- und Einzel-Aussagen und ihre logischen Verknüpfungen, wie sie als UND, ODER, ENTWEDER-ODER, NICHT bekannt sind, ganz allgemein darstellen und abstrakte Regeln für sie und die aus ihnen möglichen Schlussfolgerungen aufstellen. Wenn zum Beispiel eine Zahl X größer ist als eine Zahl Y, und wenn dieses Y größer ist als ein Z, dann ist auch X größer als Z. Oder wenn eine Aussage A und ihr Gegenteil (Nicht-A) gelten, dann kann A nur Null (eine leere Menge) sein und also nichts bedeuten. Oder mit einer anderen Aussage B: Wenn „A und (A oder B)“ gilt, dann gilt auf jeden Fall A. Boole stellte nach und nach viele Regeln zusammen – zum Beispiel: „A und B“ ist gleichbedeutend mit „B und A“ oder eine doppelte Verneinung hebt sich auf – und führte Zeichen für die logischen Operationen ein, und dabei entstand das, was Lehrbücher heute als Boolsche Logik oder Boolsche Algebra bezeichnen, weil man mit seinen Regeln in jeder Hinsicht rechnen kann. Es war der bereits erwähnte Claude Shannon, der1940 bemerkte, dass sich mit der Boolschen Logik und dem dazugehörigen Kalkül elektrische Schaltungen beschreiben ließen, die als UND-Gatter oder als ODER-Gatter realisiert werden konnten. Bei einem UND-Gatter hat man zum Beispiel zwei Eingänge (a, b), die in einen Ausgang münden, und die elektronische Schaltung sorgt dafür, dass nur dann etwas dort ankommt, wenn beide Eingänge (a und b) ein Signal bekommen. Bei einem ODER-Gatter reicht es, wenn bei a oder b ein Strom fließt.  Tatsächlich konnte die Boolsche Logik konnte als Transistorlogik (oder einfacher als Diodenlogik) in Maschinen umgesetzt und eingesetzt werden, und da sie – wie die klassische Logik der Philosophen – eindeutig zwischen einer wahren und einer falschen Aussage unterscheidet, da sie also binär gesprochen, mit den Zeichen 0 und 1 operierte, konnte sie benutzt werden, um die Grundlage für alle Computerschaltungen und Programmiersprachen darzustellen, was sie heute tut.Wenn Transistoren mit Boolscher Logik auf Chips kombiniert und integriert werden, ermöglichen sie dem dabei entstehenden Prozessor, genau so logisch zu operieren wie unser Gehirn. Man kann sich auf beide verlassen – oder?",
      "id": [
        "145873"
      ],
      "item": "Boolesche Logik"
    }
  }
]